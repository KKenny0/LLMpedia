{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction 本 Gitbook 搜集和分享个人喜欢的大语言模型（LLM）相关资源的在线百科全书。 涵盖 LLM 的基础知识、研究进展、技术应用、工具和最佳实践。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/":{"url":"Chap1-ModelArch/","title":"模型结构","keywords":"","body":"模型结构 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/Infrastructure_transformer_transformer-explainer.html":{"url":"Chap1-ModelArch/Infrastructure_transformer_transformer-explainer.html","title":"Transformer 可视化解释","keywords":"","body":"Transformer Expander 是一种交互式可视化工具，旨在帮助任何人了解 GPT 等基于 Transformer 的模型如何工作。 它直接在您的浏览器中运行实时 GPT-2 模型，允许您试验自己的文本并实时观察 Transformer 的内部组件和操作如何协同工作以预测下一个令牌。 transformer-explainer transformer-explainer-demo 什么是 Transformer？ 从根本上来说，文本生成 Transformer 模型的运行原理是下一个单词预测：给定用户的文本提示，该输入之后最有可能出现的下一个单词是什么？ Transformer 的核心创新和强大之处在于它们使用自注意力机制，这使得它们能够比以前的架构更有效地处理整个序列并捕获远程依赖关系。 Transformer 结构 每个文本生成 Transformer 都包含以下三个关键组件： Embedding Transformer 块: 注意力机制: Transformer模块的核心组件。它允许标记与其他标记进行通信，捕获上下文信息和单词之间的关系。 多层感知器层: 一个独立运行在每个令牌上的前馈网络。注意力层的目标是在 token 之间路由信息，而 MLP 的目标是细化每个 token 的表示。 输出概率: 最后的线性层和 softmax 层将处理后的嵌入转换为概率，使模型能够预测序列中的下一个标记。 Embedding 嵌入的用武之地：它将文本转换为模型可以使用的数字表示。 要将提示转换为嵌入，我们需要: 1) 对输入进行标记; 2) 获取标记嵌入; 3) 添加位置信息，最后 4) 将标记和位置编码相加以获得最终嵌入。 Step1: Tokenization 标记化是将输入文本分解为更小、更易于管理的片段（称为标记）的过程。 这些标记可以是单词或子单词。 “数据”和“可视化”一词对应于独特的标记，而“授权”一词则分为两个标记。 令牌的完整词汇表在训练模型之前确定：GPT-2 的词汇表有 50,257 个唯一令牌。 现在我们将输入文本分割成具有不同 ID 的标记，我们可以从嵌入中获取它们的向量表示。 Step2: Token Embedding GPT-2 Small 将词汇表中的每个 token 表示为 768 维向量；向量的维度取决于模型。 这些嵌入向量存储在形状为 (50,257, 768) 的矩阵中，包含大约 3900 万个参数！ 这个广泛的矩阵允许模型为每个标记分配语义。 Step3: 位置编码 嵌入层还对输入提示中每个标记的位置信息进行编码。不同的模型使用不同的位置编码方法。 GPT-2 从头开始训练自己的位置编码矩阵，并将其直接集成到训练过程中。 Step4: 最终 Embedding 最后，我们将令牌和位置编码相加以获得最终的嵌入表示。这种组合表示捕获了标记的语义及其在输入序列中的位置。 Transformer 块 Transformer 处理的核心在于 Transformer 块，其中包括多头自注意力和多层感知器层。 大多数模型由多个这样的块组成，这些块按顺序依次堆叠。 Token 表示从第一个区块到第 12 个区块逐层演变，使模型能够建立对每个代币的复杂理解。 这种分层方法导致输入的高阶表示。 多头注意力 Query, Key and Value 矩阵 Masked Self-Attention Attention Score Masking Softmax Output: 该模型使用屏蔽的自注意力分数并将其与值矩阵相乘以获得自注意力机制的最终输出。 MLP 在多个自注意力头捕获输入标记之间的不同关系后，连接的输出将通过多层感知器（MLP）层，以增强模型的表示能力。 MLP 块由两个线性变换组成，中间有一个 GELU 激活函数。 第一个线性变换将输入的维度从 768 增加四倍到 3072。 第二个线性变换将维度降低回原始大小 768，确保后续层接收一致维度的输入。 与自注意力机制不同，MLP 独立处理 token，并简单地将它们从一种表示映射到另一种表示。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/Infrastructure_luxiangdong_Transformer-OverallArch.html":{"url":"Chap1-ModelArch/Infrastructure_luxiangdong_Transformer-OverallArch.html","title":"土猛的员外-Transformer 架构的整体指南","keywords":"","body":" Transformer 架构的整体指南 Transformer架构的核心组件 Transformer 架构的核心在于注意力机制，它包括编码器（Encoder）和解码器（Decoder）两个主要部分。 编码器负责将输入序列转换为压缩表示，而解码器则基于编码器的输出生成目标序列。 这一架构还包括全连接层、归一化层、Embedding 层和位置编码层等组件，共同作用于提高模型的性能和泛化能力。 特别是注意力机制，它允许模型动态地聚焦于输入序列中最重要的部分，从而提升模型对信息的处理能力。 Encoder和Decoder的结构与功能 Transformer 模型中的编码器（Encoder）由多个相同的层组成，每层都包含多头自注意力（MHA）和前馈神经网络（MLP）。 编码器的主要任务是理解输入序列，并将其编码成一个连续的表示，这个表示能够捕捉输入数据的深层语义信息。 解码器（Decoder）的结构与编码器类似，但它还包括一个额外的多头注意力层，用于融合编码器的输出和目标序列的信息。 解码器的主要任务是生成与输入序列相对应的输出序列，例如在机器翻译任务中的翻译文本。 注意力机制的详细解释 注意力机制是 Transformer 模型的核心，它允许模型在处理序列时动态地聚焦于不同的部分。 注意力机制涉及查询（Query）、键（Key）和值（Value）三个主要组件，通过计算查询和所有键的匹配程度来确定每个值的重要性。 这种机制使得模型能够捕捉序列内部的长距离依赖关系，并在生成输出时考虑到整个输入序列的上下文信息。 多头注意力和MLPs的作用 多头注意力（Multi-Head Attention）是 Transformer 模型中的一个关键特性，它允许模型在不同的表示子空间中并行地捕捉信息，从而提高了模型对复杂关系的理解能力。 MLPs（多层感知器）作为 Transformer 中的另一个子层，负责处理序列中的每个位置，它们通过非线性变换增强模型的表达能力。 Embeddings和位置编码层的重要性 在 Transformer 模型中，输入序列首先被转换为嵌入（Embeddings），这些嵌入是固定大小的密集向量，能够捕捉单词或token的语义信息。 位置编码（Positional Encoding）层则用于保持序列中单词的顺序信息，因为 Transformer 模型的自注意力机制本身不具备对序列顺序的感知能力。 残差连接、层规范化和Dropout的讨论 为了提高模型的训练效率和稳定性，Transformer 模型采用了残差连接（Residual Connections）和层规范化（Layer Normalization）。 残差连接允许模型在每一层中直接传播信息，而层规范化则有助于稳定训练过程。 Dropout 作为一种正则化技术，被用于防止模型过拟合，提高模型的泛化能力。 线性和Softmax图层的功能 在解码器的最后，线性层（Linear Layer）将解码器的激活投影到词汇表的大小，产生对数概率。 Softmax 层（Softmax Layer）则将这些对数概率转换为下一个 token 的概率分布，使得模型能够预测序列中的下一个 token。 注意力机制的可视化和解释 通过可视化工具，我们可以直观地看到 Transformer 模型中的注意力权重，这有助于我们理解模型在处理输入序列时关注的重点。 这种可视化不仅增强了模型的可解释性，也为我们提供了洞察模型内部工作机制的手段。 注意力机制的优势和挑战 注意力机制带来了许多优势，包括更好地处理长期依赖关系、提高并行化能力、增强模型的可解释性以及在多个任务中提高性能。 然而，它也面临着挑战，如随着序列长度增加而增长的内存消耗和计算成本，以及在推理过程中可能需要的顺序方法。 大型语言模型的演变和设计 大型语言模型（LLMs）是 Transformer 模型的直接扩展，它们通过在大量文本数据上进行预训练，获得了强大的语言理解和生成能力。 这些模型的设计包括纯编码器模型（如 BERT）、纯解码器模型（如 GPT 系列）和编码器-解码器模型（如 T5）。 这些模型在不同的任务和应用中展现出了卓越的性能。 Encoder、Decoder 和 Encoder-Decoder 大语言模型的比较 不同类型的大型语言模型根据其架构设计适用于不同的任务。 纯编码器模型适合于 NLP 的判别任务，如文本分类；纯解码器模型适合于生成任务，如文本续写； 编码器-解码器模型则适用于需要处理两个序列的任务，如机器翻译。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/Infrastructure_HF_Encoder-models.html":{"url":"Chap1-ModelArch/Infrastructure_HF_Encoder-models.html","title":"Encoder 模型","keywords":"","body":"编码器模型 Encoder models 编码器模型仅使用 Transformer 模型的编码器。 在每个阶段，注意力层都可以访问初始句子中的所有单词。 这些模型通常被描述为具有“双向”注意力，并且通常被称为自编码模型。 这些模型的预训练通常围绕着以某种方式破坏给定的句子（例如，通过屏蔽其中的随机单词）并要求模型查找或重建初始句子。 编码器模型最适合需要理解完整句子的任务，例如句子分类、命名实体识别（以及更一般的单词分类）和提取式问答。 这个模型家族的代表包括: BERT ALBERT DistillBERT ELECTRA RoBERTa BERT BERT BERT 模型在 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 中提出。 它是一个双向 Transformer，使用掩码语言建模（masked language modeling）目标和下一句预测（next sentence prediction）相结合的方式对大型语料库进行预训练。 使用技巧 BERT 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 BERT 使用掩码语言模型 (MLM) 和下一句预测 (NSP) 目标进行训练。一般来说，它在预测屏蔽标记和 NLU 方面非常有效，但对于文本生成来说并不是最佳选择。 通过使用随机屏蔽（random masking）来破坏输入，更准确地说，在预训练期间，给定百分比的标记（通常为 15%）被以下方式屏蔽： 标记有 0.8 的概率打上特殊掩码 token 标记有 0.1 的概率打上随机 token（与原始标记不同） 标记有 0.1 的概率使用原始 token 该模型必须预测原始句子，但还有第二个目标：输入是两个句子 A 和 B（中间有一个分隔标记）。句子在语料库中连续的概率为 50%，其余 50% 的句子不相关。该模型必须预测句子是否连续。 ALBERT ALBERT ALBERT 模型在 《ALBERT：A Lite BERT for Self-supervised Learning of Language Representations》 中提出。 它提出了两种参数减少技术来降低内存消耗并提高 BERT 的训练速度： 将嵌入矩阵拆分为两个较小的矩阵 使用重复层（repeating layers）在组之间进行分割 ALBERT 还使用了一种自我监督损失，重点是对句子间的连贯性进行建模，实验表明这始终有助于具有多句子输入的下游任务。 使用技巧 ALBERT 是一个具有绝对位置嵌入的模型，因此通常建议将输入填充在右侧而不是左侧。 ALBERT 使用重复层，这会导致内存占用较小，但计算成本仍然类似于具有相同数量隐藏层的类 BERT 架构，因为它必须迭代相同数量的（重复）层。 层被划分为共享参数的组(以节省内存)。下一个句子预测被一个句子顺序预测所取代: 在输入中，我们有两个句子 A 和 B (是连续的) ，我们要么输入 A 后面跟着 B，要么输入 B 后面跟着 A。模型必须预测它们是否被交换了。 DistilBERT DistilBERT DistilBERT 在 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter 中提出。 DistilBERT 是一个通过蒸馏 BERT 基础训练而成的小型、快速、廉价且轻量的 Transformer 模型。 在这项工作中，作者提出了一种预训练较小的通用语言表示模型的方法，称为 DistilBERT，然后可以对其进行微调，使其在广泛的任务（如其较大的对应任务）上具有良好的性能。 虽然大多数先前的工作研究了如何使用蒸馏来构建特定于任务的模型，但作者在预训练阶段利用了知识蒸馏，并表明可以将 BERT 模型的大小减少 40%，同时保留 97% 的语言理解能力，速度提高 60%。 为了利用预训练期间较大模型学到的归纳偏差，作者引入了结合语言建模、蒸馏和余弦距离损失的三重损失。 使用技巧 (HF) DistilBERT 没有 token_type_ids，不需要指示哪个 token 属于哪个段。只需使用分隔标记 tokenizer.sep_token （或 [SEP]）分隔片段。 DistilBERT 没有选择输入位置（position_ids 输入）的选项。 与 BERT 相同但更小。通过预训练 BERT 模型的蒸馏进行训练，这意味着它经过训练可以预测与较大模型相同的概率。实际目标是以下各项的组合： 找到与教师模型（Teacher Model）相同的概率 正确预测屏蔽标记（但没有下一句目标） 学生模型和教师模型的隐藏状态之间的余弦相似度 ELECTRA ELECTRA ELECTRA 模型是在 ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators 中提出的。 ELECTRA 是一种新的预训练方法，它训练两个 Transformer 模型：生成器（generator）和判别器（discriminator）。 生成器的作用是替换序列中的标记，因此被训练为掩码语言模型。 判别器是我们感兴趣的模型，它尝试识别序列中哪些标记被生成器替换。 论文摘要： 掩码语言建模 (MLM) 预训练方法（例如 BERT）通过用 [MASK] 替换一些标记来破坏输入，然后训练模型来重建原始标记。 虽然它们在转移到下游 NLP 任务时会产生良好的结果，但它们通常需要大量计算才能有效。 作为替代方案，我们提出了一种样本效率更高的预训练任务，称为替换令牌检测。 我们的方法不是屏蔽输入，而是通过用从小型生成器网络采样的合理替代方案替换一些令牌来破坏输入。 然后，我们不是训练一个预测损坏令牌原始身份的模型，而是训练一个判别模型来预测损坏输入中的每个令牌是否被生成器样本替换。 彻底的实验证明这个新的预训练任务比 MLM 更有效，因为该任务是在所有输入标记上定义的，而不仅仅是被屏蔽的小子集。 使用技巧 ELECTRA 是预训练方法，因此对底层模型 BERT 几乎没有任何改变。唯一的变化是嵌入大小和隐藏大小的分离：嵌入大小通常较小，而隐藏大小较大。附加的投影层（线性）用于将嵌入从嵌入大小投影到隐藏大小。在嵌入大小与隐藏大小相同的情况下，不使用投影层。 ELECTRA 是一个 Transformer 模型，使用另一个（小型）掩码语言模型进行预训练。输入被该语言模型破坏，它接受随机屏蔽的输入文本，并输出一个文本，ELECTRA 必须预测其中哪个标记是原始标记，哪个标记被替换。与 GAN 训练一样，先对小语言模型进行几步训练（但目标是原始文本，而不是像传统的 GAN 设置那样愚弄 ELECTRA 模型），然后再对 ELECTRA 模型进行几步训练。 RoBERTa [RoBERTa(https://huggingface.co/docs/transformers/model_doc/roberta) RoBERTa 模型在 RoBERTa: A Robustly Optimized BERT Pretraining Approach 中提出。 它建立在 BERT 的基础上，修改了关键的超参数，删除了下一句话预训练目标，并使用更大的小批量和学习率进行训练。 论文摘要： 我们提出了 BERT 预训练的复制研究（Devlin 等人，2019），该研究仔细测量了许多关键超参数和训练数据大小的影响。 我们发现 BERT 的训练明显不足，但可以匹配或超过其之后发布的每个模型的性能。 实验结果凸显了以前被忽视的设计选择的重要性， 使用技巧 此实现与 BertModel 相同，只是对嵌入进行了细微调整，以及 RoBERTa 预训练模型的设置。 RoBERTa 具有与 BERT 相同的架构，但使用字节级 BPE （byte-level BPE）作为 tokenizer（与 GPT-2 相同）并使用不同的预训练方案。 RoBERTa 与 BERT 类似，但具有更好的预训练技术： 动态屏蔽：令牌在每个时期的屏蔽方式不同，而 BERT 则一劳永逸。 句子打包（Sentence packing）：句子打包在一起达到 512 个标记（因此句子的顺序可能跨越多个文档）。 较大批次：训练使用较大批次。 字节级 BPE 词汇：使用 BPE，将字节作为子单元，而不是字符，以适应 Unicode 字符。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/Infrastructure_HF_Decoder-models.html":{"url":"Chap1-ModelArch/Infrastructure_HF_Decoder-models.html","title":"Decoder 模型","keywords":"","body":"解码器模型 解码器模型仅使用 Transformer 模型的解码器。 在每个阶段，对于给定的单词，注意力层只能访问句子中位于该单词之前的单词。 这些模型通常称为自回归模型。 解码器模型的预训练通常围绕预测句子中的下一个单词进行。 这些模型最适合涉及文本生成的任务。 这个模型家族的代表包括: CTRL GPT GPT-2 CTRL CTRL 模型在 CTRL: A Conditional Transformer Language Model for Controllable Generation 中提出。 它是一个因果（单向）转换器，使用语言模型在大约 140 GB 文本数据的非常大的语料库上进行预训练，第一个标记保留为控制代码（例如链接、书籍、维基百科等）。 论文摘要： 这是一个条件 Transformer 模型，经过训练以控制样式、内容和特定任务行为的控制代码为条件。 控制代码源自与原始文本自然共存的结构，保留了无监督学习的优势，同时提供对文本生成的更明确的控制。 这些代码还允许 CTRL 预测训练数据的哪些部分最有可能给出序列。 这提供了一种通过基于模型的源归因来分析大量数据的潜在方法。 使用技巧 CTRL 利用控制代码来生成文本：它需要从某些单词、句子或链接开始生成，以生成连贯的文本。 CTRL 是一个具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 CTRL 经过因果语言建模 (CLM) 目标的训练，因此在预测序列中的下一个标记方面非常强大。利用此功能，CTRL 可以生成语法连贯的文本。 OpenAI GPT OpenAI GPT 模型在 Improving Language Understanding by Generative Pre-Training 中提出。 它是一个因果（单向）转换器，使用具有长范围依赖性的大型语料库（多伦多图书语料库）上的语言建模进行预训练。 论文摘要： 实验证明，通过在各种未标记文本的语料库上对语言模型进行生成式预训练，然后对每个特定任务进行区分性微调，可以实现这些任务的巨大收益。 与以前的方法相比，我们在微调过程中利用任务感知输入转换来实现有效的传输，同时需要对模型架构进行最小的更改。 使用技巧 GPT 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 GPT 使用因果语言建模 (CLM) 目标进行训练，因此在预测序列中的下一个标记方面非常强大。利用此功能，GPT 可以生成语法连贯的文本。 OpenAI GPT-2 模型在 Language Models are Unsupervised Multitask Learners 中提出。 它是一个因果（单向）Transformer，使用语言建模在约 40 GB 文本数据的非常大的语料库上进行预训练。 论文摘要： GPT-2 是一个基于 Transformer 的大型语言模型，拥有 15 亿个参数，在包含 800 万个网页的数据集上进行训练。 GPT-2 的训练目标很简单：根据某个文本中所有先前的单词来预测下一个单词。 数据集的多样性导致这个简单的目标包含跨不同领域的许多任务的自然发生的演示。 GPT-2 是 GPT 的直接扩展，参数增加了 10 倍以上，训练数据量增加了 10 倍以上。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/Infrastructure_HF_Encoder-Decoder-models.html":{"url":"Chap1-ModelArch/Infrastructure_HF_Encoder-Decoder-models.html","title":"Encoder-Decoder 模型","keywords":"","body":"Encoder-Decoder 模型 编码器-解码器模型（也称为序列到序列模型）使用 Transformer 架构的两个部分。 在每个阶段，编码器的注意力层可以访问初始句子中的所有单词，而解码器的注意力层只能访问位于输入中给定单词之前的单词。 这些模型的预训练可以使用编码器或解码器模型的目标来完成，但通常会涉及一些更复杂的内容。 例如，T5 的预训练方法是用一个掩码特殊字符替换随机跨度的文本（可能包含多个单词），然后目标是预测这个掩码单词所替换的文本。 序列到序列模型最适合根据给定输入生成新句子的任务，例如摘要、翻译或生成式问答。 这个模型家族的代表包括: BART T5 BART BART Bart 模型是在 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 中提出。 论文摘要： Bart 使用标准的 seq2seq/机器翻译架构，带有双向编码器（如 BERT）和从左到右的解码器（如 GPT）。 预训练任务涉及随机打乱原始句子的顺序和新颖的填充方案，其中文本跨度被单个掩码标记替换。 BART 在针对文本生成进行微调时特别有效，而且也适用于理解任务。 使用技巧 BART 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 具有编码器和解码器的序列到序列模型。编码器接收的是已损坏版本的 tokens，解码器接收的是原始 tokens（但有一个掩码来隐藏未来的词块，就像普通的变换 transformer 解码器一样）。在编码器的预训练任务中应用了以下变换组合： Token 随机遮掩：与BERT一样，采用随机的 token 并将其替换为 [MASK]。 Token 随机删除：输入中的随机tokens被删除。与token masking不同的是，模型必须决定哪些位置是缺失的输入。 文本填充：使用单个掩码标记遮掩 k 个范围的标记（0 范围对应 [MASK] token 的插入）。 排列句子：以句号(full stop)为单位将文档划分为句子，这些句子进行随机排列。 旋转文档：均匀随机选择一个 token，将文档旋转使得其以该 token 开始。这将训练模型识别文档开头的能力。 T5 T5 T5 模型在 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 中提出。 论文摘要： 在本文中，我们通过引入一个统一的框架来探索 NLP 迁移学习技术的前景，该框架将每个语言问题转换为文本到文本的格式。 我们的系统研究比较了数十种语言理解任务的预训练目标、架构、未标记数据集、迁移方法和其他因素。 通过将我们探索中的见解与规模和新的“巨大的干净爬行语料库”相结合，我们在涵盖摘要、问答、文本分类等的许多基准上取得了最先进的结果。 使用技巧 T5是一个编码器-解码器模型，预先对无监督和监督任务的多任务混合进行训练，并将每个任务转换为文本到文本的格式。T5通过为每个任务相应的输入添加不同的前缀，可以很好地处理各种开箱即用的任务，例如，对于翻译: 将英语翻译成德语: ...，对于摘要: 总结: ...。 预训练包括监督训练和自监督训练。 自监督培训练使用损坏的 tokens，通过随机删除 15% 的 tokens 并用单个哨兵令牌（sentinel tokens）替换它们(如果几个连续的令牌被标记为删除，则整个组被替换为单个哨兵令牌)。编码器的输入是被破坏的句子，解码器的输入是原始句子，目标是由哨兵标记分隔的丢弃标记。 Original text: Thank you for inviting me to your party last week. Inputs: Thank you me to your party week. Targets: for inviting last T5 使用相对标量嵌入。编码器输入填充可以在左侧和右侧完成。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap1-ModelArch/Advanced_Blog_AttentionAttention.html":{"url":"Chap1-ModelArch/Advanced_Blog_AttentionAttention.html","title":"Lilian-Attention?Attention!","keywords":"","body":"注意力机制 Attention? Attention! 简而言之，深度学习中的注意力可以被广义地解释为重要性权重向量：为了预测或推断一个元素，例如图像中的一个像素或句子中的一个单词，我们使用注意力向量来估计它与其他元素的相关性（或 \"关注\"，您可能在许多论文中读到过），并将它们的值之和经注意力向量加权后作为目标的近似值。 为翻译而生 注意力机制的诞生是为了帮助神经机器翻译（NMT）记忆长句。 注意力发明的秘诀是在上下文向量和整个源输入之间创建捷径，而不是从编码器的最后一个隐藏状态中建立一个单一的上下文向量。 这些捷径连接的权重可针对每个输出元素进行定制。 虽然上下文向量可以访问整个输入序列，但我们不必担心遗忘问题。 源代码和目标代码之间的对齐是由上下文向量学习和控制的。从本质上讲，上下文向量需要消耗三项信息： 编码器隐藏状态 解码器隐藏状态 源和目标之间的对齐 一系列注意力机制 概括 下面是几种流行的注意力机制和相应的对齐评分函数的汇总表： 名称 对齐评分函数 引用 Content-base attention score(st,hi)=cosine[st,hi]score(s_t, h_i) = cosine[s_t, h_i]score(st​,hi​)=cosine[st​,hi​] Graves2014 Additive(*) score(st,hi)=vaTtanh(Wa[st−1;hi]score(s_t, h_i) = v^T_atanh(W_a[s_{t-1};h_i]score(st​,hi​)=vaT​tanh(Wa​[st−1​;hi​] Bahdanau2015 Location-Base αt,i=softmax(Wast)\\alpha_{t,i} = softmax(W_as_t)αt,i​=softmax(Wa​st​) Luong2015 General score(st,hi)=stTWahiscore(s_t, h_i) = s^T_tW_ah_iscore(st​,hi​)=stT​Wa​hi​ Luong2015 Dot-Product score(st,hi)=stThiscore(s_t, h_i) = s^T_th_iscore(st​,hi​)=stT​hi​ Luong2015 Scaled Dot-Product(^) score(st,hi)=stThinscore(s_t, h_i) = \\frac{s^T_th_i}{\\sqrt{n}}score(st​,hi​)=n​stT​hi​​ Xu2015 以下是更广泛类别的注意力机制的摘要： 名称 对齐评分函数 引用 Self-Attention(&) 关联同一输入序列的不同位置。理论上，自注意力可以采用上述任何评分函数，但只需将目标序列替换为相同的输入序列即可。 Cheng2016 Global/Soft 关注整个输入状态空间。 Xu2015 Local/Hard 关注输入状态空间部分；即输入图像的一个 patch。 Luong2015 自注意力 自注意力，也称为内部注意力，是一种将单个序列的不同位置相关联的注意力机制，以便计算同一序列的表示。 它已被证明在机器阅读、抽象概括或图像描述生成中非常有用。 在下面的例子中，自注意力机制使我们能够学习当前单词和句子前一部分之间的相关性。 软注意力 vs 硬注意力 本文首先根据注意力是访问整个图像还是仅访问一个补丁，提出了“软”注意力和“硬”注意力之间的区别： 软注意力：学习对齐权重，并将其 \"柔和 \"地置于源图像中的所有 Patch 上；这与 Bahdanau et al., 2015 中的关注类型基本相同。 优点：模型平滑且可微分 缺点：当源输入很大时，成本昂贵 硬注意力：一次只选择要注意的图像的一个 patch 优点：推理时的计算量较少 缺点：该模型是不可微分的，需要更复杂的技术（例如方差减少或强化学习）来训练 全局注意力 vs 局部注意力 Luong, et al., 2015 提出了“全局”和“局部”注意力。 全局注意力类似于软注意力，而局部注意力是硬注意力和软注意力的有趣混合，是对硬注意力的改进，使其可微分：模型首先预测当前目标词的单一对齐位置，然后使用以源位置为中心的窗口计算上下文向量。 Transformer 毫无疑问，“Attention is All you Need” 是 2017 年最具影响力和最有趣的论文之一。 它对软注意力进行了大量改进，使得在没有递归网络单元的情况下进行 seq2seq 建模成为可能。 所提出的 \"transformer\" 模型完全建立在自注意机制上，而不使用序列对齐的递归架构。 Key, Value and Query Transformer 中的主要组件是多头自注意力机制单元。 Transformer 将输入的编码表示视为一组 key-value 对，(K, V)，维度均为 nnn（输入序列长度）； 在 NMT（Neural Machine Translation）的上下文中，键和值都是编码器隐藏状态。 在解码器中，先前的输出被压缩为 query（维度为 mmm 的 Q），并且通过映射此查询以及键和值集来生成下一个输出。 Transformer 采用缩放点积注意力：输出是 value 的加权和，其中分配给每个值的权重由 query 与所有 key 的点积确定： Attention(Q,K,V)=softmax(QK⊤n)V\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}Attention(Q,K,V)=softmax(n​QK⊤​)V 多头注意力 多头机制不是只计算一次注意力，而是多次并行地运行缩放的点积注意力。 独立的注意力输出被简单串联并线性转换为预期维度。 我想这是因为集成总是有帮助的吧？ 根据论文所述，\"多头注意允许模型联合注意来自不同位置的不同表征子空间的信息。 如果只有一个注意力集中的头，平均值就会抑制这一点。\"。 MultiHead(Q,K,V)=[head1;… ;headh]WOwhere headi=Attention(QWiQ,KWiK,VWiV) \\begin{aligned} \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\ \\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i) \\end{aligned} MultiHead(Q,K,V)where headi​​=[head1​;…;headh​]WO=Attention(QWiQ​,KWiK​,VWiV​)​ 编码器 编码器生成基于注意力的表示，能够从潜在无限大的上下文中定位特定的信息。 N=6 个相同层的堆叠。 每层都有一个多头自注意力层和一个简单的位置全连接前馈网络 每个子层都采用残差连接和层归一化。所有子层输出相同维度 dmodel=512d_{model} = 512dmodel​=512 的数据。 解码器 解码器能够从编码表示中检索。 N = 6 个相同层的堆叠 每层都有两个多头注意力机制子层和一个全连接前馈网络子层。[ 与编码器类似，每个子层都采用残差连接和层归一化。 第一个多头注意子层被修改以防止位置关注后续位置，因为我们不希]()望在预测当前位置时关注未来的目标序列。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap2-TrainingTech/":{"url":"Chap2-TrainingTech/","title":"训练技术","keywords":"","body":"训练技术 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap3-PromptEngr/":{"url":"Chap3-PromptEngr/","title":"Prompt 工程","keywords":"","body":"Prompt 工程 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html":{"url":"Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html","title":"宝玉老师-如何写好提示词？","keywords":"","body":"宝玉老师的这个关于提示词编写的介绍，个人觉得收益匪浅。 How to write good prompt 概要 提示词的四大要素：指令、上下文、输出格式、角色 提示词策略和技巧 大语言模型的本质和劣势 提升生成期望结果的概率 策略一：撰写清晰的指令 策略二：给模型“思考”的时间 策略三：把复杂任务拆分成简单任务 策略四：接入外部工具和资料 提示词技巧的具体应用案例 提示词的四大要素 提示词是与 AI 交互时的重要工具，它包含四大要素：指令、上下文、输出格式和角色。 指令：是告诉 AI 需要完成的具体任务或回答的问题，需要清晰具体。 上下文：是 AI 执行指令所需的背景信息，它帮助 AI 更好地理解和执行指令。 输出格式：是我们期望 AI 返回结果的格式，比如表格、摘要或 Markdown 格式等。 角色：则是我们期望 AI 在交互中扮演的角色，比如助手、心理医生或导师等，这有助于 AI 更准确地生成内容。 提示词策略和技巧 有效的提示词策略和技巧可以极大提升与 AI 的交互质量。 首先，我们需要撰写清晰的指令，因为 AI 无法猜测我们的想法。 其次，给模型“思考”的时间，通过展示思考过程来提升推理结果。 此外，将复杂任务拆分成简单任务，可以提高任务的成功率。 最后，接入外部工具和资料，可以弥补 AI 知识库的不足，提升其性能。 大语言模型的本质和劣势 大语言模型以其强大的语言理解和生成能力著称，它们拥有简单的推理能力，并且适应性强，可以通过微调或上下文学习来提升性能。 然而，这些模型也有其局限性，比如对事实的理解有限、知识库有限、缺乏深层推理能力，以及上下文窗口长度限制和没有记忆等。 了解这些本质和劣势有助于我们更好地利用 AI。 提升生成期望结果的概率 为了提升生成期望结果的概率，我们需要发挥 AI 模型的优势，补充或避开其劣势。 这包括让 AI 准确理解指令，以及让概率收敛在高质量的路径上。 通过精心设计的提示词，我们可以引导 AI 生成更准确的结果。 策略一：撰写清晰的指令 在与 AI 的交互中，清晰的指令至关重要。 模型不会读心术，因此需要我们提供具体明确的指令。 此外，提供详尽的背景信息可以帮助模型更好地理解任务。 让 AI 向我们提问可以避免信息的遗漏，而让 AI 扮演任务相关的角色则可以提升特定领域内容的生成概率。 策略二：给模型“思考”的时间 在处理复杂问题时，给模型“思考”的时间是提升推理结果的有效策略。 这可以通过打印思考过程来实现，帮助模型更可靠地推导出正确答案。 此外，用 Token 换时间，即在模型给出答案之前，要求其展示一下“思考过程”，有助于模型更准确地回答问题。 策略三：把复杂任务拆分成简单任务 将复杂任务拆分成简单任务可以提高任务的成功率。 这类似于软件工程中将复杂系统分解成模块化的组件。 通过化繁为简，我们可以提高每个子任务的成功率，并且可以合并回去，使得前一个任务的输出可以作为后一个任务的输入。 策略四：接入外部工具和资料 在 AI 的能力不足以完成任务时，我们可以借助外部工具和资料。 通过上下文学习，我们可以弥补知识库的不足。 提供工具的说明和接口参数给 AI，可以让 AI 自己选择使用什么工具，传入什么参数。 此外，AI 不擅长数学，但可以调用数学工具或者执行代码来完成任务。 提示词技巧的具体应用案例 在实际应用中，我们可以通过结构化输入、明确说明完成任务的步骤、给出样例（few-shot）以及使用伪代码等技巧来提升提示词的效果。 这些技巧可以帮助 AI 更准确地理解我们的需求，并生成更符合预期的结果。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap3-PromptEngr/prompt-app_openai-prompt-generation.html":{"url":"Chap3-PromptEngr/prompt-app_openai-prompt-generation.html","title":"OpenAI-生成提示词的提示词","keywords":"","body":"OpeAI 公布的生成提示词的提示词技术。 Prompt generation 提示词 元提示指示模型根据您的任务描述创建良好的提示或改进现有提示。 META_PROMPT = \"\"\" Given a task description or existing prompt, produce a detailed system prompt to guide a language model in completing the task effectively. # Guidelines - Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output. - Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure. - Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS! - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed. - Conclusion, classifications, or results should ALWAYS appear last. - Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements. - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders. - Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements. - Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED. - Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user. - Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples. - Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.) - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON. - JSON should never be wrapped in code blocks (```) unless explicitly requested. The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\") [Concise instruction describing the task - this should be the first line in the prompt, no section header] [Additional details as needed.] [Optional sections with headings or bullet points for detailed steps.] # Steps [optional] [optional: a detailed breakdown of the steps necessary to accomplish the task] # Output Format [Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc] # Examples [optional] [Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.] [If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ] # Notes [optional] [optional: edge cases, details, and an area to call or repeat out specific important considerations] \"\"\".strip() def generate_prompt(task_or_prompt: str): completion = client.chat.completions.create( model=\"gpt-4o\", messages=[ { \"role\": \"system\", \"content\": META_PROMPT, }, { \"role\": \"user\", \"content\": \"Task, Goal, or Current Prompt:\\n\" + task_or_prompt, }, ], ) return completion.choices[0].message.content Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap4-InferAndOpt/":{"url":"Chap4-InferAndOpt/","title":"推理与优化","keywords":"","body":"推理与优化 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "},"Chap5-App/":{"url":"Chap5-App/","title":"应用方向","keywords":"","body":"应用方向 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-15 10:15:14 "}}