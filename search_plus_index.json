{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction 本 Gitbook 搜集和分享个人喜欢的大语言模型（LLM）相关资源的在线百科全书。 涵盖 LLM 的基础知识、研究进展、技术应用、工具和最佳实践。 大语言模型（LLM）学习路径和资料汇总 Embedding Model Fine-Tuning 提示工程相关资料： Anthropic 提示工程概览 Claude 提示库 OpenAI 提示工程指南 OpenAI 提示示例 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-19 16:56:04 "},"Chap1-ModelArch/":{"url":"Chap1-ModelArch/","title":"模型结构","keywords":"","body":"模型结构 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.html","title":"Transformer 可视化解释","keywords":"","body":"Transformer Expander 是一种交互式可视化工具，旨在帮助任何人了解 GPT 等基于 Transformer 的模型如何工作。 它直接在您的浏览器中运行实时 GPT-2 模型，允许您试验自己的文本并实时观察 Transformer 的内部组件和操作如何协同工作以预测下一个令牌。 transformer-explainer transformer-explainer-demo 什么是 Transformer？ 从根本上来说，文本生成 Transformer 模型的运行原理是下一个单词预测：给定用户的文本提示，该输入之后最有可能出现的下一个单词是什么？ Transformer 的核心创新和强大之处在于它们使用自注意力机制，这使得它们能够比以前的架构更有效地处理整个序列并捕获远程依赖关系。 Transformer 结构 每个文本生成 Transformer 都包含以下三个关键组件： Embedding Transformer 块: 注意力机制: Transformer模块的核心组件。它允许标记与其他标记进行通信，捕获上下文信息和单词之间的关系。 多层感知器层: 一个独立运行在每个令牌上的前馈网络。注意力层的目标是在 token 之间路由信息，而 MLP 的目标是细化每个 token 的表示。 输出概率: 最后的线性层和 softmax 层将处理后的嵌入转换为概率，使模型能够预测序列中的下一个标记。 Embedding 嵌入的用武之地：它将文本转换为模型可以使用的数字表示。 要将提示转换为嵌入，我们需要: 1) 对输入进行标记; 2) 获取标记嵌入; 3) 添加位置信息，最后 4) 将标记和位置编码相加以获得最终嵌入。 Step1: Tokenization 标记化是将输入文本分解为更小、更易于管理的片段（称为标记）的过程。 这些标记可以是单词或子单词。 “数据”和“可视化”一词对应于独特的标记，而“授权”一词则分为两个标记。 令牌的完整词汇表在训练模型之前确定：GPT-2 的词汇表有 50,257 个唯一令牌。 现在我们将输入文本分割成具有不同 ID 的标记，我们可以从嵌入中获取它们的向量表示。 Step2: Token Embedding GPT-2 Small 将词汇表中的每个 token 表示为 768 维向量；向量的维度取决于模型。 这些嵌入向量存储在形状为 (50,257, 768) 的矩阵中，包含大约 3900 万个参数！ 这个广泛的矩阵允许模型为每个标记分配语义。 Step3: 位置编码 嵌入层还对输入提示中每个标记的位置信息进行编码。不同的模型使用不同的位置编码方法。 GPT-2 从头开始训练自己的位置编码矩阵，并将其直接集成到训练过程中。 Step4: 最终 Embedding 最后，我们将令牌和位置编码相加以获得最终的嵌入表示。这种组合表示捕获了标记的语义及其在输入序列中的位置。 Transformer 块 Transformer 处理的核心在于 Transformer 块，其中包括多头自注意力和多层感知器层。 大多数模型由多个这样的块组成，这些块按顺序依次堆叠。 Token 表示从第一个区块到第 12 个区块逐层演变，使模型能够建立对每个代币的复杂理解。 这种分层方法导致输入的高阶表示。 多头注意力 Query, Key and Value 矩阵 Masked Self-Attention Attention Score Masking Softmax Output: 该模型使用屏蔽的自注意力分数并将其与值矩阵相乘以获得自注意力机制的最终输出。 MLP 在多个自注意力头捕获输入标记之间的不同关系后，连接的输出将通过多层感知器（MLP）层，以增强模型的表示能力。 MLP 块由两个线性变换组成，中间有一个 GELU 激活函数。 第一个线性变换将输入的维度从 768 增加四倍到 3072。 第二个线性变换将维度降低回原始大小 768，确保后续层接收一致维度的输入。 与自注意力机制不同，MLP 独立处理 token，并简单地将它们从一种表示映射到另一种表示。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/transformer_arch/Infrastructure_luxiangdong_Transformer-OverallArch.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_luxiangdong_Transformer-OverallArch.html","title":"土猛的员外-Transformer 架构的整体指南","keywords":"","body":" Transformer 架构的整体指南 Transformer架构的核心组件 Transformer 架构的核心在于注意力机制，它包括编码器（Encoder）和解码器（Decoder）两个主要部分。 编码器负责将输入序列转换为压缩表示，而解码器则基于编码器的输出生成目标序列。 这一架构还包括全连接层、归一化层、Embedding 层和位置编码层等组件，共同作用于提高模型的性能和泛化能力。 特别是注意力机制，它允许模型动态地聚焦于输入序列中最重要的部分，从而提升模型对信息的处理能力。 Encoder和Decoder的结构与功能 Transformer 模型中的编码器（Encoder）由多个相同的层组成，每层都包含多头自注意力（MHA）和前馈神经网络（MLP）。 编码器的主要任务是理解输入序列，并将其编码成一个连续的表示，这个表示能够捕捉输入数据的深层语义信息。 解码器（Decoder）的结构与编码器类似，但它还包括一个额外的多头注意力层，用于融合编码器的输出和目标序列的信息。 解码器的主要任务是生成与输入序列相对应的输出序列，例如在机器翻译任务中的翻译文本。 注意力机制的详细解释 注意力机制是 Transformer 模型的核心，它允许模型在处理序列时动态地聚焦于不同的部分。 注意力机制涉及查询（Query）、键（Key）和值（Value）三个主要组件，通过计算查询和所有键的匹配程度来确定每个值的重要性。 这种机制使得模型能够捕捉序列内部的长距离依赖关系，并在生成输出时考虑到整个输入序列的上下文信息。 多头注意力和MLPs的作用 多头注意力（Multi-Head Attention）是 Transformer 模型中的一个关键特性，它允许模型在不同的表示子空间中并行地捕捉信息，从而提高了模型对复杂关系的理解能力。 MLPs（多层感知器）作为 Transformer 中的另一个子层，负责处理序列中的每个位置，它们通过非线性变换增强模型的表达能力。 Embeddings和位置编码层的重要性 在 Transformer 模型中，输入序列首先被转换为嵌入（Embeddings），这些嵌入是固定大小的密集向量，能够捕捉单词或token的语义信息。 位置编码（Positional Encoding）层则用于保持序列中单词的顺序信息，因为 Transformer 模型的自注意力机制本身不具备对序列顺序的感知能力。 残差连接、层规范化和Dropout的讨论 为了提高模型的训练效率和稳定性，Transformer 模型采用了残差连接（Residual Connections）和层规范化（Layer Normalization）。 残差连接允许模型在每一层中直接传播信息，而层规范化则有助于稳定训练过程。 Dropout 作为一种正则化技术，被用于防止模型过拟合，提高模型的泛化能力。 线性和Softmax图层的功能 在解码器的最后，线性层（Linear Layer）将解码器的激活投影到词汇表的大小，产生对数概率。 Softmax 层（Softmax Layer）则将这些对数概率转换为下一个 token 的概率分布，使得模型能够预测序列中的下一个 token。 注意力机制的可视化和解释 通过可视化工具，我们可以直观地看到 Transformer 模型中的注意力权重，这有助于我们理解模型在处理输入序列时关注的重点。 这种可视化不仅增强了模型的可解释性，也为我们提供了洞察模型内部工作机制的手段。 注意力机制的优势和挑战 注意力机制带来了许多优势，包括更好地处理长期依赖关系、提高并行化能力、增强模型的可解释性以及在多个任务中提高性能。 然而，它也面临着挑战，如随着序列长度增加而增长的内存消耗和计算成本，以及在推理过程中可能需要的顺序方法。 大型语言模型的演变和设计 大型语言模型（LLMs）是 Transformer 模型的直接扩展，它们通过在大量文本数据上进行预训练，获得了强大的语言理解和生成能力。 这些模型的设计包括纯编码器模型（如 BERT）、纯解码器模型（如 GPT 系列）和编码器-解码器模型（如 T5）。 这些模型在不同的任务和应用中展现出了卓越的性能。 Encoder、Decoder 和 Encoder-Decoder 大语言模型的比较 不同类型的大型语言模型根据其架构设计适用于不同的任务。 纯编码器模型适合于 NLP 的判别任务，如文本分类；纯解码器模型适合于生成任务，如文本续写； 编码器-解码器模型则适用于需要处理两个序列的任务，如机器翻译。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-models.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-models.html","title":"Encoder 模型","keywords":"","body":"编码器模型 Encoder models 编码器模型仅使用 Transformer 模型的编码器。 在每个阶段，注意力层都可以访问初始句子中的所有单词。 这些模型通常被描述为具有“双向”注意力，并且通常被称为自编码模型。 这些模型的预训练通常围绕着以某种方式破坏给定的句子（例如，通过屏蔽其中的随机单词）并要求模型查找或重建初始句子。 编码器模型最适合需要理解完整句子的任务，例如句子分类、命名实体识别（以及更一般的单词分类）和提取式问答。 这个模型家族的代表包括: BERT ALBERT DistillBERT ELECTRA RoBERTa BERT BERT BERT 模型在 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 中提出。 它是一个双向 Transformer，使用掩码语言建模（masked language modeling）目标和下一句预测（next sentence prediction）相结合的方式对大型语料库进行预训练。 使用技巧 BERT 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 BERT 使用掩码语言模型 (MLM) 和下一句预测 (NSP) 目标进行训练。一般来说，它在预测屏蔽标记和 NLU 方面非常有效，但对于文本生成来说并不是最佳选择。 通过使用随机屏蔽（random masking）来破坏输入，更准确地说，在预训练期间，给定百分比的标记（通常为 15%）被以下方式屏蔽： 标记有 0.8 的概率打上特殊掩码 token 标记有 0.1 的概率打上随机 token（与原始标记不同） 标记有 0.1 的概率使用原始 token 该模型必须预测原始句子，但还有第二个目标：输入是两个句子 A 和 B（中间有一个分隔标记）。句子在语料库中连续的概率为 50%，其余 50% 的句子不相关。该模型必须预测句子是否连续。 ALBERT ALBERT ALBERT 模型在 《ALBERT：A Lite BERT for Self-supervised Learning of Language Representations》 中提出。 它提出了两种参数减少技术来降低内存消耗并提高 BERT 的训练速度： 将嵌入矩阵拆分为两个较小的矩阵 使用重复层（repeating layers）在组之间进行分割 ALBERT 还使用了一种自我监督损失，重点是对句子间的连贯性进行建模，实验表明这始终有助于具有多句子输入的下游任务。 使用技巧 ALBERT 是一个具有绝对位置嵌入的模型，因此通常建议将输入填充在右侧而不是左侧。 ALBERT 使用重复层，这会导致内存占用较小，但计算成本仍然类似于具有相同数量隐藏层的类 BERT 架构，因为它必须迭代相同数量的（重复）层。 层被划分为共享参数的组(以节省内存)。下一个句子预测被一个句子顺序预测所取代: 在输入中，我们有两个句子 A 和 B (是连续的) ，我们要么输入 A 后面跟着 B，要么输入 B 后面跟着 A。模型必须预测它们是否被交换了。 DistilBERT DistilBERT DistilBERT 在 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter 中提出。 DistilBERT 是一个通过蒸馏 BERT 基础训练而成的小型、快速、廉价且轻量的 Transformer 模型。 在这项工作中，作者提出了一种预训练较小的通用语言表示模型的方法，称为 DistilBERT，然后可以对其进行微调，使其在广泛的任务（如其较大的对应任务）上具有良好的性能。 虽然大多数先前的工作研究了如何使用蒸馏来构建特定于任务的模型，但作者在预训练阶段利用了知识蒸馏，并表明可以将 BERT 模型的大小减少 40%，同时保留 97% 的语言理解能力，速度提高 60%。 为了利用预训练期间较大模型学到的归纳偏差，作者引入了结合语言建模、蒸馏和余弦距离损失的三重损失。 使用技巧 (HF) DistilBERT 没有 token_type_ids，不需要指示哪个 token 属于哪个段。只需使用分隔标记 tokenizer.sep_token （或 [SEP]）分隔片段。 DistilBERT 没有选择输入位置（position_ids 输入）的选项。 与 BERT 相同但更小。通过预训练 BERT 模型的蒸馏进行训练，这意味着它经过训练可以预测与较大模型相同的概率。实际目标是以下各项的组合： 找到与教师模型（Teacher Model）相同的概率 正确预测屏蔽标记（但没有下一句目标） 学生模型和教师模型的隐藏状态之间的余弦相似度 ELECTRA ELECTRA ELECTRA 模型是在 ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators 中提出的。 ELECTRA 是一种新的预训练方法，它训练两个 Transformer 模型：生成器（generator）和判别器（discriminator）。 生成器的作用是替换序列中的标记，因此被训练为掩码语言模型。 判别器是我们感兴趣的模型，它尝试识别序列中哪些标记被生成器替换。 论文摘要： 掩码语言建模 (MLM) 预训练方法（例如 BERT）通过用 [MASK] 替换一些标记来破坏输入，然后训练模型来重建原始标记。 虽然它们在转移到下游 NLP 任务时会产生良好的结果，但它们通常需要大量计算才能有效。 作为替代方案，我们提出了一种样本效率更高的预训练任务，称为替换令牌检测。 我们的方法不是屏蔽输入，而是通过用从小型生成器网络采样的合理替代方案替换一些令牌来破坏输入。 然后，我们不是训练一个预测损坏令牌原始身份的模型，而是训练一个判别模型来预测损坏输入中的每个令牌是否被生成器样本替换。 彻底的实验证明这个新的预训练任务比 MLM 更有效，因为该任务是在所有输入标记上定义的，而不仅仅是被屏蔽的小子集。 使用技巧 ELECTRA 是预训练方法，因此对底层模型 BERT 几乎没有任何改变。唯一的变化是嵌入大小和隐藏大小的分离：嵌入大小通常较小，而隐藏大小较大。附加的投影层（线性）用于将嵌入从嵌入大小投影到隐藏大小。在嵌入大小与隐藏大小相同的情况下，不使用投影层。 ELECTRA 是一个 Transformer 模型，使用另一个（小型）掩码语言模型进行预训练。输入被该语言模型破坏，它接受随机屏蔽的输入文本，并输出一个文本，ELECTRA 必须预测其中哪个标记是原始标记，哪个标记被替换。与 GAN 训练一样，先对小语言模型进行几步训练（但目标是原始文本，而不是像传统的 GAN 设置那样愚弄 ELECTRA 模型），然后再对 ELECTRA 模型进行几步训练。 RoBERTa [RoBERTa(https://huggingface.co/docs/transformers/model_doc/roberta) RoBERTa 模型在 RoBERTa: A Robustly Optimized BERT Pretraining Approach 中提出。 它建立在 BERT 的基础上，修改了关键的超参数，删除了下一句话预训练目标，并使用更大的小批量和学习率进行训练。 论文摘要： 我们提出了 BERT 预训练的复制研究（Devlin 等人，2019），该研究仔细测量了许多关键超参数和训练数据大小的影响。 我们发现 BERT 的训练明显不足，但可以匹配或超过其之后发布的每个模型的性能。 实验结果凸显了以前被忽视的设计选择的重要性， 使用技巧 此实现与 BertModel 相同，只是对嵌入进行了细微调整，以及 RoBERTa 预训练模型的设置。 RoBERTa 具有与 BERT 相同的架构，但使用字节级 BPE （byte-level BPE）作为 tokenizer（与 GPT-2 相同）并使用不同的预训练方案。 RoBERTa 与 BERT 类似，但具有更好的预训练技术： 动态屏蔽：令牌在每个时期的屏蔽方式不同，而 BERT 则一劳永逸。 句子打包（Sentence packing）：句子打包在一起达到 512 个标记（因此句子的顺序可能跨越多个文档）。 较大批次：训练使用较大批次。 字节级 BPE 词汇：使用 BPE，将字节作为子单元，而不是字符，以适应 Unicode 字符。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Decoder-models.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Decoder-models.html","title":"Decoder 模型","keywords":"","body":"解码器模型 解码器模型仅使用 Transformer 模型的解码器。 在每个阶段，对于给定的单词，注意力层只能访问句子中位于该单词之前的单词。 这些模型通常称为自回归模型。 解码器模型的预训练通常围绕预测句子中的下一个单词进行。 这些模型最适合涉及文本生成的任务。 这个模型家族的代表包括: CTRL GPT GPT-2 CTRL CTRL 模型在 CTRL: A Conditional Transformer Language Model for Controllable Generation 中提出。 它是一个因果（单向）转换器，使用语言模型在大约 140 GB 文本数据的非常大的语料库上进行预训练，第一个标记保留为控制代码（例如链接、书籍、维基百科等）。 论文摘要： 这是一个条件 Transformer 模型，经过训练以控制样式、内容和特定任务行为的控制代码为条件。 控制代码源自与原始文本自然共存的结构，保留了无监督学习的优势，同时提供对文本生成的更明确的控制。 这些代码还允许 CTRL 预测训练数据的哪些部分最有可能给出序列。 这提供了一种通过基于模型的源归因来分析大量数据的潜在方法。 使用技巧 CTRL 利用控制代码来生成文本：它需要从某些单词、句子或链接开始生成，以生成连贯的文本。 CTRL 是一个具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 CTRL 经过因果语言建模 (CLM) 目标的训练，因此在预测序列中的下一个标记方面非常强大。利用此功能，CTRL 可以生成语法连贯的文本。 OpenAI GPT OpenAI GPT 模型在 Improving Language Understanding by Generative Pre-Training 中提出。 它是一个因果（单向）转换器，使用具有长范围依赖性的大型语料库（多伦多图书语料库）上的语言建模进行预训练。 论文摘要： 实验证明，通过在各种未标记文本的语料库上对语言模型进行生成式预训练，然后对每个特定任务进行区分性微调，可以实现这些任务的巨大收益。 与以前的方法相比，我们在微调过程中利用任务感知输入转换来实现有效的传输，同时需要对模型架构进行最小的更改。 使用技巧 GPT 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 GPT 使用因果语言建模 (CLM) 目标进行训练，因此在预测序列中的下一个标记方面非常强大。利用此功能，GPT 可以生成语法连贯的文本。 OpenAI GPT-2 模型在 Language Models are Unsupervised Multitask Learners 中提出。 它是一个因果（单向）Transformer，使用语言建模在约 40 GB 文本数据的非常大的语料库上进行预训练。 论文摘要： GPT-2 是一个基于 Transformer 的大型语言模型，拥有 15 亿个参数，在包含 800 万个网页的数据集上进行训练。 GPT-2 的训练目标很简单：根据某个文本中所有先前的单词来预测下一个单词。 数据集的多样性导致这个简单的目标包含跨不同领域的许多任务的自然发生的演示。 GPT-2 是 GPT 的直接扩展，参数增加了 10 倍以上，训练数据量增加了 10 倍以上。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-Decoder-models.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-Decoder-models.html","title":"Encoder-Decoder 模型","keywords":"","body":"Encoder-Decoder 模型 编码器-解码器模型（也称为序列到序列模型）使用 Transformer 架构的两个部分。 在每个阶段，编码器的注意力层可以访问初始句子中的所有单词，而解码器的注意力层只能访问位于输入中给定单词之前的单词。 这些模型的预训练可以使用编码器或解码器模型的目标来完成，但通常会涉及一些更复杂的内容。 例如，T5 的预训练方法是用一个掩码特殊字符替换随机跨度的文本（可能包含多个单词），然后目标是预测这个掩码单词所替换的文本。 序列到序列模型最适合根据给定输入生成新句子的任务，例如摘要、翻译或生成式问答。 这个模型家族的代表包括: BART T5 BART BART Bart 模型是在 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 中提出。 论文摘要： Bart 使用标准的 seq2seq/机器翻译架构，带有双向编码器（如 BERT）和从左到右的解码器（如 GPT）。 预训练任务涉及随机打乱原始句子的顺序和新颖的填充方案，其中文本跨度被单个掩码标记替换。 BART 在针对文本生成进行微调时特别有效，而且也适用于理解任务。 使用技巧 BART 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 具有编码器和解码器的序列到序列模型。编码器接收的是已损坏版本的 tokens，解码器接收的是原始 tokens（但有一个掩码来隐藏未来的词块，就像普通的变换 transformer 解码器一样）。在编码器的预训练任务中应用了以下变换组合： Token 随机遮掩：与BERT一样，采用随机的 token 并将其替换为 [MASK]。 Token 随机删除：输入中的随机tokens被删除。与token masking不同的是，模型必须决定哪些位置是缺失的输入。 文本填充：使用单个掩码标记遮掩 k 个范围的标记（0 范围对应 [MASK] token 的插入）。 排列句子：以句号(full stop)为单位将文档划分为句子，这些句子进行随机排列。 旋转文档：均匀随机选择一个 token，将文档旋转使得其以该 token 开始。这将训练模型识别文档开头的能力。 T5 T5 T5 模型在 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 中提出。 论文摘要： 在本文中，我们通过引入一个统一的框架来探索 NLP 迁移学习技术的前景，该框架将每个语言问题转换为文本到文本的格式。 我们的系统研究比较了数十种语言理解任务的预训练目标、架构、未标记数据集、迁移方法和其他因素。 通过将我们探索中的见解与规模和新的“巨大的干净爬行语料库”相结合，我们在涵盖摘要、问答、文本分类等的许多基准上取得了最先进的结果。 使用技巧 T5是一个编码器-解码器模型，预先对无监督和监督任务的多任务混合进行训练，并将每个任务转换为文本到文本的格式。T5通过为每个任务相应的输入添加不同的前缀，可以很好地处理各种开箱即用的任务，例如，对于翻译: 将英语翻译成德语: ...，对于摘要: 总结: ...。 预训练包括监督训练和自监督训练。 自监督培训练使用损坏的 tokens，通过随机删除 15% 的 tokens 并用单个哨兵令牌（sentinel tokens）替换它们(如果几个连续的令牌被标记为删除，则整个组被替换为单个哨兵令牌)。编码器的输入是被破坏的句子，解码器的输入是原始句子，目标是由哨兵标记分隔的丢弃标记。 Original text: Thank you for inviting me to your party last week. Inputs: Thank you me to your party week. Targets: for inviting last T5 使用相对标量嵌入。编码器输入填充可以在左侧和右侧完成。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/attention/Advanced_Blog_AttentionAttention.html":{"url":"Chap1-ModelArch/attention/Advanced_Blog_AttentionAttention.html","title":"Lilian-Attention?Attention!","keywords":"","body":"注意力机制 Attention? Attention! 简而言之，深度学习中的注意力可以被广义地解释为重要性权重向量：为了预测或推断一个元素，例如图像中的一个像素或句子中的一个单词，我们使用注意力向量来估计它与其他元素的相关性（或 \"关注\"，您可能在许多论文中读到过），并将它们的值之和经注意力向量加权后作为目标的近似值。 为翻译而生 注意力机制的诞生是为了帮助神经机器翻译（NMT）记忆长句。 注意力发明的秘诀是在上下文向量和整个源输入之间创建捷径，而不是从编码器的最后一个隐藏状态中建立一个单一的上下文向量。 这些捷径连接的权重可针对每个输出元素进行定制。 虽然上下文向量可以访问整个输入序列，但我们不必担心遗忘问题。 源代码和目标代码之间的对齐是由上下文向量学习和控制的。从本质上讲，上下文向量需要消耗三项信息： 编码器隐藏状态 解码器隐藏状态 源和目标之间的对齐 一系列注意力机制 概括 下面是几种流行的注意力机制和相应的对齐评分函数的汇总表： 名称 对齐评分函数 引用 Content-base attention score(st,hi)=cosine[st,hi]score(s_t, h_i) = cosine[s_t, h_i]score(st​,hi​)=cosine[st​,hi​] Graves2014 Additive(*) score(st,hi)=vaTtanh(Wa[st−1;hi]score(s_t, h_i) = v^T_atanh(W_a[s_{t-1};h_i]score(st​,hi​)=vaT​tanh(Wa​[st−1​;hi​] Bahdanau2015 Location-Base αt,i=softmax(Wast)\\alpha_{t,i} = softmax(W_as_t)αt,i​=softmax(Wa​st​) Luong2015 General score(st,hi)=stTWahiscore(s_t, h_i) = s^T_tW_ah_iscore(st​,hi​)=stT​Wa​hi​ Luong2015 Dot-Product score(st,hi)=stThiscore(s_t, h_i) = s^T_th_iscore(st​,hi​)=stT​hi​ Luong2015 Scaled Dot-Product(^) score(st,hi)=stThinscore(s_t, h_i) = \\frac{s^T_th_i}{\\sqrt{n}}score(st​,hi​)=n​stT​hi​​ Xu2015 以下是更广泛类别的注意力机制的摘要： 名称 对齐评分函数 引用 Self-Attention(&) 关联同一输入序列的不同位置。理论上，自注意力可以采用上述任何评分函数，但只需将目标序列替换为相同的输入序列即可。 Cheng2016 Global/Soft 关注整个输入状态空间。 Xu2015 Local/Hard 关注输入状态空间部分；即输入图像的一个 patch。 Luong2015 自注意力 自注意力，也称为内部注意力，是一种将单个序列的不同位置相关联的注意力机制，以便计算同一序列的表示。 它已被证明在机器阅读、抽象概括或图像描述生成中非常有用。 在下面的例子中，自注意力机制使我们能够学习当前单词和句子前一部分之间的相关性。 软注意力 vs 硬注意力 本文首先根据注意力是访问整个图像还是仅访问一个补丁，提出了“软”注意力和“硬”注意力之间的区别： 软注意力：学习对齐权重，并将其 \"柔和 \"地置于源图像中的所有 Patch 上；这与 Bahdanau et al., 2015 中的关注类型基本相同。 优点：模型平滑且可微分 缺点：当源输入很大时，成本昂贵 硬注意力：一次只选择要注意的图像的一个 patch 优点：推理时的计算量较少 缺点：该模型是不可微分的，需要更复杂的技术（例如方差减少或强化学习）来训练 全局注意力 vs 局部注意力 Luong, et al., 2015 提出了“全局”和“局部”注意力。 全局注意力类似于软注意力，而局部注意力是硬注意力和软注意力的有趣混合，是对硬注意力的改进，使其可微分：模型首先预测当前目标词的单一对齐位置，然后使用以源位置为中心的窗口计算上下文向量。 Transformer 毫无疑问，“Attention is All you Need” 是 2017 年最具影响力和最有趣的论文之一。 它对软注意力进行了大量改进，使得在没有递归网络单元的情况下进行 seq2seq 建模成为可能。 所提出的 \"transformer\" 模型完全建立在自注意机制上，而不使用序列对齐的递归架构。 Key, Value and Query Transformer 中的主要组件是多头自注意力机制单元。 Transformer 将输入的编码表示视为一组 key-value 对，(K, V)，维度均为 nnn（输入序列长度）； 在 NMT（Neural Machine Translation）的上下文中，键和值都是编码器隐藏状态。 在解码器中，先前的输出被压缩为 query（维度为 mmm 的 Q），并且通过映射此查询以及键和值集来生成下一个输出。 Transformer 采用缩放点积注意力：输出是 value 的加权和，其中分配给每个值的权重由 query 与所有 key 的点积确定： Attention(Q,K,V)=softmax(QK⊤n)V\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}Attention(Q,K,V)=softmax(n​QK⊤​)V 多头注意力 多头机制不是只计算一次注意力，而是多次并行地运行缩放的点积注意力。 独立的注意力输出被简单串联并线性转换为预期维度。 我想这是因为集成总是有帮助的吧？ 根据论文所述，\"多头注意允许模型联合注意来自不同位置的不同表征子空间的信息。 如果只有一个注意力集中的头，平均值就会抑制这一点。\"。 MultiHead(Q,K,V)=[head1;… ;headh]WOwhere headi=Attention(QWiQ,KWiK,VWiV) \\begin{aligned} \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\ \\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i) \\end{aligned} MultiHead(Q,K,V)where headi​​=[head1​;…;headh​]WO=Attention(QWiQ​,KWiK​,VWiV​)​ 编码器 编码器生成基于注意力的表示，能够从潜在无限大的上下文中定位特定的信息。 N=6 个相同层的堆叠。 每层都有一个多头自注意力层和一个简单的位置全连接前馈网络 每个子层都采用残差连接和层归一化。所有子层输出相同维度 dmodel=512d_{model} = 512dmodel​=512 的数据。 解码器 解码器能够从编码表示中检索。 N = 6 个相同层的堆叠 每层都有两个多头注意力机制子层和一个全连接前馈网络子层。[ 与编码器类似，每个子层都采用残差连接和层归一化。 第一个多头注意子层被修改以防止位置关注后续位置，因为我们不希]()望在预测当前位置时关注未来的目标序列。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap1-ModelArch/attention/mha-related.html":{"url":"Chap1-ModelArch/attention/mha-related.html","title":"缓存优化与效果-KV","keywords":"","body":"缓存与效果——结构优化 在 Transformer 解码器中，由于 token 的注意力依赖于前面的 token，因此，与其重新计算前面的上下文，不如缓存其 Key 和 Value。 这可以显著加速推理速度，但随着序列长度和模型维度的增长（dim 和 layers），可能会带来昂贵的内存开销。 在这种背景下，引入了多种注意力机制（为了尽可能支持更大的模型或者更长的序列，需要对 kv 进行压缩）： Multi-Head Attention (MHA) Multi-Query Attention (MQA) Grouped-Query Attention (GQA) Multi-Head Latent Attention (MLA) MHA 标准多头注意力（MHA）计算每个注意力头的 query、key 和 value 矩阵。 Ot,iO_{t, i}Ot,i​ 是第 iii 个注意力头的输出。在推理过程中，所有 key 和 value 都会被缓存以加快推理速度。 但这种繁重的 KV 缓存是一个很大的瓶颈，会限制最大序列长度和批量大小。 MQA 为了缓解 MHA 中的 KV-cache 瓶颈，Shazeer 引入了 Multi-Query Attention (MQA)，其中 key 和 value 在所有不同的注意力头之间共享。 这只需要非常轻量的 KV-cache，从而大大加快解码器推理速度。 然而，MQA 会导致质量下降和训练不稳定。 使用 MQA 的模型包括 PaLM、Gemini 等。 GQA Grouped-Query Attention (GQA) 是 MHA 和 MQA 之间的插值，通过引入多个查询头子组（少于注意力头总数），每个子组都有一个 key 和 value 头。 与 MQA 相比，随着模型大小的增加，GQA 的内存带宽和容量保持相同比例的减少。 中间数量的子组会产生比 MQA 质量更高但比 MHA 更快的插值模型。 MLA Multi-Head Latent Attention (MLA) 实现了比 MHA 更优越的性能，并且显著降低了 KV-cache 提升推理效率。 MLA 不像 MQA 和 GQA 那样减少 KV-heads, 而是将 Key 和 Value 联合压缩到一个潜在向量中。 Low-Rank Key-Value Joint Compression MLA 将 key 和 value 矩阵联合压缩在低秩向量中，这样可以缓存更少的项目，因为压缩维度比 MHA 中的输出投影矩阵维度要小得多。 总结 Attention Mechanism KV Cache per Token (# Element) Capability Multi-Head Attention (MHA) 2nhdhl2n_hd_hl2nh​dh​l Strong Grouped-Query Attention (GQA) 2ngdhl2n_gd_hl2ng​dh​l Moderate Multi-Query Attentioin (MQA) 2dhl2d_hl2dh​l Weak Multi-Head Latent Attention (MLA) (dc+dhR)l≈92dhl(d_c + d^R_h)l \\approx \\frac{9}{2}d_hl(dc​+dhR​)l≈29​dh​l Stronger nhn_hnh​ 是头数，dhd_hdh​ 是每个头的维度，lll 是层数，ngn_gng​ 是 GQA 中的子组数，dcd_cdc​ 是压缩维度。 缓存与效果——工程优化 KV cache 根据 Decoder-only 的特性，每次前向完，把 KV 保留下来，用于之后的计算。 # q, k, v 当前 timestep 的 query, key, value # K_prev, V_prev 之前所有 timestamp 的 key 和 value for _ in range(time_step): # ... K = torch.cat([K_prev, k], dim=-2) # [b, h, n, d] V = torch.cat([V_prev, v], dim=-2) # [b, h, n, d] logits = torch.einsum(\"bhd, bhnd->bhn\", q, K) weights = torch.softmax(logits/math.sqrt(d), dim=-1) outs = torch.einsum(\"bhn, bhnd->bhd\", weights, V) # ... K_prev, V_prev = K, V Flash attention 有关计算和内存的基本概念 计算（Compute）指的是 GPU 计算实际浮点运算（FLOPS）所花费的时间。 内存（Memory）指的是在 GPU 内传输张量所花费的时间。 我们的 GPU 架构中，可以把记忆体简单地分成 HBM（High Bandwidth Memory）和 SRAM（Static Random Access Memory）两个部分： HBM 的记忆体空间很大，但是频宽较低 SRAM 的记忆体空间很小，但是频宽较高，用来做运算 在 GPU 跑 Attention 的流程如下： Load QQQ, KKK by blocks from HBM, compute S=QKTS = QK^TS=QKT, write SSS to HBM Read SSS from HBM, compute P=softmax(S)P = softmax(S)P=softmax(S), write PPP to HBM Load PPP and VVV by blocks from HBM, compute O=PVO = PVO=PV, write OOO to HBM. Read OOO 由于 SRAM 又贵又小，实际上 query state 或 key state 是一小块一小块 load 进去 SRAM 的。 而矩阵 S 维度爆炸为 N∗NN * NN∗N，占用大量的内存，这样大量的读写导致 Attention 运算速度很慢，使得 Attention 操作成为内存绑定操作，而且会有记忆体碎片化问题。 FlashAttention V1 Kernel Fusion 为减少显存读取次数，若 SRAM 容量允许，多个计算步骤（矩阵乘法、softmax 归一化、masking 和 dropout）可合并在一次数据加载中完成。 这样就可以大大减少读写次数。 Backward Recomputation 在前向传播时保存归一化因子，舍弃存储中间结果 PPP 和 SSS。 在反向传播时通过重计算得出注意力矩阵，以完成反向传播，这虽然增加了浮点运算次数，但通过减少 HBM 访问，提升了整体效率。 Softmax Tiling Attention 当中的一个核心步骤就是 Softmax Function，受限于 SRAM 的大小关系，我们不可能一次算出所有数值的 softmax，所以需要把所有中间计算的数值存在 HBM。 tiling 的做法是，先把一块丢进去计算出 softmax，这里的 m 代表的是这一块 load 到 SRAM 的最大值——local maxima，然后就可以计算出 local softmax： 接下来第二块进来，我们把第一块的最大值和第二块的最大值取最大值，就可以得到这两块数值的最大值，然后用相同的方式计算，就可以得到这两块的 local softmax。 我们不需要把每块算出来的数值存在 HBM，我们只需要存当下的最大值 m(x)m(x)m(x) 和分母加总 l(x)l(x)l(x) 就可以了。 所以实际上的流程就会是这样，蓝色的区域就是 HBM，橘色虚线的区块就是 SRAM，每次运算的时候，因为 SRAM 大小有限， 所以我们只 Load 一部分的 Key state 和 value state，红色的字就是我们第一个 block 的计算，蓝色的字就是我们第二个 block 的计算。 Paged attention PagedAttention 是 vLLM 性能增强的核心。 它通过将 KV cache 缓存划分为块来解决 LLM 服务中内存管理的关键问题，从而允许在内存中非连续存储键和值。 每个 block 类比于虚拟内存中的一个 page。每个 block 的大小是固定的，在 vLLM 中默认大小为 16，即可装 16 个 token 的 K/V 值； Shared prefix: 在某些大模型中，所有请求可能都会共享一个前置信息（比如 system message），这些前置信息没有必要重复存储 KV cache； Beam Search、并行采样中有大量的 KV cache 是重复的。内存使用率降低 55%。 对物理块的引用计数进行跟踪，并实现写时复制机制。 References: LLM 性能优化中的一些概念扫盲 MHA vs MQA vs GQA vs MLA 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA 榨乾GPU效能的Flash Attention 3 Flash Attention三部曲 vLLM and PagedAttention: A Comprehensive Overview Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-29 15:29:35 "},"Chap2-TrainingTech/":{"url":"Chap2-TrainingTech/","title":"训练技术","keywords":"","body":"训练技术 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-19 16:52:21 "},"Chap2-TrainingTech/distributed-training/overview.html":{"url":"Chap2-TrainingTech/distributed-training/overview.html","title":"分布式训练","keywords":"","body":"分布式训练概述 1.概述.md 数据并行 在数据并行训练过程中，整个数据集被划分为多个小块，每个小块被独立分配给不同的计算设备。 这种并行化策略主要在批次（Batch）维度上展开，意味着训练过程在多个设备上同时进行。 每个设备上都运行着模型的一个完整副本，并且仅针对其分配的数据块进行训练。 训练完成后，通过反向传播得到的梯度会在所有设备间汇总，以确保各设备上的模型参数保持一致性。 PyTorch的分布式数据并行（DDP）是实现数据并行的一个典型例子。 模型并行 在数据并行训练中，一个显著特征是每个GPU都拥有模型权重的完整副本，这不可避免地导致了资源冗余。 与之相对的是模型并行策略，该策略将模型分割并跨多个设备分布。 模型并行主要分为两大类：张量并行和流水线并行。 张量并行：专注于在单个操作中实现并行化，例如在矩阵-矩阵乘法中同时处理多个数据点（可视为层内并行，因为它在单个层内分配计算任务）。 流水线并行：在模型的不同层之间进行并行处理，允许各层同时进行计算（相当于层间并行，因为它在模型的不同层之间分配计算任务）。 张量并行 张量并行训练技术通过沿特定维度将一个张量划分为N个部分，使得每个设备仅存储整个张量的1/N，而不破坏计算图的完整性。 为了确保计算结果的准确性，需要进行额外的通信步骤。 以矩阵乘法为例，考虑计算C=ABC = ABC=AB。 我们可以将矩阵 BBB 按列分割成多个子矩阵 [B0,B1,B2,...,Bn][B0, B1, B2, ..., Bn][B0,B1,B2,...,Bn]，每个设备负责一个子矩阵。 接着，每个设备将本地的子矩阵 BiBiBi 与矩阵 AAA 相乘，得到一系列部分乘积 [AB0,AB1,AB2,...,ABn][AB0, AB1, AB2, ..., ABn][AB0,AB1,AB2,...,ABn]。 此时，每个设备仅持有部分结果，例如，设备0（rank=0）持有 AB0AB0AB0。 为了获得完整的结果 CCC，我们需要将所有设备上的部分乘积收集起来，并沿着列维度进行拼接。 这样，我们就能在多个设备间分配张量计算任务，同时确保整个计算过程的正确性。 典型的张量并行实现：Megatron-LM（1D）、Colossal-AI（2D、2.5D、3D）。 流水线并行 流水线并行的核心理念在于将模型分解成多个层级，并分别分配给不同的计算设备。具体来说： 模型分割：模型被划分为若干独立的层块，每个层块负责处理模型的一部分计算任务。 前向传播优化：在前向传播阶段，每个设备负责计算其分配的层块，并将其产生的中间激活值传递给流水线中的下一个设备，以此类推。 后向传播优化：在后向传播阶段，每个设备接收来自下一个阶段的输入张量梯度，并将其回传至前一个阶段，以此类推。 并行计算优势：通过这种方式，各个设备可以并行执行各自的计算任务，显著提高了模型训练的并行度和吞吐量，从而加快训练速度。 流水线并行训练的一个明显缺点是训练设备容易出现空闲状态（因为后一个阶段需要等待前一个阶段执行完毕），导致计算资源的浪费，加速效率没有数据并行高。 典型的流水线并行实现：GPipe、PipeDream、PipeDream-2BW、PipeDream Flush（1F1B）。 优化器相关并行 目前随着模型越来越大，单个GPU的显存目前通常无法装下那么大的模型了。那么就要想办法对占显存的地方进行优化。 通常来说，模型训练的过程中，GPU上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的Buffer。 模型参数仅占模型训练过程中所有数据的一部分，当进行混合精度运算时，模型状态参数（优化器状态+梯度+模型参数）占到了一大半以上。 因此，我们需要想办法去除模型训练过程中的冗余数据。 优化器相关的并行就是一种去除冗余数据的并行方案，目前这种并行最流行的方法是 ZeRO（即零冗余优化器）。 ZeRO 通过分片技术优化模型状态的存储，确保每张 GPU 仅存储模型状态的 1/N 部分，从而在整个系统中只维护一份模型状态。 ZeRO 分为三个级别，它们分别对应不同程度的模型状态分片： ZeRO-1 : 对优化器状态分片（Optimizer States Sharding） ZeRO-2 : 对优化器状态和梯度分片（Optimizer States & Gradients Sharding） ZeRO-3 : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States & Gradients & Parameters Sharding） 多维混合并行 多维混合并行指将数据并行、模型并行和流水线并行等多种并行技术结合起来进行分布式训练。 通常，在进行超大规模模型的预训练和全参数微调时，都需要用到多维混合并行。 为了充分利用带宽，通常情况下，张量并行所需的通信量最大，而数据并行与流水线并行所需的通信量相对来说较小。 因此，同一个服务器内使用张量并行，而服务器之间使用数据并行与流水线并行。 MoE 并行/专家并行 通常来讲，模型规模的扩展会导致训练成本显著增加，计算资源的限制成为了大规模密集模型训练的瓶颈。 为了解决这个问题，一种基于稀疏 MoE 层的深度学习模型架构被提出，即将大模型拆分成多个小模型(专家，expert)， 每轮迭代根据样本决定激活一部分专家用于计算，达到了节省计算资源的效果； 并引入可训练并确保稀疏性的门(gate)机制，以保证计算能力的优化。 使用 MoE 结构，可以在计算成本次线性增加的同时实现超大规模模型训练，为恒定的计算资源预算带来巨大增益。 而 MOE 并行，本质上也是一种模型并行方法。 下图展示了一个有六个专家网络的模型被两路专家并行地训练。 其中，专家 1-3 被放置在第一个计算单元上，而专家 4-6 被放置在第二个计算单元上。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 13:22:33 "},"Chap2-TrainingTech/distributed-training/data-parallel.html":{"url":"Chap2-TrainingTech/distributed-training/data-parallel.html","title":"数据并行","keywords":"","body":"数据并行 2.数据并行 数据并行的概念和原理 数据并行是一种分布式训练技术，它允许我们将大规模的数据集分割成多个小份，并将它们分配到多个 GPU 节点上进行处理。 每个 GPU 节点都持有一个完整的模型副本，并基于其分配的数据独立进行梯度计算。 在这种设置中，通常有一个 GPU（如 GPU0）作为参数服务器，负责聚合各个 GPU 节点的梯度，并更新模型参数，然后广播更新后的参数到所有节点（也可以将参数服务器分布在所有节点上，每个 GPU 只更新其中一部分梯度）。 这种并行方式不仅可以应用于训练数据，还可以扩展到模型梯度、权重参数以及优化器状态等多个方面，以提高训练效率。 数据并行（PyTorch DP） PyTorch中的数据并行（DP）是最早提供的一种并行方式，它基于单进程多线程实现。 在 DP 中，数据和模型被分发到每个 GPU 上，每个 GPU 独立进行前向传播和梯度计算，然后将计算结果返回到主 GPU 上进行损失函数计算和梯度更新。 DP 的优点是实现简单，只需一行代码即可完成设置。 然而，它的缺点也很明显，比如受制于全局解释器锁（GIL），导致性能开销较大，且只能在单台服务器上使用（单机多卡），不支持分布式训练和 Apex 混合精度训练。 分布式数据并行（PyTorch DDP） 分布式数据并行（DDP）是 PyTorch 中一种更高级的数据并行方案，它基于多进程实现，每个进程都有自己的优化器，独立执行更新过程。 DDP 通过在进程间传递梯度来减少网络通信的瓶颈，适用于单机和多机情况，并且避免了 GIL 带来的性能开销。 DDP 的每个进程都会创建一个本地的梯度同步器，确保在训练过程中，每个 GPU 都能独立进行反向传播和参数更新，从而实现更高效的负载均衡和运行效率。 DP和DDP的区别 DDP（torch.nn.DistributedDataParallel）是PyTorch中用于分布式训练的模块，它允许在多个GPU甚至多个节点上并行训练模型。 以下是 DDP 的一些详细特点和工作原理： 多进程管理：DDP 通过启动多个进程来实现分布式训练，每个进程控制一个 GPU，并加载模型的一个副本，每个进程都有独立的优化器，执行自己的更新过程； 数据分发：在每个 GPU 上分发参数并建立模型副本，每个进程都会加载不同的数据，DDP 会自动处理跨 GPU 的梯度同步； 梯度同步：每个 GPU 上的模型都会在自己的数据上进行前向和反向传播，然后通过梯度同步机制更新模型参数，确保所有GPU上的模型状态保持一致； Ring-Reduce方法：DDP 使用 Ring-Reduce（组内 reduce -> 组间 all reduce -> 组内 broadcast）的方法进行通信，交换梯度，每个进程获取所有进程梯度，并用平均后的梯度更新参数。 DP 和 DDP 的主要区别在于它们的实现方式和参数更新机制。 DP 基于单进程多线程，而 DDP 基于多进程，每个 GPU 对应一个进程。 在参数更新方面，DDP 在各进程梯度计算完成后，会将梯度汇总平均，然后由主进程广播到所有进程，各进程用该梯度独立更新参数。 相比之下，DP是将梯度汇总到主卡，然后在主卡上更新参数，再广播给其他GPU。 DDP 由于避免了 GIL 和减少了数据传输量，训练效率更高，且不存在 DP 中的负载不均衡问题。 DDP 支持模型并行，而 DP 并不支持，这意味如果模型太大单卡显存不足时，只能使用DDP。 完全分片数据并行（PyTorch FSDP） 完全分片数据并行（FSDP）受到 DeepSpeed ZeRO 优化策略的启发。 FSDP 通过分片模型参数、梯度和优化器状态，进一步减少了模型训练过程中的冗余数据。 FSDP 可以选择将模型参数分片卸载到 CPU，以提高内存效率。 FSDP 的出现打破了大模型训练中模型分片的障碍，同时保持了数据并行的简单性，使得训练更大的模型成为可能。 ZeRO 优化策略 ZeRO 是 DeepSpeed 提出的一种优化策略，它通过分片模型状态参数来减少冗余数据。 ZeRO 有三个不同的级别： ZeRO-1 只对优化器状态进行分片， ZeRO-2 对优化器状态和梯度进行分片， ZeRO-3 则进一步对模型权重参数也进行分片。 这种分片策略可以显著减少GPU上需要存储的数据量，从而提高内存效率和训练大模型的能力。 DDP和FSDP的区别 DDP 和 FSDP 在模型副本和梯度聚合方式上有所不同。 在 DDP 中，每个 GPU 上都有一个完整的模型副本，而在 FSDP 中，每个 GPU 上仅存在模型的分片。 在梯度聚合方面，DDP 在局部计算后需要将梯度同步给主节点，而 FSDP 则通过 all-gather 操作从其他 GPU 收集所有权重，以在本地计算前向传播。 FSDP 在内存效率上有优势，因为它可以在每层前向传播后丢弃全部权重，为后续层节省内存。 这种设计使得 FSDP 特别适合训练大型模型。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 14:06:11 "},"Chap2-TrainingTech/distributed-training/pipeline-parallel.html":{"url":"Chap2-TrainingTech/distributed-training/pipeline-parallel.html","title":"流水线并行","keywords":"","body":"流水线并行 数据并行与模型并行的对比 在分布式训练中，数据并行和模型并行是两种常见的并行技术。 数据并行训练中，每个 GPU 持有整个模型权重的副本，这虽然简单，却带来了参数冗余的问题。 为了解决这一问题，模型并行技术应运而生，它将模型分割并分布在一个设备阵列上，每个设备只保存模型的一部分参数。 模型并行进一步分为张量并行和流水线并行，前者是层内并行，后者是层间并行，针对的是模型的不同 Transformer 层间进行分割。 流水线并行的简介 流水线并行技术是为了应对单个模型过大，无法放置在单张 GPU 卡中的问题。 通过将模型的不同层放置到不同的计算设备上，可以降低单个计算设备的显存消耗，实现超大规模模型的训练。 在前向计算过程中，输入数据在设备间传递并逐层处理，最终得到结果。 反向传播过程类似，但传递的是梯度信息。 由于仅传输相邻设备间的输出张量，通信量相对较小。 朴素流水线并行 朴素流水线并行是实现流水线并行的最直接方法，它将模型按层间切分成多个部分，并将每个部分分配给一个 GPU。 然而，这种方法存在明显的缺陷， 在任意给定时刻，除了一个 GPU 之外的其他所有GPU都是空闲的，导致 GPU 使用率极低。 在设备之间复制数据的额外通信开销，这进一步降低了效率。 通信和计算没有交错的问题：当通过网络进行数据通信时，没有 GPU 执行任何操作。 高内存需求：先执行前向传播的GPU（如：GPU1）将保留整个小批量缓存的所有激活，直到最后。如果批量大小很大，可能会产生内存问题。 微批次流水线并行 为了解决 GPU 空闲问题，微批次流水线并行通过将传入的小批次分块为微批次，并人为创建流水线来提高设备利用率。 这种方法允许不同的 GPU 同时参与计算过程，显著提升了流水线并行设备的利用率，并减少了设备空闲状态的时间。 GPipe GPipe 是谷歌提出的一种流水线并行方案，它基于 TensorFlow 和 PyTorch 实现。 GPipe 通过微批次流水线并行提高了模型训练的并行度，并利用重计算技术降低了显存消耗。 这种方法通过纵向对模型进行切分解决了单个设备无法训练大模型的问题，同时增加了多设备上的并行程度。 流水线并行策略 流水线并行策略可以根据执行的策略分为 F-then-B 和 1F1B 两种模式。 F-then-B 模式先进行前向计算，再进行反向计算，可能会导致内存占用很高。 而 1F1B 模式，即 One Forward pass followed by One Backward pass，是一种前向计算和反向计算交叉进行的方式，可以及时释放不必要的中间变量，从而节省显存。 PipeDream PipeDream 是微软 DeepSpeed 提出的 1F1B 策略，它通过合理安排前向和反向过程的顺序来解决内存过高的问题。 这种策略可以解决缓存 activation 的份数问题，使得 activation 的缓存数量只跟 stage 数相关，进一步节省显存，训练更大的模型。 PipeDream-2BW PipeDream-2BW 是 PipeDream 的一个变体，它在流水线中只维护两个版本的模型权重，称为双缓冲权重。 这种方法减少了需要维护的权重版本总数，从而极大降低了内存的占用。 PipeDream-Flush（1F1B） PipeDream-Flush 是 PipeDream 的一个变体，它维护单个权重版本并引入定期流水线刷新，以确保权重更新期间的权重版本保持一致。 虽然这种方法以执行性能为代价降低了峰值内存，但吞吐量较低。 1F1B 调度模式 在使用 1F1B 策略时，存在两种调度模式：非交错式和交错式调度。 非交错式调度分为热身阶段、稳定阶段和完成后向计算阶段。 而交错式调度中，每个设备可以对多个层的子集进行计算，这种模式既节省内存又节省时间，但要求 micro-batch 的数量是流水线阶段的整数倍。 PipeDream（交错式1F1B）-Megatron-LM Megatron-LM 基于 PipeDream-Flush 提出了交错式 1F1B 调度，即虚拟流水线。 这种方案通过在设备数量不变的情况下分出更多的流水线阶段，以更多的通信量换取流水线 Bubble 比率降低。 这种交错式调度需要额外的通信，但可以通过高速网络带宽来减少这种额外通信的影响。 分布式训练框架流水线并行方案 在分布式训练框架中，流水线并行方案可以细分为同步流水线并行(Sync-PP)和异步流水线并行(Async-PP)。 PyTorch 采用的是 GPipe 方案，而 DeepSpeed 采用的是 PipeDream-Flush。 Megatron-LM 基于 PipeDream-Flush 进行了改进，提供了一种交错式 1F1B 方案。 Colossal-AI 基于 Megatron-LM 的交错式 1F1B 方案，提供了非交错和交错调度策略。 Ref: Pipeline Parallel Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 15:00:44 "},"Chap2-TrainingTech/distributed-training/moe-parallel.html":{"url":"Chap2-TrainingTech/distributed-training/moe-parallel.html","title":"MoE 并行","keywords":"","body":"MoE 并行 MoE的概念和作用 MoE（Mixture of Experts）是一种深度学习模型架构，它通过将大模型拆分成多个小模型（专家）来解决模型规模扩展导致的训练成本增加问题。 这种架构允许在每轮迭代中根据样本激活一部分专家进行计算，从而节省计算资源。 MoE 引入了可训练的门（gate）机制，以确保计算能力的优化。 与传统的密集模型不同，MOE 通过门控网络决定哪些专家网络参与计算，实现了超大规模稀疏模型的训练。 MoE 层的计算过程 MoE 层的计算过程涉及对样本进行门控计算，然后通过 Softmax 处理获得样本被分配到各个专家的权重。 接着，只选择前 k 个最大权重的专家进行计算，最终的计算结果是这些选中专家网络输出的加权和。 这一过程不仅提高了模型的计算效率，还通过稀疏性减少了模型的计算负担。 MoE 分布式并行策略 在分布式训练中，MoE 架构可以通过数据并行和模型并行两种策略实现。 在数据并行策略中，门网络和专家网络被复制地放置在各个运算单元上，这种方式对现有代码的侵入性较小，但专家的数量受到单个计算单元内存大小的限制。 模型并行策略则是将门网络复制放置，而专家网络独立放置在各个计算单元上，这需要额外的通信操作，但可以支持更多的专家网络同时训练。 这两种策略各有优势，需要根据具体的模型和设备拓扑来选择最合适的并行策略。 业界大模型的 MoE 并行方案 业界已经有一些大模型采用了 MoE 并行方案，如 GShard、Switch-Transformer 和 GLaM。 GShard 将 MoE 应用于 Transformer，并引入了 Expert capacity balancing 等设计来优化模型。 Switch-Transformer 简化了 MoE 的 routing 算法，提高了计算效率。 GLaM 是 Google 推出的超大模型，通过使用 Sparse MoE 设计，在不显著增加计算成本的情况下，实现了模型参数量的大幅增加。这些方案展示了MOE在构建大规模模型中的重要性和潜力。 AI 训练框架中的 MoE 并行训练 在 AI 训练框架中，MoE 并行训练也得到了广泛的应用。 例如，PaddlePaddle 框架提供了 MoE 并行的适配和训练示例，展示了如何在动态图模式下使用 PaddlePaddle 进行 MoE 架构的训练。 DeepSpeed 框架也支持 MoE 并行，并提供了多种并行形式，可以同时利用 GPU 和 CPU 内存。 这些框架的支持使得 MoE 并行训练变得更加便捷和高效。 Ref: 混合专家模型 (MoE) 详解 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 15:53:00 "},"Chap2-TrainingTech/fine-tuning/overview.html":{"url":"Chap2-TrainingTech/fine-tuning/overview.html","title":"有监督微调","keywords":"","body":"微调的定义 微调（Fine-tuning）是一种迁移学习的方法，利用特定领域的数据集对已预训练的大模型进行进一步训练的过程。 其目的在于优化模型在特定任务上的性能，使模型能够更好地适应和完成特定领域的任务。 下面是一般的微调步骤： 预训练模型选择：选择一个在大规模数据上进行预训练的模型作为基础模型。例如，可以选择一种预训练的语言模型，如BERT、GPT等。 数据准备：准备用于微调的特定任务数据集。这些数据集应包含任务相关的样本和相应的标签或目标。确保数据集与任务的特定领域或问题相关。 构建任务特定的模型头：根据任务的要求，构建一个特定的模型头（task-specific head）。模型头是添加到预训练模型之上的额外层或结构，用于根据任务要求进行输出预测或分类。例如，对于文本分类任务，可以添加一个全连接层和softmax激活函数。 参数初始化：将预训练模型的参数作为初始参数加载到微调模型中。这些参数可以被视为模型已经学习到的通用语言表示。 微调训练：使用特定任务的数据集对模型进行有监督训练。这包括将任务数据输入到模型中，计算损失函数，并通过反向传播和优化算法（如梯度下降）更新模型参数。在微调过程中，只有模型头的参数会被更新，而预训练模型的参数会保持不变。 调整超参数：微调过程中，可以根据需要调整学习率、批量大小、训练迭代次数等超参数，以达到更好的性能。 评估和验证：在微调完成后，使用验证集或测试集对微调模型进行评估，以评估其在特定任务上的性能。可以使用各种指标，如准确率、精确率、召回率等。 可选的后续微调：根据实际情况，可以选择在特定任务的数据上进行进一步的微调迭代，以进一步提高模型性能。 为什么需要参数高效微调（PEFT） Parameter-Efficient Fine-Tuning（PEFT）是一种微调策略，旨在通过最小化微调参数数量和计算复杂度，实现高效的迁移学习。 它仅更新模型中的部分参数，显著降低训练时间和成本，适用于计算资源有限的情况。 然而，需要注意的是，PEFT 有时可能会达到与完全微调不同的性能水平，特别是在预训练模型需要进行重大调整才能在新任务上表现良好的情况下。 高效微调技术可以粗略分为以下三大类：增加额外参数（A）、选取一部分参数更新（S）、引入重参数化（R）。 而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。 PEFT 有什么优点 这里讨论 PEFT 相对于传统微调的好处。 减少计算和存储成本：PEFT 只涉及微调少量额外的模型参数，而冻结预训练 LLM 的大部分参数，从而显着降低计算和存储成本。 克服灾难性遗忘：模型在全面微调起茧，可能会遗忘它在预训练期间学到的知识的地方。 低数据环境下更好的性能：PEFT 方法在低数据环境下的表现优于完全微调，并且可以更好地推广到域外场景。 与完全微调相当的性能：PEFT 仅使用少量可训练参数即可实现与完全微调相当的性能。 多种不同的高效微调方法对比 参数有效策略可能涉及多种技术： 选择性层调整：可以只微调层的一个子集，而不是微调模型的所有层。这减少了需要更新的参数数量。 适配器（Adapters）：适配器层是插入预训练模型层之间的小型神经网络。在微调过程中，只训练这些适配器层。适配器学习将预训练的模型提取到的特征使用到新任务。 稀疏微调：稀疏微调只涉及更改模型参数的一个子集。这通常基于一些标准来完成，这些标准标识了与新任务最相关的参数。 低秩近似（Low-Rank Approximations）：用一个参数较少但在任务中表现相似的模型来近似微调后的模型。 正则化技术：可以将正则化项添加到损失函数中，以阻止参数发生较大变化，从而以更“参数高效”的方式有效地微调模型。 任务特定的头（Task-specific Heads）：有时，在预先训练的模型架构中添加一个任务特定的层或“头”，只对这个头进行微调，从而减少需要学习的参数数量。 Refs: 基本概念.md 一文搞懂大模型微调 通俗解读大模型微调 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-19 10:49:38 "},"Chap2-TrainingTech/fine-tuning/finetuning-discussion.html":{"url":"Chap2-TrainingTech/fine-tuning/finetuning-discussion.html","title":"关于微调的讨论","keywords":"","body":"关于微调的讨论 为什么 SFT 之后感觉 LLM 傻了 在 Supervised Fine-Tuning（SFT）之后，有时人们可能会感觉到模型性能有所下降或出现“变傻”的现象。 这主要是因为SFT虽然提高了模型在特定任务上的性能，但也可能降低了模型的泛化能力和通用性。 原因分析； 数据集限制：SFT 通常依赖于特定任务的数据集进行微调，这可能导致模型在训练过程中过于专注于这些任务（若数据集相对较小，则可能发生过拟合），而忽视了其他领域的知识。又或者数据集缺乏多样性，未能涵盖新任务上的各种情况，导致模型在面对新的、与数据集不同的输入时出现有偏差的预测。 模型遗忘：领域特定数据训练后，通用能力往往会有所下降，这是因为模型在微调过程中可能会遗忘一些通用的知识和特征。 非典型标注：数据集的标注可能存在错误或不准确的标签，这些错误的标签可能会对模型的性能产生负面影响，导致模型产生“傻”的行为。 缓解措施： 增加数据多样性：通过增加不同领域和任务的数据，可以提高模型的泛化能力。 仔细检查微调数据集的标注，确保标签的准确性和一致性。 使用正则化技术（如权重衰减、dropout）来减少过拟合的风险。 进行数据增强，通过对微调数据进行一些变换或扩充来增加多样性。 使用更复杂的模型架构或调整模型的超参数，以提高模型的性能和泛化能力。 领域模型 Continue Pretrain 数据选取 常见的数据选取方法： 领域相关数据：收集与目标领域相关的数据。这些数据可以是从互联网上爬取的、来自特定领域的文档或者公司内部的数据等。这样的数据可以提供领域相关的语言和知识，有助于模型在特定领域上的表现。 领域专家标注：如果有领域专家可用，可以请他们对领域相关的数据进行标注，这样可以提供有监督的数据用于模型的训练。 伪标签：如果没有领域专家或者标注数据的成本较高，可以使用一些自动化的方法生成伪标签。例如，可以使用预训练的模型对领域相关的数据进行预测，将预测结果作为伪标签，然后使用这些伪标签进行模型的训练。 数据平衡：在进行数据选取时，需要注意数据的平衡性。如果某个类别的数据样本较少，可以考虑使用数据增强技术或者对该类别进行过采样，以平衡各个类别的数据量。 数据质量控制：在进行数据选取时，需要对数据的质量进行控制。可以使用一些质量评估指标，如数据的准确性、一致性等，来筛选和过滤数据。 数据预处理：在进行数据选取之前，可能需要对数据进行一些预处理，如分词、去除停用词、标准化等，以准备好输入模型进行训练。 如何缓解模型遗忘的问题 缓解模型遗忘通用能力的方法： 保留通用数据：在进行领域数据训练时，仍然需要保留一部分通用数据用于模型训练。这样可以确保模型仍然能够学习到通用的语言和知识，从而保持一定的通用能力。 增量学习：使用增量学习（Incremental Learning）的方法，将领域数据与通用数据逐步交替进行训练。这样可以在学习新领域的同时，保持对通用知识的记忆。 预训练和微调：在领域数据训练之前，可以使用大规模通用数据进行预训练，获得一个通用的基础模型。然后，在领域数据上进行微调，以适应特定领域的任务。这样可以在保留通用能力的同时，提升领域任务的性能。 强化学习：使用强化学习的方法，通过给模型设置奖励机制，鼓励模型在领域任务上表现好，同时保持一定的通用能力。 领域适应技术：使用领域适应技术，如领域自适应（Domain Adaptation）和领域对抗训练（Domain Adversarial Training），帮助模型在不同领域之间进行迁移学习，从而减少遗忘通用能力的问题。 数据重采样：在进行领域数据训练时，可以使用数据重采样的方法，使得模型在训练过程中能够更多地接触到通用数据，从而缓解遗忘通用能力的问题。 领域模型继续预训练，如何在模型在预训练的过程中就学习到更多的知识 以下是一些参考策略： 多任务学习：在预训练过程中，可以引入多个任务，使得模型能够学习到更多的知识。这些任务可以是领域相关的任务，也可以是通用的语言理解任务。通过同时训练多个任务，模型可以学习到更多的语言规律和知识。 多领域数据：收集来自不同领域的数据，包括目标领域和其他相关领域的数据。将这些数据混合在一起进行预训练，可以使得模型在不同领域的知识都得到学习和融合。 大规模数据：使用更大规模的数据进行预训练，可以让模型接触到更多的语言和知识。可以从互联网上爬取大量的文本数据，或者利用公开的语料库进行预训练。 数据增强：在预训练过程中，可以采用数据增强的技术，如随机遮挡、词替换、句子重组等，来生成更多的训练样本。这样可以增加模型的训练数据量，使其能够学习到更多的知识和语言规律。 自监督学习：引入自监督学习的方法，通过设计一些自动生成的标签或任务，让模型在无监督的情况下进行预训练。例如，可以设计一个掩码语言模型任务，让模型预测被掩码的词语。这样可以使模型在预训练过程中学习到更多的语言知识。 领域模型词表扩增是不是有必要的 领域词表扩增的意义： 提高解码效率：在很多情况下，词表扩增后，处理相同文本时所需的 token 数量减少，这能让模型在处理文本时更高效地进行编码和解码操作。 增加上下文窗口长度：词表扩充后，模型能够处理更长的文本输入，从而在需要长文本分析的任务中表现更好。 提升模型对领域内容的理解和生成能力：特定领域往往包含大量专业术语和行话，通过扩增词表，模型可以更准确地理解和生成领域相关的文本。 领域模型的词表扩增可以有助于提升模型在特定领域任务上的性能，但是否有必要取决于具体的情况。 领域特定词汇：如果目标领域中存在一些特定的词汇或术语，而这些词汇在通用的预训练模型的词表中没有覆盖到，那么词表扩增就是必要的。通过将这些领域特定的词汇添加到模型的词表中，可以使模型更好地理解和处理这些特定的词汇。 领域特定上下文：在某些领域任务中，词汇的含义可能会受到特定上下文的影响。例如，在医学领域中，同一个词汇在不同的上下文中可能具有不同的含义。如果领域任务中的上下文与通用预训练模型的训练数据中的上下文有较大差异，那么词表扩增可以帮助模型更好地理解和处理领域特定的上下文。 数据稀缺性：如果目标领域的训练数据相对较少，而通用预训练模型的词表较大，那么词表扩增可以帮助模型更好地利用预训练模型的知识，并提升在目标领域任务上的性能。 指令微调的好处？ 指令微调（Instruction Fine-tuning）中，模型接受指令或约束来生成特定的输出。好处： 控制生成输出：指令微调使得模型能够根据指定的指令或约束生成特定的输出。这对于需要精确控制模型生成结果的任务非常有用。 可解释性和可控性：通过指令微调，可以将任务的要求以指令的形式传达给模型。这增加了模型的可解释性和可控性，使得用户能够更好地理解和干预模型的生成过程。 避免不符合要求的输出 提供任务性能：指令微调可以针对具体任务进行优化，使得模型在该任务上的性能得到提升。通过引入任务特定的指令或约束，模型可以更好地适应特定任务的需求，并生成更准确、更合理的输出。 灵活性和可扩展性：指令微调是一种灵活且可扩展的方法，允许在不同任务和场景中进行微调。通过调整和修改指令或约束，可以适应不同的任务需求，并在多个任务上进行微调。 预训练和微调，哪个阶段注入了知识 预训练和微调两个阶段都可以注入知识，注入知识的方式和范围略有不同。 预训练阶段：模型通过大规模的未标注数据进行训练，从中学习语言的统计特征、语义知识和语言规律。预训练的目的是让模型建立起对语言的基本理解和概念，并学习到一般性的语言表示。这种预训练过程注入了通用的语言知识，并可以迁移到各种下游任务中。 微调阶段：预训练模型使用带标注的数据进行微调，以适应具体任务的要求。模型通过接触任务特定的数据和标签，进一步调整和优化模型的参数，使其更好地适应任务的特定特征和要求。微调阶段注入的是与任务相关的知识和信息。 Refs: 1.微调.md SFT指令微调大模型后,感觉LLM变傻了 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-19 15:05:20 "},"Chap3-PromptEngr/":{"url":"Chap3-PromptEngr/","title":"Prompt 工程","keywords":"","body":"Prompt 工程 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html":{"url":"Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html","title":"宝玉老师-如何写好提示词？","keywords":"","body":"宝玉老师的这个关于提示词编写的介绍，个人觉得收益匪浅。 How to write good prompt 概要 提示词的四大要素：指令、上下文、输出格式、角色 提示词策略和技巧 大语言模型的本质和劣势 提升生成期望结果的概率 策略一：撰写清晰的指令 策略二：给模型“思考”的时间 策略三：把复杂任务拆分成简单任务 策略四：接入外部工具和资料 提示词技巧的具体应用案例 提示词的四大要素 提示词是与 AI 交互时的重要工具，它包含四大要素：指令、上下文、输出格式和角色。 指令：是告诉 AI 需要完成的具体任务或回答的问题，需要清晰具体。 上下文：是 AI 执行指令所需的背景信息，它帮助 AI 更好地理解和执行指令。 输出格式：是我们期望 AI 返回结果的格式，比如表格、摘要或 Markdown 格式等。 角色：则是我们期望 AI 在交互中扮演的角色，比如助手、心理医生或导师等，这有助于 AI 更准确地生成内容。 提示词策略和技巧 有效的提示词策略和技巧可以极大提升与 AI 的交互质量。 首先，我们需要撰写清晰的指令，因为 AI 无法猜测我们的想法。 其次，给模型“思考”的时间，通过展示思考过程来提升推理结果。 此外，将复杂任务拆分成简单任务，可以提高任务的成功率。 最后，接入外部工具和资料，可以弥补 AI 知识库的不足，提升其性能。 大语言模型的本质和劣势 大语言模型以其强大的语言理解和生成能力著称，它们拥有简单的推理能力，并且适应性强，可以通过微调或上下文学习来提升性能。 然而，这些模型也有其局限性，比如对事实的理解有限、知识库有限、缺乏深层推理能力，以及上下文窗口长度限制和没有记忆等。 了解这些本质和劣势有助于我们更好地利用 AI。 提升生成期望结果的概率 为了提升生成期望结果的概率，我们需要发挥 AI 模型的优势，补充或避开其劣势。 这包括让 AI 准确理解指令，以及让概率收敛在高质量的路径上。 通过精心设计的提示词，我们可以引导 AI 生成更准确的结果。 策略一：撰写清晰的指令 在与 AI 的交互中，清晰的指令至关重要。 模型不会读心术，因此需要我们提供具体明确的指令。 此外，提供详尽的背景信息可以帮助模型更好地理解任务。 让 AI 向我们提问可以避免信息的遗漏，而让 AI 扮演任务相关的角色则可以提升特定领域内容的生成概率。 策略二：给模型“思考”的时间 在处理复杂问题时，给模型“思考”的时间是提升推理结果的有效策略。 这可以通过打印思考过程来实现，帮助模型更可靠地推导出正确答案。 此外，用 Token 换时间，即在模型给出答案之前，要求其展示一下“思考过程”，有助于模型更准确地回答问题。 策略三：把复杂任务拆分成简单任务 将复杂任务拆分成简单任务可以提高任务的成功率。 这类似于软件工程中将复杂系统分解成模块化的组件。 通过化繁为简，我们可以提高每个子任务的成功率，并且可以合并回去，使得前一个任务的输出可以作为后一个任务的输入。 策略四：接入外部工具和资料 在 AI 的能力不足以完成任务时，我们可以借助外部工具和资料。 通过上下文学习，我们可以弥补知识库的不足。 提供工具的说明和接口参数给 AI，可以让 AI 自己选择使用什么工具，传入什么参数。 此外，AI 不擅长数学，但可以调用数学工具或者执行代码来完成任务。 提示词技巧的具体应用案例 在实际应用中，我们可以通过结构化输入、明确说明完成任务的步骤、给出样例（few-shot）以及使用伪代码等技巧来提升提示词的效果。 这些技巧可以帮助 AI 更准确地理解我们的需求，并生成更符合预期的结果。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap3-PromptEngr/prompt-app_openai-prompt-generation.html":{"url":"Chap3-PromptEngr/prompt-app_openai-prompt-generation.html","title":"OpenAI-生成提示词的提示词","keywords":"","body":"OpeAI 公布的生成提示词的提示词技术。 Prompt generation 提示词 元提示指示模型根据您的任务描述创建良好的提示或改进现有提示。 META_PROMPT = \"\"\" Given a task description or existing prompt, produce a detailed system prompt to guide a language model in completing the task effectively. # Guidelines - Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output. - Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure. - Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS! - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed. - Conclusion, classifications, or results should ALWAYS appear last. - Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements. - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders. - Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements. - Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED. - Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user. - Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples. - Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.) - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON. - JSON should never be wrapped in code blocks (```) unless explicitly requested. The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\") [Concise instruction describing the task - this should be the first line in the prompt, no section header] [Additional details as needed.] [Optional sections with headings or bullet points for detailed steps.] # Steps [optional] [optional: a detailed breakdown of the steps necessary to accomplish the task] # Output Format [Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc] # Examples [optional] [Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.] [If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ] # Notes [optional] [optional: edge cases, details, and an area to call or repeat out specific important considerations] \"\"\".strip() def generate_prompt(task_or_prompt: str): completion = client.chat.completions.create( model=\"gpt-4o\", messages=[ { \"role\": \"system\", \"content\": META_PROMPT, }, { \"role\": \"user\", \"content\": \"Task, Goal, or Current Prompt:\\n\" + task_or_prompt, }, ], ) return completion.choices[0].message.content Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap4-InferAndOpt/":{"url":"Chap4-InferAndOpt/","title":"推理与优化","keywords":"","body":"推理与优化 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "},"Chap5-App/":{"url":"Chap5-App/","title":"应用方向","keywords":"","body":"应用方向 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-11-18 11:15:29 "}}