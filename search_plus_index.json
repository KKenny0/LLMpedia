{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction 本 Gitbook 搜集和分享个人喜欢的大语言模型（LLM）相关资源的在线百科全书。 涵盖 LLM 的基础知识、研究进展、技术应用、工具和最佳实践。 大语言模型（LLM）学习路径和资料汇总 Embedding Model Fine-Tuning 提示工程相关资料： Anthropic 提示工程概览 Claude 提示库 OpenAI 提示工程指南 OpenAI 提示示例 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/":{"url":"Chap1-ModelArch/","title":"模型结构","keywords":"","body":"模型结构 Transformer 架构与模型 Transformer 可视化解释 土猛的员外-Transformer 架构的整体指南 Encoder 模型 Decoder 模型 注意力机制 Lilian-Attention?Attention! 缓存优化与效果-KV Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_wiki.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_wiki.html","title":"Transformer 结构-Wiki","keywords":"","body":" Transformer (deep learning architecture) Architecture 主要组件: Tokenizers: Embedding layer: convert tokens and positions of the tokens into vector representations Transformer layers: carry out repeated transformations on the vector representations, extracting more and more linguistic information. Alternating attention and feedforward layers. Two major types transformer layers: encoder layers and decoder layers Un-embedding layer: convert the final vector representations back to a probability distribution over the tokens Embedding Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix MMM. Un-embedding Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens. The un-embedding layer is a linear-softmax layer: UnEmbed(x)=softmax(xW+b) UnEmbed(x) = softmax(xW+b) UnEmbed(x)=softmax(xW+b) Encoder-decoder (overview) The original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far. Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. Feedforward network The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons. Scaled dot-product attention Attention head The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each vector xi,queryx_{i, query}xi,query​ in the query sequence, it is multiplied by a matrix WQW^QWQ to produce a query vector qi=xi,queryWQq_i = x_{i, query}W^Qqi​=xi,query​WQ. The matrix of all query vectors is the query matrix: Q=XqueryWQQ = X_{query}W^QQ=Xquery​WQ. Similarly, we construct the key matrix K=XkeyWKK = X_{key}W^KK=Xkey​WK and the value matrix V=XvalueWVV = X_{value}W^VV=Xvalue​WV. It is usually the case that all WQ,WK,WV{\\displaystyle W^{Q},W^{K},W^{V}}WQ,WK,WV are square matrices. Attention weights are calculated using the query and key vectors：the attention weight from token iii to token jjj is the dot product between qiq_iqi​ and kjk_jkj​. The attention weights are divided by the square root of the dimension of the key vectors, dk{\\displaystyle {\\sqrt {d_{k}}}}dk​​, which stabilizes gradients during training, and passed through a softmax which normalizes the weights The matrices Q{\\displaystyle Q}Q, K{\\displaystyle K}K and V{\\displaystyle V}V are defined as the matrices where the {\\displaystyle i}th rows are vectors qi{\\displaystyle q_{i}}qi​, ki{\\displaystyle k_{i}}ki​, and vi{\\displaystyle v_{i}}vi​ respectively. Then we can represent the attention as: The attention mechanism requires the following three equalities to hold: If the attention is used in a self-attention fashion, then Xquery=Xkey=XvalueX_{query} = X_{key} = X_{value}Xquery​=Xkey​=Xvalue​. If the attention is used in a cross-attention fashion, then usually Xquery≠Xkey=XvalX_{query} \\neq X_{key} = X_{val}Xquery​​=Xkey​=Xval​. Multihead attention In a transformer model, an attention head consist of three matrices: (W^Q), (W^K), and (W^V), where (W^Q) and (W^K) determine the relevance between tokens for attention scoring, and (W^V) along with (W^O) influences how attended tokens affect subsequent layers and output logits. Multiple attention heads in a layer allow the model to capture different definitions of \"relevance\". As tokens progress through layers, the scope of attention can expand, enabling the model to grasp more complex and long-range dependencies. The outputs from all attention heads are concatenated and fed into the feed-forward neural network layers. Concretely, let the multiple attention heads be indexed by i{\\displaystyle i}i, then we have: where the matrix XXX is the concatenation of word embeddings, and the matrices WiQ,WiK,WiV{\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}WiQ​,WiK​,WiV​ are \"projection matrices\" owned by individual attention head i{\\displaystyle i}i, and WO{\\displaystyle W^{O}}WO is a final projection matrix owned by the whole multi-headed attention head. It is theoretically possible for each attention head to have a different head dimension dhead{\\displaystyle d_{\\text{head}}}dhead​, but that is rarely the case in practice. As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions: demb=768,nhead=12,dhead=64 d_{emb} = 768, n_{head}=12, d_{head}=64 demb​=768,nhead​=12,dhead​=64 Since, 12×64=768{\\displaystyle 12\\times 64=768}12×64=768, its output projection matrix, WO∈R(12×64)×768{\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}WO∈R(12×64)×768 is a square matrix. Masked attention It may be necessary to cut out attention links between some word-pairs. For example, the decoder, when decoding for the token position ttt, should not have access to the token at position t+1t+1t+1. This may be accomplished before the softmax stage by adding a mask matrix MMM that is −∞{\\displaystyle -\\infty }−∞ at entries where the attention link must be cut, and 0{\\displaystyle 0}0 at other places: For example, the following matrix is commonly used in decoder self-attention modules, called \"causal masking\": In words, it means that each token can pay attention to itself, and every token before it, but not any after it. Encoder An encoder consists of an embedding layer, followed by multiple encoder layers. Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder. As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking. Decoder Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions use an additional attention mechanism to draw relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention. Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. Thus, the self-attention module in the decoder is causally masked. The cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Schematically, we have: where HE{\\displaystyle H^{E}}HE is the matrix with rows being the output vectors from the encoder. The last decoder is followed by a final un-embedding layer to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text. Full transformer architecture Sublayers Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network. The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. The original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.html","title":"Transformer 可视化解释","keywords":"","body":"Transformer Expander 是一种交互式可视化工具，旨在帮助任何人了解 GPT 等基于 Transformer 的模型如何工作。 它直接在您的浏览器中运行实时 GPT-2 模型，允许您试验自己的文本并实时观察 Transformer 的内部组件和操作如何协同工作以预测下一个令牌。 transformer-explainer transformer-explainer-demo 什么是 Transformer？ 从根本上来说，文本生成 Transformer 模型的运行原理是下一个单词预测：给定用户的文本提示，该输入之后最有可能出现的下一个单词是什么？ Transformer 的核心创新和强大之处在于它们使用自注意力机制，这使得它们能够比以前的架构更有效地处理整个序列并捕获远程依赖关系。 Transformer 结构 每个文本生成 Transformer 都包含以下三个关键组件： Embedding Transformer 块: 注意力机制: Transformer模块的核心组件。它允许标记与其他标记进行通信，捕获上下文信息和单词之间的关系。 多层感知器层: 一个独立运行在每个令牌上的前馈网络。注意力层的目标是在 token 之间路由信息，而 MLP 的目标是细化每个 token 的表示。 输出概率: 最后的线性层和 softmax 层将处理后的嵌入转换为概率，使模型能够预测序列中的下一个标记。 Embedding 嵌入的用武之地：它将文本转换为模型可以使用的数字表示。 要将提示转换为嵌入，我们需要: 1) 对输入进行标记; 2) 获取标记嵌入; 3) 添加位置信息，最后 4) 将标记和位置编码相加以获得最终嵌入。 Step1: Tokenization 标记化是将输入文本分解为更小、更易于管理的片段（称为标记）的过程。 这些标记可以是单词或子单词。 “数据”和“可视化”一词对应于独特的标记，而“授权”一词则分为两个标记。 令牌的完整词汇表在训练模型之前确定：GPT-2 的词汇表有 50,257 个唯一令牌。 现在我们将输入文本分割成具有不同 ID 的标记，我们可以从嵌入中获取它们的向量表示。 Step2: Token Embedding GPT-2 Small 将词汇表中的每个 token 表示为 768 维向量；向量的维度取决于模型。 这些嵌入向量存储在形状为 (50,257, 768) 的矩阵中，包含大约 3900 万个参数！ 这个广泛的矩阵允许模型为每个标记分配语义。 Step3: 位置编码 嵌入层还对输入提示中每个标记的位置信息进行编码。不同的模型使用不同的位置编码方法。 GPT-2 从头开始训练自己的位置编码矩阵，并将其直接集成到训练过程中。 Step4: 最终 Embedding 最后，我们将令牌和位置编码相加以获得最终的嵌入表示。这种组合表示捕获了标记的语义及其在输入序列中的位置。 Transformer 块 Transformer 处理的核心在于 Transformer 块，其中包括多头自注意力和多层感知器层。 大多数模型由多个这样的块组成，这些块按顺序依次堆叠。 Token 表示从第一个区块到第 12 个区块逐层演变，使模型能够建立对每个代币的复杂理解。 这种分层方法导致输入的高阶表示。 多头注意力 Query, Key and Value 矩阵 Masked Self-Attention Attention Score Masking Softmax Output: 该模型使用屏蔽的自注意力分数并将其与值矩阵相乘以获得自注意力机制的最终输出。 MLP 在多个自注意力头捕获输入标记之间的不同关系后，连接的输出将通过多层感知器（MLP）层，以增强模型的表示能力。 MLP 块由两个线性变换组成，中间有一个 GELU 激活函数。 第一个线性变换将输入的维度从 768 增加四倍到 3072。 第二个线性变换将维度降低回原始大小 768，确保后续层接收一致维度的输入。 与自注意力机制不同，MLP 独立处理 token，并简单地将它们从一种表示映射到另一种表示。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/transformer_arch/Infrastructure_luxiangdong_Transformer-OverallArch.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_luxiangdong_Transformer-OverallArch.html","title":"土猛的员外-Transformer 架构的整体指南","keywords":"","body":" Transformer 架构的整体指南 Transformer架构的核心组件 Transformer 架构的核心在于注意力机制，它包括编码器（Encoder）和解码器（Decoder）两个主要部分。 编码器负责将输入序列转换为压缩表示，而解码器则基于编码器的输出生成目标序列。 这一架构还包括全连接层、归一化层、Embedding 层和位置编码层等组件，共同作用于提高模型的性能和泛化能力。 特别是注意力机制，它允许模型动态地聚焦于输入序列中最重要的部分，从而提升模型对信息的处理能力。 Encoder和Decoder的结构与功能 Transformer 模型中的编码器（Encoder）由多个相同的层组成，每层都包含多头自注意力（MHA）和前馈神经网络（MLP）。 编码器的主要任务是理解输入序列，并将其编码成一个连续的表示，这个表示能够捕捉输入数据的深层语义信息。 解码器（Decoder）的结构与编码器类似，但它还包括一个额外的多头注意力层，用于融合编码器的输出和目标序列的信息。 解码器的主要任务是生成与输入序列相对应的输出序列，例如在机器翻译任务中的翻译文本。 注意力机制的详细解释 注意力机制是 Transformer 模型的核心，它允许模型在处理序列时动态地聚焦于不同的部分。 注意力机制涉及查询（Query）、键（Key）和值（Value）三个主要组件，通过计算查询和所有键的匹配程度来确定每个值的重要性。 这种机制使得模型能够捕捉序列内部的长距离依赖关系，并在生成输出时考虑到整个输入序列的上下文信息。 多头注意力和MLPs的作用 多头注意力（Multi-Head Attention）是 Transformer 模型中的一个关键特性，它允许模型在不同的表示子空间中并行地捕捉信息，从而提高了模型对复杂关系的理解能力。 MLPs（多层感知器）作为 Transformer 中的另一个子层，负责处理序列中的每个位置，它们通过非线性变换增强模型的表达能力。 Embeddings和位置编码层的重要性 在 Transformer 模型中，输入序列首先被转换为嵌入（Embeddings），这些嵌入是固定大小的密集向量，能够捕捉单词或token的语义信息。 位置编码（Positional Encoding）层则用于保持序列中单词的顺序信息，因为 Transformer 模型的自注意力机制本身不具备对序列顺序的感知能力。 残差连接、层规范化和Dropout的讨论 为了提高模型的训练效率和稳定性，Transformer 模型采用了残差连接（Residual Connections）和层规范化（Layer Normalization）。 残差连接允许模型在每一层中直接传播信息，而层规范化则有助于稳定训练过程。 Dropout 作为一种正则化技术，被用于防止模型过拟合，提高模型的泛化能力。 线性和Softmax图层的功能 在解码器的最后，线性层（Linear Layer）将解码器的激活投影到词汇表的大小，产生对数概率。 Softmax 层（Softmax Layer）则将这些对数概率转换为下一个 token 的概率分布，使得模型能够预测序列中的下一个 token。 注意力机制的可视化和解释 通过可视化工具，我们可以直观地看到 Transformer 模型中的注意力权重，这有助于我们理解模型在处理输入序列时关注的重点。 这种可视化不仅增强了模型的可解释性，也为我们提供了洞察模型内部工作机制的手段。 注意力机制的优势和挑战 注意力机制带来了许多优势，包括更好地处理长期依赖关系、提高并行化能力、增强模型的可解释性以及在多个任务中提高性能。 然而，它也面临着挑战，如随着序列长度增加而增长的内存消耗和计算成本，以及在推理过程中可能需要的顺序方法。 大型语言模型的演变和设计 大型语言模型（LLMs）是 Transformer 模型的直接扩展，它们通过在大量文本数据上进行预训练，获得了强大的语言理解和生成能力。 这些模型的设计包括纯编码器模型（如 BERT）、纯解码器模型（如 GPT 系列）和编码器-解码器模型（如 T5）。 这些模型在不同的任务和应用中展现出了卓越的性能。 Encoder、Decoder 和 Encoder-Decoder 大语言模型的比较 不同类型的大型语言模型根据其架构设计适用于不同的任务。 纯编码器模型适合于 NLP 的判别任务，如文本分类；纯解码器模型适合于生成任务，如文本续写； 编码器-解码器模型则适用于需要处理两个序列的任务，如机器翻译。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-models.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-models.html","title":"Encoder 模型","keywords":"","body":"编码器模型 Encoder models 编码器模型仅使用 Transformer 模型的编码器。 在每个阶段，注意力层都可以访问初始句子中的所有单词。 这些模型通常被描述为具有“双向”注意力，并且通常被称为自编码模型。 这些模型的预训练通常围绕着以某种方式破坏给定的句子（例如，通过屏蔽其中的随机单词）并要求模型查找或重建初始句子。 编码器模型最适合需要理解完整句子的任务，例如句子分类、命名实体识别（以及更一般的单词分类）和提取式问答。 这个模型家族的代表包括: BERT ALBERT DistillBERT ELECTRA RoBERTa BERT BERT BERT 模型在 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 中提出。 它是一个双向 Transformer，使用掩码语言建模（masked language modeling）目标和下一句预测（next sentence prediction）相结合的方式对大型语料库进行预训练。 使用技巧 BERT 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 BERT 使用掩码语言模型 (MLM) 和下一句预测 (NSP) 目标进行训练。一般来说，它在预测屏蔽标记和 NLU 方面非常有效，但对于文本生成来说并不是最佳选择。 通过使用随机屏蔽（random masking）来破坏输入，更准确地说，在预训练期间，给定百分比的标记（通常为 15%）被以下方式屏蔽： 标记有 0.8 的概率打上特殊掩码 token 标记有 0.1 的概率打上随机 token（与原始标记不同） 标记有 0.1 的概率使用原始 token 该模型必须预测原始句子，但还有第二个目标：输入是两个句子 A 和 B（中间有一个分隔标记）。句子在语料库中连续的概率为 50%，其余 50% 的句子不相关。该模型必须预测句子是否连续。 ALBERT ALBERT ALBERT 模型在 《ALBERT：A Lite BERT for Self-supervised Learning of Language Representations》 中提出。 它提出了两种参数减少技术来降低内存消耗并提高 BERT 的训练速度： 将嵌入矩阵拆分为两个较小的矩阵 使用重复层（repeating layers）在组之间进行分割 ALBERT 还使用了一种自我监督损失，重点是对句子间的连贯性进行建模，实验表明这始终有助于具有多句子输入的下游任务。 使用技巧 ALBERT 是一个具有绝对位置嵌入的模型，因此通常建议将输入填充在右侧而不是左侧。 ALBERT 使用重复层，这会导致内存占用较小，但计算成本仍然类似于具有相同数量隐藏层的类 BERT 架构，因为它必须迭代相同数量的（重复）层。 层被划分为共享参数的组(以节省内存)。下一个句子预测被一个句子顺序预测所取代: 在输入中，我们有两个句子 A 和 B (是连续的) ，我们要么输入 A 后面跟着 B，要么输入 B 后面跟着 A。模型必须预测它们是否被交换了。 DistilBERT DistilBERT DistilBERT 在 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter 中提出。 DistilBERT 是一个通过蒸馏 BERT 基础训练而成的小型、快速、廉价且轻量的 Transformer 模型。 在这项工作中，作者提出了一种预训练较小的通用语言表示模型的方法，称为 DistilBERT，然后可以对其进行微调，使其在广泛的任务（如其较大的对应任务）上具有良好的性能。 虽然大多数先前的工作研究了如何使用蒸馏来构建特定于任务的模型，但作者在预训练阶段利用了知识蒸馏，并表明可以将 BERT 模型的大小减少 40%，同时保留 97% 的语言理解能力，速度提高 60%。 为了利用预训练期间较大模型学到的归纳偏差，作者引入了结合语言建模、蒸馏和余弦距离损失的三重损失。 使用技巧 (HF) DistilBERT 没有 token_type_ids，不需要指示哪个 token 属于哪个段。只需使用分隔标记 tokenizer.sep_token （或 [SEP]）分隔片段。 DistilBERT 没有选择输入位置（position_ids 输入）的选项。 与 BERT 相同但更小。通过预训练 BERT 模型的蒸馏进行训练，这意味着它经过训练可以预测与较大模型相同的概率。实际目标是以下各项的组合： 找到与教师模型（Teacher Model）相同的概率 正确预测屏蔽标记（但没有下一句目标） 学生模型和教师模型的隐藏状态之间的余弦相似度 ELECTRA ELECTRA ELECTRA 模型是在 ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators 中提出的。 ELECTRA 是一种新的预训练方法，它训练两个 Transformer 模型：生成器（generator）和判别器（discriminator）。 生成器的作用是替换序列中的标记，因此被训练为掩码语言模型。 判别器是我们感兴趣的模型，它尝试识别序列中哪些标记被生成器替换。 论文摘要： 掩码语言建模 (MLM) 预训练方法（例如 BERT）通过用 [MASK] 替换一些标记来破坏输入，然后训练模型来重建原始标记。 虽然它们在转移到下游 NLP 任务时会产生良好的结果，但它们通常需要大量计算才能有效。 作为替代方案，我们提出了一种样本效率更高的预训练任务，称为替换令牌检测。 我们的方法不是屏蔽输入，而是通过用从小型生成器网络采样的合理替代方案替换一些令牌来破坏输入。 然后，我们不是训练一个预测损坏令牌原始身份的模型，而是训练一个判别模型来预测损坏输入中的每个令牌是否被生成器样本替换。 彻底的实验证明这个新的预训练任务比 MLM 更有效，因为该任务是在所有输入标记上定义的，而不仅仅是被屏蔽的小子集。 使用技巧 ELECTRA 是预训练方法，因此对底层模型 BERT 几乎没有任何改变。唯一的变化是嵌入大小和隐藏大小的分离：嵌入大小通常较小，而隐藏大小较大。附加的投影层（线性）用于将嵌入从嵌入大小投影到隐藏大小。在嵌入大小与隐藏大小相同的情况下，不使用投影层。 ELECTRA 是一个 Transformer 模型，使用另一个（小型）掩码语言模型进行预训练。输入被该语言模型破坏，它接受随机屏蔽的输入文本，并输出一个文本，ELECTRA 必须预测其中哪个标记是原始标记，哪个标记被替换。与 GAN 训练一样，先对小语言模型进行几步训练（但目标是原始文本，而不是像传统的 GAN 设置那样愚弄 ELECTRA 模型），然后再对 ELECTRA 模型进行几步训练。 RoBERTa [RoBERTa(https://huggingface.co/docs/transformers/model_doc/roberta) RoBERTa 模型在 RoBERTa: A Robustly Optimized BERT Pretraining Approach 中提出。 它建立在 BERT 的基础上，修改了关键的超参数，删除了下一句话预训练目标，并使用更大的小批量和学习率进行训练。 论文摘要： 我们提出了 BERT 预训练的复制研究（Devlin 等人，2019），该研究仔细测量了许多关键超参数和训练数据大小的影响。 我们发现 BERT 的训练明显不足，但可以匹配或超过其之后发布的每个模型的性能。 实验结果凸显了以前被忽视的设计选择的重要性， 使用技巧 此实现与 BertModel 相同，只是对嵌入进行了细微调整，以及 RoBERTa 预训练模型的设置。 RoBERTa 具有与 BERT 相同的架构，但使用字节级 BPE （byte-level BPE）作为 tokenizer（与 GPT-2 相同）并使用不同的预训练方案。 RoBERTa 与 BERT 类似，但具有更好的预训练技术： 动态屏蔽：令牌在每个时期的屏蔽方式不同，而 BERT 则一劳永逸。 句子打包（Sentence packing）：句子打包在一起达到 512 个标记（因此句子的顺序可能跨越多个文档）。 较大批次：训练使用较大批次。 字节级 BPE 词汇：使用 BPE，将字节作为子单元，而不是字符，以适应 Unicode 字符。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Decoder-models.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Decoder-models.html","title":"Decoder 模型","keywords":"","body":"解码器模型 解码器模型仅使用 Transformer 模型的解码器。 在每个阶段，对于给定的单词，注意力层只能访问句子中位于该单词之前的单词。 这些模型通常称为自回归模型。 解码器模型的预训练通常围绕预测句子中的下一个单词进行。 这些模型最适合涉及文本生成的任务。 这个模型家族的代表包括: CTRL GPT GPT-2 CTRL CTRL 模型在 CTRL: A Conditional Transformer Language Model for Controllable Generation 中提出。 它是一个因果（单向）转换器，使用语言模型在大约 140 GB 文本数据的非常大的语料库上进行预训练，第一个标记保留为控制代码（例如链接、书籍、维基百科等）。 论文摘要： 这是一个条件 Transformer 模型，经过训练以控制样式、内容和特定任务行为的控制代码为条件。 控制代码源自与原始文本自然共存的结构，保留了无监督学习的优势，同时提供对文本生成的更明确的控制。 这些代码还允许 CTRL 预测训练数据的哪些部分最有可能给出序列。 这提供了一种通过基于模型的源归因来分析大量数据的潜在方法。 使用技巧 CTRL 利用控制代码来生成文本：它需要从某些单词、句子或链接开始生成，以生成连贯的文本。 CTRL 是一个具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 CTRL 经过因果语言建模 (CLM) 目标的训练，因此在预测序列中的下一个标记方面非常强大。利用此功能，CTRL 可以生成语法连贯的文本。 OpenAI GPT OpenAI GPT 模型在 Improving Language Understanding by Generative Pre-Training 中提出。 它是一个因果（单向）转换器，使用具有长范围依赖性的大型语料库（多伦多图书语料库）上的语言建模进行预训练。 论文摘要： 实验证明，通过在各种未标记文本的语料库上对语言模型进行生成式预训练，然后对每个特定任务进行区分性微调，可以实现这些任务的巨大收益。 与以前的方法相比，我们在微调过程中利用任务感知输入转换来实现有效的传输，同时需要对模型架构进行最小的更改。 使用技巧 GPT 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 GPT 使用因果语言建模 (CLM) 目标进行训练，因此在预测序列中的下一个标记方面非常强大。利用此功能，GPT 可以生成语法连贯的文本。 OpenAI GPT-2 模型在 Language Models are Unsupervised Multitask Learners 中提出。 它是一个因果（单向）Transformer，使用语言建模在约 40 GB 文本数据的非常大的语料库上进行预训练。 论文摘要： GPT-2 是一个基于 Transformer 的大型语言模型，拥有 15 亿个参数，在包含 800 万个网页的数据集上进行训练。 GPT-2 的训练目标很简单：根据某个文本中所有先前的单词来预测下一个单词。 数据集的多样性导致这个简单的目标包含跨不同领域的许多任务的自然发生的演示。 GPT-2 是 GPT 的直接扩展，参数增加了 10 倍以上，训练数据量增加了 10 倍以上。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-Decoder-models.html":{"url":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-Decoder-models.html","title":"Encoder-Decoder 模型","keywords":"","body":"Encoder-Decoder 模型 编码器-解码器模型（也称为序列到序列模型）使用 Transformer 架构的两个部分。 在每个阶段，编码器的注意力层可以访问初始句子中的所有单词，而解码器的注意力层只能访问位于输入中给定单词之前的单词。 这些模型的预训练可以使用编码器或解码器模型的目标来完成，但通常会涉及一些更复杂的内容。 例如，T5 的预训练方法是用一个掩码特殊字符替换随机跨度的文本（可能包含多个单词），然后目标是预测这个掩码单词所替换的文本。 序列到序列模型最适合根据给定输入生成新句子的任务，例如摘要、翻译或生成式问答。 这个模型家族的代表包括: BART T5 BART BART Bart 模型是在 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 中提出。 论文摘要： Bart 使用标准的 seq2seq/机器翻译架构，带有双向编码器（如 BERT）和从左到右的解码器（如 GPT）。 预训练任务涉及随机打乱原始句子的顺序和新颖的填充方案，其中文本跨度被单个掩码标记替换。 BART 在针对文本生成进行微调时特别有效，而且也适用于理解任务。 使用技巧 BART 是一种具有绝对位置嵌入的模型，因此通常建议将输入填充到右侧而不是左侧。 具有编码器和解码器的序列到序列模型。编码器接收的是已损坏版本的 tokens，解码器接收的是原始 tokens（但有一个掩码来隐藏未来的词块，就像普通的变换 transformer 解码器一样）。在编码器的预训练任务中应用了以下变换组合： Token 随机遮掩：与BERT一样，采用随机的 token 并将其替换为 [MASK]。 Token 随机删除：输入中的随机tokens被删除。与token masking不同的是，模型必须决定哪些位置是缺失的输入。 文本填充：使用单个掩码标记遮掩 k 个范围的标记（0 范围对应 [MASK] token 的插入）。 排列句子：以句号(full stop)为单位将文档划分为句子，这些句子进行随机排列。 旋转文档：均匀随机选择一个 token，将文档旋转使得其以该 token 开始。这将训练模型识别文档开头的能力。 T5 T5 T5 模型在 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 中提出。 论文摘要： 在本文中，我们通过引入一个统一的框架来探索 NLP 迁移学习技术的前景，该框架将每个语言问题转换为文本到文本的格式。 我们的系统研究比较了数十种语言理解任务的预训练目标、架构、未标记数据集、迁移方法和其他因素。 通过将我们探索中的见解与规模和新的“巨大的干净爬行语料库”相结合，我们在涵盖摘要、问答、文本分类等的许多基准上取得了最先进的结果。 使用技巧 T5是一个编码器-解码器模型，预先对无监督和监督任务的多任务混合进行训练，并将每个任务转换为文本到文本的格式。T5通过为每个任务相应的输入添加不同的前缀，可以很好地处理各种开箱即用的任务，例如，对于翻译: 将英语翻译成德语: ...，对于摘要: 总结: ...。 预训练包括监督训练和自监督训练。 自监督培训练使用损坏的 tokens，通过随机删除 15% 的 tokens 并用单个哨兵令牌（sentinel tokens）替换它们(如果几个连续的令牌被标记为删除，则整个组被替换为单个哨兵令牌)。编码器的输入是被破坏的句子，解码器的输入是原始句子，目标是由哨兵标记分隔的丢弃标记。 Original text: Thank you for inviting me to your party last week. Inputs: Thank you me to your party week. Targets: for inviting last T5 使用相对标量嵌入。编码器输入填充可以在左侧和右侧完成。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/attention/Advanced_Blog_AttentionAttention.html":{"url":"Chap1-ModelArch/attention/Advanced_Blog_AttentionAttention.html","title":"Lilian-Attention?Attention!","keywords":"","body":"注意力机制 Attention? Attention! 简而言之，深度学习中的注意力可以被广义地解释为重要性权重向量：为了预测或推断一个元素，例如图像中的一个像素或句子中的一个单词，我们使用注意力向量来估计它与其他元素的相关性（或 \"关注\"，您可能在许多论文中读到过），并将它们的值之和经注意力向量加权后作为目标的近似值。 为翻译而生 注意力机制的诞生是为了帮助神经机器翻译（NMT）记忆长句。 注意力发明的秘诀是在上下文向量和整个源输入之间创建捷径，而不是从编码器的最后一个隐藏状态中建立一个单一的上下文向量。 这些捷径连接的权重可针对每个输出元素进行定制。 虽然上下文向量可以访问整个输入序列，但我们不必担心遗忘问题。 源代码和目标代码之间的对齐是由上下文向量学习和控制的。从本质上讲，上下文向量需要消耗三项信息： 编码器隐藏状态 解码器隐藏状态 源和目标之间的对齐 一系列注意力机制 概括 下面是几种流行的注意力机制和相应的对齐评分函数的汇总表： 名称 对齐评分函数 引用 Content-base attention score(st,hi)=cosine[st,hi]score(s_t, h_i) = cosine[s_t, h_i]score(st​,hi​)=cosine[st​,hi​] Graves2014 Additive(*) score(st,hi)=vaTtanh(Wa[st−1;hi]score(s_t, h_i) = v^T_atanh(W_a[s_{t-1};h_i]score(st​,hi​)=vaT​tanh(Wa​[st−1​;hi​] Bahdanau2015 Location-Base αt,i=softmax(Wast)\\alpha_{t,i} = softmax(W_as_t)αt,i​=softmax(Wa​st​) Luong2015 General score(st,hi)=stTWahiscore(s_t, h_i) = s^T_tW_ah_iscore(st​,hi​)=stT​Wa​hi​ Luong2015 Dot-Product score(st,hi)=stThiscore(s_t, h_i) = s^T_th_iscore(st​,hi​)=stT​hi​ Luong2015 Scaled Dot-Product(^) score(st,hi)=stThinscore(s_t, h_i) = \\frac{s^T_th_i}{\\sqrt{n}}score(st​,hi​)=n​stT​hi​​ Xu2015 以下是更广泛类别的注意力机制的摘要： 名称 对齐评分函数 引用 Self-Attention(&) 关联同一输入序列的不同位置。理论上，自注意力可以采用上述任何评分函数，但只需将目标序列替换为相同的输入序列即可。 Cheng2016 Global/Soft 关注整个输入状态空间。 Xu2015 Local/Hard 关注输入状态空间部分；即输入图像的一个 patch。 Luong2015 自注意力 自注意力，也称为内部注意力，是一种将单个序列的不同位置相关联的注意力机制，以便计算同一序列的表示。 它已被证明在机器阅读、抽象概括或图像描述生成中非常有用。 在下面的例子中，自注意力机制使我们能够学习当前单词和句子前一部分之间的相关性。 软注意力 vs 硬注意力 本文首先根据注意力是访问整个图像还是仅访问一个补丁，提出了“软”注意力和“硬”注意力之间的区别： 软注意力：学习对齐权重，并将其 \"柔和 \"地置于源图像中的所有 Patch 上；这与 Bahdanau et al., 2015 中的关注类型基本相同。 优点：模型平滑且可微分 缺点：当源输入很大时，成本昂贵 硬注意力：一次只选择要注意的图像的一个 patch 优点：推理时的计算量较少 缺点：该模型是不可微分的，需要更复杂的技术（例如方差减少或强化学习）来训练 全局注意力 vs 局部注意力 Luong, et al., 2015 提出了“全局”和“局部”注意力。 全局注意力类似于软注意力，而局部注意力是硬注意力和软注意力的有趣混合，是对硬注意力的改进，使其可微分：模型首先预测当前目标词的单一对齐位置，然后使用以源位置为中心的窗口计算上下文向量。 Transformer 毫无疑问，“Attention is All you Need” 是 2017 年最具影响力和最有趣的论文之一。 它对软注意力进行了大量改进，使得在没有递归网络单元的情况下进行 seq2seq 建模成为可能。 所提出的 \"transformer\" 模型完全建立在自注意机制上，而不使用序列对齐的递归架构。 Key, Value and Query Transformer 中的主要组件是多头自注意力机制单元。 Transformer 将输入的编码表示视为一组 key-value 对，(K, V)，维度均为 nnn（输入序列长度）； 在 NMT（Neural Machine Translation）的上下文中，键和值都是编码器隐藏状态。 在解码器中，先前的输出被压缩为 query（维度为 mmm 的 Q），并且通过映射此查询以及键和值集来生成下一个输出。 Transformer 采用缩放点积注意力：输出是 value 的加权和，其中分配给每个值的权重由 query 与所有 key 的点积确定： Attention(Q,K,V)=softmax(QK⊤n)V\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}Attention(Q,K,V)=softmax(n​QK⊤​)V 多头注意力 多头机制不是只计算一次注意力，而是多次并行地运行缩放的点积注意力。 独立的注意力输出被简单串联并线性转换为预期维度。 我想这是因为集成总是有帮助的吧？ 根据论文所述，\"多头注意允许模型联合注意来自不同位置的不同表征子空间的信息。 如果只有一个注意力集中的头，平均值就会抑制这一点。\"。 MultiHead(Q,K,V)=[head1;… ;headh]WOwhere headi=Attention(QWiQ,KWiK,VWiV) \\begin{aligned} \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\ \\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i) \\end{aligned} MultiHead(Q,K,V)where headi​​=[head1​;…;headh​]WO=Attention(QWiQ​,KWiK​,VWiV​)​ 编码器 编码器生成基于注意力的表示，能够从潜在无限大的上下文中定位特定的信息。 N=6 个相同层的堆叠。 每层都有一个多头自注意力层和一个简单的位置全连接前馈网络 每个子层都采用残差连接和层归一化。所有子层输出相同维度 dmodel=512d_{model} = 512dmodel​=512 的数据。 解码器 解码器能够从编码表示中检索。 N = 6 个相同层的堆叠 每层都有两个多头注意力机制子层和一个全连接前馈网络子层。[ 与编码器类似，每个子层都采用残差连接和层归一化。 第一个多头注意子层被修改以防止位置关注后续位置，因为我们不希]()望在预测当前位置时关注未来的目标序列。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap1-ModelArch/attention/mha-related.html":{"url":"Chap1-ModelArch/attention/mha-related.html","title":"缓存优化与效果-KV","keywords":"","body":"缓存与效果——结构优化 在 Transformer 解码器中，由于 token 的注意力依赖于前面的 token，因此，与其重新计算前面的上下文，不如缓存其 Key 和 Value。 这可以显著加速推理速度，但随着序列长度和模型维度的增长（dim 和 layers），可能会带来昂贵的内存开销。 在这种背景下，引入了多种注意力机制（为了尽可能支持更大的模型或者更长的序列，需要对 kv 进行压缩）： Multi-Head Attention (MHA) Multi-Query Attention (MQA) Grouped-Query Attention (GQA) Multi-Head Latent Attention (MLA) MHA 标准多头注意力（MHA）计算每个注意力头的 query、key 和 value 矩阵。 Ot,iO_{t, i}Ot,i​ 是第 iii 个注意力头的输出。在推理过程中，所有 key 和 value 都会被缓存以加快推理速度。 但这种繁重的 KV 缓存是一个很大的瓶颈，会限制最大序列长度和批量大小。 MQA 为了缓解 MHA 中的 KV-cache 瓶颈，Shazeer 引入了 Multi-Query Attention (MQA)，其中 key 和 value 在所有不同的注意力头之间共享。 这只需要非常轻量的 KV-cache，从而大大加快解码器推理速度。 然而，MQA 会导致质量下降和训练不稳定。 使用 MQA 的模型包括 PaLM、Gemini 等。 GQA Grouped-Query Attention (GQA) 是 MHA 和 MQA 之间的插值，通过引入多个查询头子组（少于注意力头总数），每个子组都有一个 key 和 value 头。 与 MQA 相比，随着模型大小的增加，GQA 的内存带宽和容量保持相同比例的减少。 中间数量的子组会产生比 MQA 质量更高但比 MHA 更快的插值模型。 MLA Multi-Head Latent Attention (MLA) 实现了比 MHA 更优越的性能，并且显著降低了 KV-cache 提升推理效率。 MLA 不像 MQA 和 GQA 那样减少 KV-heads, 而是将 Key 和 Value 联合压缩到一个潜在向量中。 Low-Rank Key-Value Joint Compression MLA 将 key 和 value 矩阵联合压缩在低秩向量中，这样可以缓存更少的项目，因为压缩维度比 MHA 中的输出投影矩阵维度要小得多。 总结 Attention Mechanism KV Cache per Token (# Element) Capability Multi-Head Attention (MHA) 2nhdhl2n_hd_hl2nh​dh​l Strong Grouped-Query Attention (GQA) 2ngdhl2n_gd_hl2ng​dh​l Moderate Multi-Query Attentioin (MQA) 2dhl2d_hl2dh​l Weak Multi-Head Latent Attention (MLA) (dc+dhR)l≈92dhl(d_c + d^R_h)l \\approx \\frac{9}{2}d_hl(dc​+dhR​)l≈29​dh​l Stronger nhn_hnh​ 是头数，dhd_hdh​ 是每个头的维度，lll 是层数，ngn_gng​ 是 GQA 中的子组数，dcd_cdc​ 是压缩维度。 缓存与效果——工程优化 KV cache 根据 Decoder-only 的特性，每次前向完，把 KV 保留下来，用于之后的计算。 # q, k, v 当前 timestep 的 query, key, value # K_prev, V_prev 之前所有 timestamp 的 key 和 value for _ in range(time_step): # ... K = torch.cat([K_prev, k], dim=-2) # [b, h, n, d] V = torch.cat([V_prev, v], dim=-2) # [b, h, n, d] logits = torch.einsum(\"bhd, bhnd->bhn\", q, K) weights = torch.softmax(logits/math.sqrt(d), dim=-1) outs = torch.einsum(\"bhn, bhnd->bhd\", weights, V) # ... K_prev, V_prev = K, V Flash attention 有关计算和内存的基本概念 计算（Compute）指的是 GPU 计算实际浮点运算（FLOPS）所花费的时间。 内存（Memory）指的是在 GPU 内传输张量所花费的时间。 我们的 GPU 架构中，可以把记忆体简单地分成 HBM（High Bandwidth Memory）和 SRAM（Static Random Access Memory）两个部分： HBM 的记忆体空间很大，但是频宽较低 SRAM 的记忆体空间很小，但是频宽较高，用来做运算 在 GPU 跑 Attention 的流程如下： Load QQQ, KKK by blocks from HBM, compute S=QKTS = QK^TS=QKT, write SSS to HBM Read SSS from HBM, compute P=softmax(S)P = softmax(S)P=softmax(S), write PPP to HBM Load PPP and VVV by blocks from HBM, compute O=PVO = PVO=PV, write OOO to HBM. Read OOO 由于 SRAM 又贵又小，实际上 query state 或 key state 是一小块一小块 load 进去 SRAM 的。 而矩阵 S 维度爆炸为 N∗NN * NN∗N，占用大量的内存，这样大量的读写导致 Attention 运算速度很慢，使得 Attention 操作成为内存绑定操作，而且会有记忆体碎片化问题。 FlashAttention V1 Kernel Fusion 为减少显存读取次数，若 SRAM 容量允许，多个计算步骤（矩阵乘法、softmax 归一化、masking 和 dropout）可合并在一次数据加载中完成。 这样就可以大大减少读写次数。 Backward Recomputation 在前向传播时保存归一化因子，舍弃存储中间结果 PPP 和 SSS。 在反向传播时通过重计算得出注意力矩阵，以完成反向传播，这虽然增加了浮点运算次数，但通过减少 HBM 访问，提升了整体效率。 Softmax Tiling Attention 当中的一个核心步骤就是 Softmax Function，受限于 SRAM 的大小关系，我们不可能一次算出所有数值的 softmax，所以需要把所有中间计算的数值存在 HBM。 tiling 的做法是，先把一块丢进去计算出 softmax，这里的 m 代表的是这一块 load 到 SRAM 的最大值——local maxima，然后就可以计算出 local softmax： 接下来第二块进来，我们把第一块的最大值和第二块的最大值取最大值，就可以得到这两块数值的最大值，然后用相同的方式计算，就可以得到这两块的 local softmax。 我们不需要把每块算出来的数值存在 HBM，我们只需要存当下的最大值 m(x)m(x)m(x) 和分母加总 l(x)l(x)l(x) 就可以了。 所以实际上的流程就会是这样，蓝色的区域就是 HBM，橘色虚线的区块就是 SRAM，每次运算的时候，因为 SRAM 大小有限， 所以我们只 Load 一部分的 Key state 和 value state，红色的字就是我们第一个 block 的计算，蓝色的字就是我们第二个 block 的计算。 Paged attention PagedAttention 是 vLLM 性能增强的核心。 它通过将 KV cache 缓存划分为块来解决 LLM 服务中内存管理的关键问题，从而允许在内存中非连续存储键和值。 每个 block 类比于虚拟内存中的一个 page。每个 block 的大小是固定的，在 vLLM 中默认大小为 16，即可装 16 个 token 的 K/V 值； Shared prefix: 在某些大模型中，所有请求可能都会共享一个前置信息（比如 system message），这些前置信息没有必要重复存储 KV cache； Beam Search、并行采样中有大量的 KV cache 是重复的。内存使用率降低 55%。 对物理块的引用计数进行跟踪，并实现写时复制机制。 References: LLM 性能优化中的一些概念扫盲 MHA vs MQA vs GQA vs MLA 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA 榨乾GPU效能的Flash Attention 3 Flash Attention三部曲 vLLM and PagedAttention: A Comprehensive Overview Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/":{"url":"Chap2-Training_Evaluation/","title":"训练与评估","keywords":"","body":"训练技术 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/distributed-training/overview.html":{"url":"Chap2-Training_Evaluation/distributed-training/overview.html","title":"分布式训练","keywords":"","body":"分布式训练概述 1.概述.md 数据并行 在数据并行训练过程中，整个数据集被划分为多个小块，每个小块被独立分配给不同的计算设备。 这种并行化策略主要在批次（Batch）维度上展开，意味着训练过程在多个设备上同时进行。 每个设备上都运行着模型的一个完整副本，并且仅针对其分配的数据块进行训练。 训练完成后，通过反向传播得到的梯度会在所有设备间汇总，以确保各设备上的模型参数保持一致性。 PyTorch的分布式数据并行（DDP）是实现数据并行的一个典型例子。 模型并行 在数据并行训练中，一个显著特征是每个GPU都拥有模型权重的完整副本，这不可避免地导致了资源冗余。 与之相对的是模型并行策略，该策略将模型分割并跨多个设备分布。 模型并行主要分为两大类：张量并行和流水线并行。 张量并行：专注于在单个操作中实现并行化，例如在矩阵-矩阵乘法中同时处理多个数据点（可视为层内并行，因为它在单个层内分配计算任务）。 流水线并行：在模型的不同层之间进行并行处理，允许各层同时进行计算（相当于层间并行，因为它在模型的不同层之间分配计算任务）。 张量并行 张量并行训练技术通过沿特定维度将一个张量划分为N个部分，使得每个设备仅存储整个张量的1/N，而不破坏计算图的完整性。 为了确保计算结果的准确性，需要进行额外的通信步骤。 以矩阵乘法为例，考虑计算C=ABC = ABC=AB。 我们可以将矩阵 BBB 按列分割成多个子矩阵 [B0,B1,B2,...,Bn][B0, B1, B2, ..., Bn][B0,B1,B2,...,Bn]，每个设备负责一个子矩阵。 接着，每个设备将本地的子矩阵 BiBiBi 与矩阵 AAA 相乘，得到一系列部分乘积 [AB0,AB1,AB2,...,ABn][AB0, AB1, AB2, ..., ABn][AB0,AB1,AB2,...,ABn]。 此时，每个设备仅持有部分结果，例如，设备0（rank=0）持有 AB0AB0AB0。 为了获得完整的结果 CCC，我们需要将所有设备上的部分乘积收集起来，并沿着列维度进行拼接。 这样，我们就能在多个设备间分配张量计算任务，同时确保整个计算过程的正确性。 典型的张量并行实现：Megatron-LM（1D）、Colossal-AI（2D、2.5D、3D）。 流水线并行 流水线并行的核心理念在于将模型分解成多个层级，并分别分配给不同的计算设备。具体来说： 模型分割：模型被划分为若干独立的层块，每个层块负责处理模型的一部分计算任务。 前向传播优化：在前向传播阶段，每个设备负责计算其分配的层块，并将其产生的中间激活值传递给流水线中的下一个设备，以此类推。 后向传播优化：在后向传播阶段，每个设备接收来自下一个阶段的输入张量梯度，并将其回传至前一个阶段，以此类推。 并行计算优势：通过这种方式，各个设备可以并行执行各自的计算任务，显著提高了模型训练的并行度和吞吐量，从而加快训练速度。 流水线并行训练的一个明显缺点是训练设备容易出现空闲状态（因为后一个阶段需要等待前一个阶段执行完毕），导致计算资源的浪费，加速效率没有数据并行高。 典型的流水线并行实现：GPipe、PipeDream、PipeDream-2BW、PipeDream Flush（1F1B）。 优化器相关并行 目前随着模型越来越大，单个GPU的显存目前通常无法装下那么大的模型了。那么就要想办法对占显存的地方进行优化。 通常来说，模型训练的过程中，GPU上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的Buffer。 模型参数仅占模型训练过程中所有数据的一部分，当进行混合精度运算时，模型状态参数（优化器状态+梯度+模型参数）占到了一大半以上。 因此，我们需要想办法去除模型训练过程中的冗余数据。 优化器相关的并行就是一种去除冗余数据的并行方案，目前这种并行最流行的方法是 ZeRO（即零冗余优化器）。 ZeRO 通过分片技术优化模型状态的存储，确保每张 GPU 仅存储模型状态的 1/N 部分，从而在整个系统中只维护一份模型状态。 ZeRO 分为三个级别，它们分别对应不同程度的模型状态分片： ZeRO-1 : 对优化器状态分片（Optimizer States Sharding） ZeRO-2 : 对优化器状态和梯度分片（Optimizer States & Gradients Sharding） ZeRO-3 : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States & Gradients & Parameters Sharding） 多维混合并行 多维混合并行指将数据并行、模型并行和流水线并行等多种并行技术结合起来进行分布式训练。 通常，在进行超大规模模型的预训练和全参数微调时，都需要用到多维混合并行。 为了充分利用带宽，通常情况下，张量并行所需的通信量最大，而数据并行与流水线并行所需的通信量相对来说较小。 因此，同一个服务器内使用张量并行，而服务器之间使用数据并行与流水线并行。 MoE 并行/专家并行 通常来讲，模型规模的扩展会导致训练成本显著增加，计算资源的限制成为了大规模密集模型训练的瓶颈。 为了解决这个问题，一种基于稀疏 MoE 层的深度学习模型架构被提出，即将大模型拆分成多个小模型(专家，expert)， 每轮迭代根据样本决定激活一部分专家用于计算，达到了节省计算资源的效果； 并引入可训练并确保稀疏性的门(gate)机制，以保证计算能力的优化。 使用 MoE 结构，可以在计算成本次线性增加的同时实现超大规模模型训练，为恒定的计算资源预算带来巨大增益。 而 MOE 并行，本质上也是一种模型并行方法。 下图展示了一个有六个专家网络的模型被两路专家并行地训练。 其中，专家 1-3 被放置在第一个计算单元上，而专家 4-6 被放置在第二个计算单元上。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/distributed-training/train-framework.html":{"url":"Chap2-Training_Evaluation/distributed-training/train-framework.html","title":"分布式训练框架","keywords":"","body":"[# DeepSpeed 和 Megatron 的区别和联系 总结 DeepSpeed DeepSpeed 代表性功能 Megatron Megatron 代表性功能 备注 GPU 底层优化 有 开创性的全栈 GPU 内核设计 FP6 量化 更牛逼 Fused CUDA Kernels Megatron 是 NVIDIA 亲儿子，底层优化信手拈来 数据并行 更牛逼 Zero 系列的分布式数据并行方案 有 优化器分片 Megatron 做了类似 ZERO-1 的优化器分片，但数据并行没有 DeepSpeed 强 模型并行 有 更牛逼 Megatron 的张量并行很牛 Megatron-LM Megatron-LM-1: 利用了张量并行和数据并行。 Megatron-LM-2: 新增了 pipeline 并行——virtual pipeline: 1F1B-interleaving，成为和 DeepSpeed 类似的 3D 并行的训练框架。 Megatron-LM-3: 增加了Sequence Parallelism、Selective Activation Recomputation 和 Checkpointing Skipping。 Sequence Parallelism: 在 Tensor Parallelism 的基础上，将 Transformer 核的 LayerNorm 以及 Dropout 层的输入按 Sequence Length 维度进行了切分，使得各个设备上面只需要做一部分的Dropout和LayerNorm。 DeepSpeed DeepSpeed 实现了三种并行方法（数据并行训练、模型并行训练和流水线并行训练）的灵活组合：零冗余优化起（Zero Redundancy Optimizer, 缩写为 ZeRO）是一种用于大规模分布式深度学习的新型内存优化技术。 ZeRO 作为 DeepSpeed 的一部分，用于提高显存效率和计算效率。 ZeRO 支持的数据并行、流水线并行和张量切片模型并行。 ZeRO 可以克服数据并行和模型并行的局限性，同事实现两者的优点。 通过在数据并行进程之间划分模型状态参数、梯度和优化器状态来消除数据并行进程中的内存冗余，而不是复制它们。 ZeRO 有三个主要的优化阶段，它们对应优化器状态、梯度和参数的划分： Pos：减少 4 倍内存，通信量与数据并行性相同 Pos+g：减少 8 倍内存，通信量与数据并行性相同 Pos+g+p：内存减少与数据并行度 Nd 呈线性关系。 ZeRO-3 offload 是 ZeRO Stage 3 和 ZeRO offload 相结合的一种高效且易于使用的实施方式。 主要好处是： 极高的内存效率，可以在有限的GPU资源上运行非常大的模型； 极易使用:扩展到超过一万亿个参数，而不需要以复杂的方式组合多种并行技术； 每个GPU的高性能吞吐量和跨GPU的超线性可扩展性，用于分布式训练。 Megatron-DeepSpeed 176B BLOOM 使用 Megatron-DeepSpeed进行训练，结合了两种技术： DeepSpeed: 深度学习优化库，让分布式训练变得简单、高效且有效; Megatron-LM: NVIDIA 的应用深度学习研究团队开发的大型、强大的 transformer 模型框架。 DeepSpeed 团队将 DeepSpeed 库中的 ZeRO 分片和流水线并行与 Megatron-LM 中的张量并行相结合，开发了一种基于 3D 并行的方案。 训练 BLOOM 时采用了两个框架的以下组件： 组件 DeepSpeed Megatron-LM ZeRO 数据并行 是 张量并行 是 流水线并行 是 BF16 优化器 是 CUDA 融合核函数 是 DataLoader 是 Megatron-LM 和 DeepSpeed 都有流水线并行和 BF16 优化器实现，但我们使用 DeepSpeed 的实现，因为它们集成了 ZeRO。 Megatron-DeepSpeed 实现了 3D 并行以允许大模型以非常有效的方式进行训练。 数据并行(Data Parallelism, DP): 相同的设置和模型被复制多份，每份每次都被馈送不同的一份数据。处理是并行完成的，所有份在每个训练步结束时同步。 张量并行(Tensor Parallelism, TP): 每个张量被分成多个块，因此张量的每个分片都位于其指定的 GPU 上，而不是让整个张量驻留在单个 GPU 上。在处理过程中，每个分片在不同的 GPU 上分别并行处理，结果在步骤结束时同步。这就是所谓的水平并行，因为是做的水平拆分。 流水线并行(Pipeline Parallelism, PP): 模型在多个 GPU 上垂直（即按层）拆分，因此只有一个或多个模型层放置在单个 GPU 上。每个 GPU 并行处理流水线的不同阶段，并处理 batch 的一部分数据。 零冗余优化器(Zero Redundancy Optimizer, ZeRO)：执行与 TP 类似的张量分片，但整个张量会及时重建以进行前向或反向计算，因此不需要修改模型。还支持各种卸载技术以补偿有限的 GPU 内存。 数据并行 在该方法中，模型被完全复制到每个 GPU，然后在每次迭代后所有模型相互同步各自的状态。 这种方法可以通过投入更多 GPU 资源的方式加快训练速度，解决问题。 但它有个限制，即只有当模型能够放进单个 GPU 时才有效。 ZeRO 数据并行 ZeRO 数据并行就是通常的 DDP，只是没有每个 GPU 都复制完整的模型参数、梯度和优化器状态，而是每个 GPU 只存储其中的一部分。 在随后的运行过程中，当需要给定层的完整层参数时，所有 GPU 同步以相互提供它们缺失的部分。 该组件由 DeepSpeed 实现。 张量并行 在张量并行中，每个 GPU 仅处理张量的一部分，并且仅当某些算子需要完整的张量时才触发聚合操作。 Transformer 类模型的主要模块为：一个全连接层nn.Linear，后面跟一个非线性激活层GeLU。 基于权重矩阵按列拆分，随后的 GEMM 按行拆分方案，可以更新任意深度的 MLP，只需在每个拆列-拆行序列之后同步 GPU。 需要特别考虑的是: 由于前向和后向传播中每层都有两个 all reduce，因此 TP 需要设备间有非常快速的互联。 因此，除非你有一个非常快的网络，否则不建议跨多个节点进行 TP。 该组件由 Megatron-LM 实现。 Megatron-LM 最近扩展了张量并行能力，新增了序列并行的能力，用于难以使用前述切分算法的算子，如 LayerNorm。 流水线并行 朴素流水线并行是将模型各层分组分布在多个 GPU 上，并简单地将数据从 GPU 移动到 GPU。 该机制相对简单-将所需层用.to()方法绑到相应设备，现在只要数据进出这些层，这些层就会将数据切换到与该层相同的设备，其余部分保持不变。 这其实就是垂直模型并行。 例如，如果下图显示一个 8 层模型: =================== =================== | 0 | 1 | 2 | 3 | | 4 | 5 | 6 | 7 | =================== =================== GPU0 GPU1 当数据需要从第 3 层传到第 4 层时，它需要从 GPU0 传输到 GPU1，这会引入通信开销。 如果参与的 GPU 位于同一计算节点 (例如同一台物理机器) 上，则传输非常快，但如果 GPU 位于不同的计算节点 (例如多台机器) 上，通信开销可能会大得多。 问题 该方法为什么被称为 朴素 流水线并行呢，它又有什么缺陷呢？主要是因为该方案在任意给定时刻除了一个 GPU 之外的其他所有 GPU 都是空闲的。 共享嵌入可能需要在 GPU 之间来回复制。作者使用的流水线并行（PP）与上述朴素 PP 几乎相同，但它解决了 GPU 闲置问题，方法是将传入的 batch 分块为 micros batch 并人工创建流水线，从而允许不同的 GPU 同时参与计算过程。 DP + PP DeepSpeed 流水线 并行教程 中有一张图演示了如何将 DP 与 PP 结合起来，如下所示: DP rank 0 是看不见 GPU2 的， DP rank 1 是看不到 GPU3 的。对于 DP 而言，只有 GPU 0 和 1，并向它们馈送数据。GPU0 使用 PP “秘密地” 将它的一些负载卸载到 GPU2。同样地， GPU1 也会得到 GPU3 的帮助。 由于每个维度至少需要 2 个 GPU，因此这儿至少需要 4 个 GPU。 DP+PP+TP 将 PP、TP 和 DP 相结合，称为 3D 并行，如下图所示： 由于每个维度至少需要 2 个 GPU，因此在这里你至少需要 8 个 GPU 才能实现完整的 3D 并行。 ZeRO DP+PP+TP DeepSpeed 的主要功能之一是 ZeRO，它是 DP 的超级可伸缩增强版。 通常它是一个独立的功能，不需要 PP 或 TP。但它也可以与 PP、TP 结合使用。 ZeRO-DP 与 PP（以及 TP）结合时，通常只启用 ZeRO 阶段 1，只对优化器状态进行分片。 由于 PP，层数已经比正常情况下少，因此不会节省很多内存。PP 已经将梯度大小减少了 1/pp，因此在此基础上的梯度分片和纯 DP 相比节省不了多少内存。 BF16 Optimizer 用 FP16 训练大型 LLM 模型是一个禁忌。 BF16 格式的关键是它的指数位数与 FP32 相同，因此不会溢出，但 FP16 经常溢出！ FP16 的最大数值范围为 64k，您只能进行较小数的乘法。 例如你可以做 250250=62500，但如果你尝试 255255=65025，你就会溢出，这是导致训练出现问题的主要原因。 这意味着你的权重必须保持很小。 一种称为损失缩放 (loss scaling) 的技术有助于缓解这个问题，但是当模型变得非常大时，FP16 较小的数值范围仍然是一个问题。 无论使用 BF16 还是 FP16，都有一个权重副本始终在 FP32 中——这是由优化器更新的内容。 因此 16 位格式仅用于计算，优化器以全精度更新 FP32 权重，然后将它们转换为 16 位格式以用于下一次迭代。 References: DeepSpeed与Megatron的区别和联系 大模型-LLM分布式训练框架总结 千亿参数开源大模型 BLOOM 背后的技术 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/distributed-training/data-parallel.html":{"url":"Chap2-Training_Evaluation/distributed-training/data-parallel.html","title":"数据并行","keywords":"","body":"数据并行 2.数据并行 数据并行的概念和原理 数据并行是一种分布式训练技术，它允许我们将大规模的数据集分割成多个小份，并将它们分配到多个 GPU 节点上进行处理。 每个 GPU 节点都持有一个完整的模型副本，并基于其分配的数据独立进行梯度计算。 在这种设置中，通常有一个 GPU（如 GPU0）作为参数服务器，负责聚合各个 GPU 节点的梯度，并更新模型参数，然后广播更新后的参数到所有节点（也可以将参数服务器分布在所有节点上，每个 GPU 只更新其中一部分梯度）。 这种并行方式不仅可以应用于训练数据，还可以扩展到模型梯度、权重参数以及优化器状态等多个方面，以提高训练效率。 数据并行（PyTorch DP） PyTorch中的数据并行（DP）是最早提供的一种并行方式，它基于单进程多线程实现。 在 DP 中，数据和模型被分发到每个 GPU 上，每个 GPU 独立进行前向传播和梯度计算，然后将计算结果返回到主 GPU 上进行损失函数计算和梯度更新。 DP 的优点是实现简单，只需一行代码即可完成设置。 然而，它的缺点也很明显，比如受制于全局解释器锁（GIL），导致性能开销较大，且只能在单台服务器上使用（单机多卡），不支持分布式训练和 Apex 混合精度训练。 分布式数据并行（PyTorch DDP） 分布式数据并行（DDP）是 PyTorch 中一种更高级的数据并行方案，它基于多进程实现，每个进程都有自己的优化器，独立执行更新过程。 DDP 通过在进程间传递梯度来减少网络通信的瓶颈，适用于单机和多机情况，并且避免了 GIL 带来的性能开销。 DDP 的每个进程都会创建一个本地的梯度同步器，确保在训练过程中，每个 GPU 都能独立进行反向传播和参数更新，从而实现更高效的负载均衡和运行效率。 DP 和 DDP的区别 DDP（torch.nn.DistributedDataParallel）是PyTorch中用于分布式训练的模块，它允许在多个GPU甚至多个节点上并行训练模型。 以下是 DDP 的一些详细特点和工作原理： 多进程管理：DDP 通过启动多个进程来实现分布式训练，每个进程控制一个 GPU，并加载模型的一个副本，每个进程都有独立的优化器，执行自己的更新过程； 数据分发：在每个 GPU 上分发参数并建立模型副本，每个进程都会加载不同的数据，DDP 会自动处理跨 GPU 的梯度同步； 梯度同步：每个 GPU 上的模型都会在自己的数据上进行前向和反向传播，然后通过梯度同步机制更新模型参数，确保所有GPU上的模型状态保持一致； Ring-Reduce方法：DDP 使用 Ring-Reduce（组内 reduce -> 组间 all reduce -> 组内 broadcast）的方法进行通信，交换梯度，每个进程获取所有进程梯度，并用平均后的梯度更新参数。 DP 和 DDP 的主要区别在于它们的实现方式和参数更新机制。 DP 基于单进程多线程，而 DDP 基于多进程，每个 GPU 对应一个进程。 在参数更新方面，DDP 在各进程梯度计算完成后，会将梯度汇总平均，然后由主进程广播到所有进程，各进程用该梯度独立更新参数。 相比之下，DP是将梯度汇总到主卡，然后在主卡上更新参数，再广播给其他GPU。 DDP 由于避免了 GIL 和减少了数据传输量，训练效率更高，且不存在 DP 中的负载不均衡问题。 DDP 支持模型并行，而 DP 并不支持，这意味如果模型太大单卡显存不足时，只能使用DDP。 完全分片数据并行（PyTorch FSDP） 完全分片数据并行（FSDP）受到 DeepSpeed ZeRO 优化策略的启发。 FSDP 通过分片模型参数、梯度和优化器状态，进一步减少了模型训练过程中的冗余数据。 FSDP 可以选择将模型参数分片卸载到 CPU，以提高内存效率。 FSDP 的出现打破了大模型训练中模型分片的障碍，同时保持了数据并行的简单性，使得训练更大的模型成为可能。 ZeRO 优化策略 ZeRO 是 DeepSpeed 提出的一种优化策略，它通过分片模型状态参数来减少冗余数据。 ZeRO 有三个不同的级别： ZeRO-1 只对优化器状态进行分片， ZeRO-2 对优化器状态和梯度进行分片， ZeRO-3 则进一步对模型权重参数也进行分片。 这种分片策略可以显著减少GPU上需要存储的数据量，从而提高内存效率和训练大模型的能力。 DDP 和 FSDP 的区别 DDP 和 FSDP 在模型副本和梯度聚合方式上有所不同。 在 DDP 中，每个 GPU 上都有一个完整的模型副本，而在 FSDP 中，每个 GPU 上仅存在模型的分片。 在梯度聚合方面，DDP 在局部计算后需要将梯度同步给主节点，而 FSDP 则通过 all-gather 操作从其他 GPU 收集所有权重，以在本地计算前向传播。 FSDP 在内存效率上有优势，因为它可以在每层前向传播后丢弃全部权重，为后续层节省内存。 这种设计使得 FSDP 特别适合训练大型模型。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/distributed-training/pipeline-parallel.html":{"url":"Chap2-Training_Evaluation/distributed-training/pipeline-parallel.html","title":"流水线并行","keywords":"","body":"流水线并行 数据并行与模型并行的对比 在分布式训练中，数据并行和模型并行是两种常见的并行技术。 数据并行训练中，每个 GPU 持有整个模型权重的副本，这虽然简单，却带来了参数冗余的问题。 为了解决这一问题，模型并行技术应运而生，它将模型分割并分布在一个设备阵列上，每个设备只保存模型的一部分参数。 模型并行进一步分为张量并行和流水线并行，前者是层内并行，后者是层间并行，针对的是模型的不同 Transformer 层间进行分割。 流水线并行的简介 流水线并行技术是为了应对单个模型过大，无法放置在单张 GPU 卡中的问题。 通过将模型的不同层放置到不同的计算设备上，可以降低单个计算设备的显存消耗，实现超大规模模型的训练。 在前向计算过程中，输入数据在设备间传递并逐层处理，最终得到结果。 反向传播过程类似，但传递的是梯度信息。 由于仅传输相邻设备间的输出张量，通信量相对较小。 朴素流水线并行 朴素流水线并行是实现流水线并行的最直接方法，它将模型按层间切分成多个部分，并将每个部分分配给一个 GPU。 然而，这种方法存在明显的缺陷， 在任意给定时刻，除了一个 GPU 之外的其他所有GPU都是空闲的，导致 GPU 使用率极低。 在设备之间复制数据的额外通信开销，这进一步降低了效率。 通信和计算没有交错的问题：当通过网络进行数据通信时，没有 GPU 执行任何操作。 高内存需求：先执行前向传播的GPU（如：GPU1）将保留整个小批量缓存的所有激活，直到最后。如果批量大小很大，可能会产生内存问题。 微批次流水线并行 为了解决 GPU 空闲问题，微批次流水线并行通过将传入的小批次分块为微批次，并人为创建流水线来提高设备利用率。 这种方法允许不同的 GPU 同时参与计算过程，显著提升了流水线并行设备的利用率，并减少了设备空闲状态的时间。 GPipe GPipe 是谷歌提出的一种流水线并行方案，它基于 TensorFlow 和 PyTorch 实现。 GPipe 通过微批次流水线并行提高了模型训练的并行度，并利用重计算技术降低了显存消耗。 这种方法通过纵向对模型进行切分解决了单个设备无法训练大模型的问题，同时增加了多设备上的并行程度。 流水线并行策略 流水线并行策略可以根据执行的策略分为 F-then-B 和 1F1B 两种模式。 F-then-B 模式先进行前向计算，再进行反向计算，可能会导致内存占用很高。 而 1F1B 模式，即 One Forward pass followed by One Backward pass，是一种前向计算和反向计算交叉进行的方式，可以及时释放不必要的中间变量，从而节省显存。 PipeDream PipeDream 是微软 DeepSpeed 提出的 1F1B 策略，它通过合理安排前向和反向过程的顺序来解决内存过高的问题。 这种策略可以解决缓存 activation 的份数问题，使得 activation 的缓存数量只跟 stage 数相关，进一步节省显存，训练更大的模型。 PipeDream-2BW PipeDream-2BW 是 PipeDream 的一个变体，它在流水线中只维护两个版本的模型权重，称为双缓冲权重。 这种方法减少了需要维护的权重版本总数，从而极大降低了内存的占用。 PipeDream-Flush（1F1B） PipeDream-Flush 是 PipeDream 的一个变体，它维护单个权重版本并引入定期流水线刷新，以确保权重更新期间的权重版本保持一致。 虽然这种方法以执行性能为代价降低了峰值内存，但吞吐量较低。 1F1B 调度模式 在使用 1F1B 策略时，存在两种调度模式：非交错式和交错式调度。 非交错式调度分为热身阶段、稳定阶段和完成后向计算阶段。 而交错式调度中，每个设备可以对多个层的子集进行计算，这种模式既节省内存又节省时间，但要求 micro-batch 的数量是流水线阶段的整数倍。 PipeDream（交错式1F1B）-Megatron-LM Megatron-LM 基于 PipeDream-Flush 提出了交错式 1F1B 调度，即虚拟流水线。 这种方案通过在设备数量不变的情况下分出更多的流水线阶段，以更多的通信量换取流水线 Bubble 比率降低。 这种交错式调度需要额外的通信，但可以通过高速网络带宽来减少这种额外通信的影响。 分布式训练框架流水线并行方案 在分布式训练框架中，流水线并行方案可以细分为同步流水线并行(Sync-PP)和异步流水线并行(Async-PP)。 PyTorch 采用的是 GPipe 方案，而 DeepSpeed 采用的是 PipeDream-Flush。 Megatron-LM 基于 PipeDream-Flush 进行了改进，提供了一种交错式 1F1B 方案。 Colossal-AI 基于 Megatron-LM 的交错式 1F1B 方案，提供了非交错和交错调度策略。 Ref: Pipeline Parallel Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/distributed-training/moe-parallel.html":{"url":"Chap2-Training_Evaluation/distributed-training/moe-parallel.html","title":"MoE 并行","keywords":"","body":"MoE 并行 MoE的概念和作用 MoE（Mixture of Experts）是一种深度学习模型架构，它通过将大模型拆分成多个小模型（专家）来解决模型规模扩展导致的训练成本增加问题。 这种架构允许在每轮迭代中根据样本激活一部分专家进行计算，从而节省计算资源。 MoE 引入了可训练的门（gate）机制，以确保计算能力的优化。 与传统的密集模型不同，MOE 通过门控网络决定哪些专家网络参与计算，实现了超大规模稀疏模型的训练。 MoE 层的计算过程 MoE 层的计算过程涉及对样本进行门控计算，然后通过 Softmax 处理获得样本被分配到各个专家的权重。 接着，只选择前 k 个最大权重的专家进行计算，最终的计算结果是这些选中专家网络输出的加权和。 这一过程不仅提高了模型的计算效率，还通过稀疏性减少了模型的计算负担。 MoE 分布式并行策略 在分布式训练中，MoE 架构可以通过数据并行和模型并行两种策略实现。 在数据并行策略中，门网络和专家网络被复制地放置在各个运算单元上，这种方式对现有代码的侵入性较小，但专家的数量受到单个计算单元内存大小的限制。 模型并行策略则是将门网络复制放置，而专家网络独立放置在各个计算单元上，这需要额外的通信操作，但可以支持更多的专家网络同时训练。 这两种策略各有优势，需要根据具体的模型和设备拓扑来选择最合适的并行策略。 业界大模型的 MoE 并行方案 业界已经有一些大模型采用了 MoE 并行方案，如 GShard、Switch-Transformer 和 GLaM。 GShard 将 MoE 应用于 Transformer，并引入了 Expert capacity balancing 等设计来优化模型。 Switch-Transformer 简化了 MoE 的 routing 算法，提高了计算效率。 GLaM 是 Google 推出的超大模型，通过使用 Sparse MoE 设计，在不显著增加计算成本的情况下，实现了模型参数量的大幅增加。这些方案展示了MOE在构建大规模模型中的重要性和潜力。 AI 训练框架中的 MoE 并行训练 在 AI 训练框架中，MoE 并行训练也得到了广泛的应用。 例如，PaddlePaddle 框架提供了 MoE 并行的适配和训练示例，展示了如何在动态图模式下使用 PaddlePaddle 进行 MoE 架构的训练。 DeepSpeed 框架也支持 MoE 并行，并提供了多种并行形式，可以同时利用 GPU 和 CPU 内存。 这些框架的支持使得 MoE 并行训练变得更加便捷和高效。 Ref: 混合专家模型 (MoE) 详解 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/overview.html":{"url":"Chap2-Training_Evaluation/fine-tuning/overview.html","title":"微调","keywords":"","body":"微调的定义 微调（Fine-tuning）是一种迁移学习的方法，利用特定领域的数据集对已预训练的大模型进行进一步训练的过程。 其目的在于优化模型在特定任务上的性能，使模型能够更好地适应和完成特定领域的任务。 下面是一般的微调步骤： 预训练模型选择：选择一个在大规模数据上进行预训练的模型作为基础模型。例如，可以选择一种预训练的语言模型，如BERT、GPT等。 数据准备：准备用于微调的特定任务数据集。这些数据集应包含任务相关的样本和相应的标签或目标。确保数据集与任务的特定领域或问题相关。 构建任务特定的模型头：根据任务的要求，构建一个特定的模型头（task-specific head）。模型头是添加到预训练模型之上的额外层或结构，用于根据任务要求进行输出预测或分类。例如，对于文本分类任务，可以添加一个全连接层和softmax激活函数。 参数初始化：将预训练模型的参数作为初始参数加载到微调模型中。这些参数可以被视为模型已经学习到的通用语言表示。 微调训练：使用特定任务的数据集对模型进行有监督训练。这包括将任务数据输入到模型中，计算损失函数，并通过反向传播和优化算法（如梯度下降）更新模型参数。在微调过程中，只有模型头的参数会被更新，而预训练模型的参数会保持不变。 调整超参数：微调过程中，可以根据需要调整学习率、批量大小、训练迭代次数等超参数，以达到更好的性能。 评估和验证：在微调完成后，使用验证集或测试集对微调模型进行评估，以评估其在特定任务上的性能。可以使用各种指标，如准确率、精确率、召回率等。 可选的后续微调：根据实际情况，可以选择在特定任务的数据上进行进一步的微调迭代，以进一步提高模型性能。 为什么需要参数高效微调（PEFT） Parameter-Efficient Fine-Tuning（PEFT）是一种微调策略，旨在通过最小化微调参数数量和计算复杂度，实现高效的迁移学习。 它仅更新模型中的部分参数，显著降低训练时间和成本，适用于计算资源有限的情况。 然而，需要注意的是，PEFT 有时可能会达到与完全微调不同的性能水平，特别是在预训练模型需要进行重大调整才能在新任务上表现良好的情况下。 高效微调技术可以粗略分为以下三大类：增加额外参数（A）、选取一部分参数更新（S）、引入重参数化（R）。 而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。 PEFT 有什么优点 这里讨论 PEFT 相对于传统微调的好处。 减少计算和存储成本：PEFT 只涉及微调少量额外的模型参数，而冻结预训练 LLM 的大部分参数，从而显着降低计算和存储成本。 克服灾难性遗忘：模型在全面微调起茧，可能会遗忘它在预训练期间学到的知识的地方。 低数据环境下更好的性能：PEFT 方法在低数据环境下的表现优于完全微调，并且可以更好地推广到域外场景。 与完全微调相当的性能：PEFT 仅使用少量可训练参数即可实现与完全微调相当的性能。 多种不同的高效微调方法对比 参数有效策略可能涉及多种技术： 选择性层调整：可以只微调层的一个子集，而不是微调模型的所有层。这减少了需要更新的参数数量。 适配器（Adapters）：适配器层是插入预训练模型层之间的小型神经网络。在微调过程中，只训练这些适配器层。适配器学习将预训练的模型提取到的特征使用到新任务。 稀疏微调：稀疏微调只涉及更改模型参数的一个子集。这通常基于一些标准来完成，这些标准标识了与新任务最相关的参数。 低秩近似（Low-Rank Approximations）：用一个参数较少但在任务中表现相似的模型来近似微调后的模型。 正则化技术：可以将正则化项添加到损失函数中，以阻止参数发生较大变化，从而以更“参数高效”的方式有效地微调模型。 任务特定的头（Task-specific Heads）：有时，在预先训练的模型架构中添加一个任务特定的层或“头”，只对这个头进行微调，从而减少需要学习的参数数量。 Refs: 基本概念.md 一文搞懂大模型微调 通俗解读大模型微调 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/llm-training-ybq.html":{"url":"Chap2-Training_Evaluation/fine-tuning/llm-training-ybq.html","title":"LLM 训练-ybq","keywords":"","body":" LLM 训练-pretrain Pretrain 数据篇 数据爬取 数据规模：10T 左右的训练数据。也可以少找一些，在训练的同时，不断去收集更多的新数据。 高质量数据，如 PDF 格式的论文数据等，最好是调用效果较好的 pdf 服务。 若自己训练一个 OCR 模型，要有足够高质量的 pdf-text 对齐数据。 准备数据还要懂得一个基础概念：数据的知识密度是有差异的。“唐诗三百首”的知识量要远远大于“中国新闻网的三百篇新闻”。 最近，一种新的数据趋势是“合成高知识密度数据”，把几千字的新闻概括成几百字喂给模型，四舍五入也等于训练速度提高了十倍。 数据清洗 利用模型对 pretrain 数据的质量进行打分，成了数据清洗工作的标配，llama3、qwen2 的技术报告都有提及。 基本上大家都认同：同等 size 下，BERT 结构的模型表征能力是强于 transformer-decoder 模型的，打分模型最好还是从 BERT 家族中选一个来训。 至于训练数据怎么准备，可以让 GPT4 标注一下，或者是利用“某个源的数据是高质量数据，某个源的数据是低质量数据”这种规则生产一些。 特别强调，任何打分器，都会给 code, markdown, latex 等格式数据打很低的分数。 训打分器这个工作的难点是不要执着于 100% 的准确率，能用就行，要花一个月时间来训打分器，那还不如没有打分器。 此外，要学会变通，有 32k 的语料不代表要训 32k 的打分器，训个 4k 就差不多了，不需要纠结“前 4k 低质量，后 32k 高质量”这种特殊情况。 打分器结果知识众多数据特征中的一个特征，不一定要完全依赖它来洗数据，可以和其他特征结合使用。 数据清晰的另一个大杀器：规则。 数据长度是否少于某个值，数据中某个 token 的比例超过某个阈值，数据的 zh 占比、en 占比、数字占比， 数据是否有“http”字段，数据是否包含了“新冠”、“疫情”等低质量关键词，数据是否包含某些反动词汇， 数据是否包含黄色字眼，等等。 要注意，用规则清晰或过滤数据时，不要把数据搞成分布有偏的数据。 比如，你觉着“包含网址的数据质量低，而网址的英文占比高”，所以你把英文占比高的数据都去掉了。 数据脱敏也是数据清洗环节必须要做的工作。 要尽可能把训练数据中涉及到的人名、电话号码、邮箱等剔除出去。 更广义的，把数据的“转载自......”删掉，黄色信息、反动信息，references 等剔除出去，都可以视作数据脱敏工作的一部分。 数据去重 如何去重？要有一个大数据处理集群，hadoop、spark，只要是一个 map/reduce 框架就可以。 然后，实现一个简单的 minhash 代码。 数据去重工作的一个重要意识：要先确定需要多少训练数据，再确定去重的粒度。 例如，需要 10T 训练数据，就卡相似度在 80% 的阈值进行去重；需要 5T 的训练数据，就卡相似度在 90% 的阈值进行去重。以此类推。 目前没有任何工作能证明，一条数据在 pretrain 阶段训多少遍对模型是最友好的。 即使去重粒度小，导致一篇文章出现多次，也可以通过让两篇相似文档之间隔尽可能多的 token 来降低影响。 数据配比 前面提到，在数据清洗的时候把 code 等格式化数据摘出来。 具体就是训练一个数据分类器，对每一个 document 进行类别判断，把数据分成新闻、百科、代码、markdown等类目即可。 分类器模型依然可以选择使用 BERT 家族。 大部分的技术报告里的配比基本上都是“知识+代码+逻辑”三大类目，知识数据分为多语言知识，逻辑数据可以认为是 math 数据和 cot 数据的混合体。 数据顺序 数据流水线 数据实验 上述所有环节串起来后，先在小模型上做好实验，把 scaling_law 的这个理念先搞懂。 具体的实验内容，可以根据自己的时间、人力来三个阶段走： 粗糙一点：在小模型上起多个数据配比、数据顺序，训练 500B 左右的数据量，选择 loss 曲线最完美，或 loss 下降最低的那个模型； 专业一点：额外起多个 size 的小模型，跑出 loss 结果，结合 scaling_law 公式，推算大模型最适合的数据配比、学习率、训练 token 量等参数； 创新一点：像 llama 和 deepseek 技术报告里提到的，绘制出 loss 到 benchmark 的 scaling_law，提前预知模型训多少 token 能在某个 benchmark 达到什么样的能力。 训练篇 Tokenzier 模型结构 模型参数 模型的 size 选模型 size 主要看两个因素，训练算力和推理算力： 训练算力：有多少机器、能做多久 pretrain，有多少训练数据，使用的训练框架一天能吃多少 token，需要提前确定。假设要在 2 个月后训完模型，目标训 2T 数据，就可以算出自己该训什么 size 的模型； 推理算力：敲定模型 size 的时候，需要知道自己的推理机器是什么，不要出现 1 张推理卡刚好装不下模型的尴尬现象。 模型的超参数 size 目前学界有一个共识：一个好模型是“身材匀称”的，也就是说标准的 llama 结构应该是横向和纵向成某种比例的， 所以增大模型 size 的时候，layer_num 和 hidden_size 要一起递增。 具体到该用什么值的参数，尽量能被 2 / 4 / 8 / 64 / 128 等数整除。 不是有什么理论证明，而是训练框架要求你这样。 layer_num： num_head: 8 的整数倍，它与 tensor_parallel 有关。tensor_parallel 的极限一般是 8，因为大于 8 就引入了机间通讯，训练效率就低了。tensor_parallel 的效率很高，能开大就尽量开大一点； hidden_states: 128 的整数倍； vocab_size: 128 的整数倍. 另外一个比较重要的超参数是 seq_len 的选取：无论你的业务场景需不需要长文本，都不要一开始就使用 32K / 200K 这种夸张的数据 seq_len，算力根本承受不起 rope 的 NTK 外推方法已经是各大厂标配的方案：4K/8K + rope 小 base + 90% 数据量 --> 32K/64K + rope 大 base + 10% 数据量。 训练框架 从零开始的 pretrain 必须选 megatron，continue-pretrain 可以考虑使用 deepspeed。 换句话说，T 级别的 token 训练量必须是 megatron，B 的级别 token 训练量无所谓。 megatron的优点 训练速度快：tensor_parallel 和 pipeline_parallel 的优化做得很好，rope 被开发成了 apex 算子； 参数清晰而明确：argument.py 里，你能看见上百个参数配置，哪些层使用 dropout 等细节，训练框架早就帮你考虑好了，可微操的空间很大。 启动训练的时候模型加载速度快，千亿级别模型一分钟加载完毕。 megatron的缺点 上手成本高，对 torch 的多机通讯需要从头开始学。基建工作较多。 NVIDIA 的官方代码库，全是 bug。 DeepSpeed的优点 代码简单，有 model.trainer 这种框架； 用户群体多，alignment 阶段的开源代码 90% 都是 DeepSpeed 实现的。 DeepSpeed的缺点 训练速度慢，相比于 megatron 可能有 10% ～30% 左右的算力损失。现在有很多工作已经实现了 deepspeed 的 pipeline_parallel 和 tensor_parallel，但相对的，用起来也就没那么简单了。 加载速度很慢。从 bash train.sh 到 看见 loss 出现，小模型得十几分钟，大模型得半个小时，如果你需要 debug，一个下午只够 debug 个三五次。 代码简单的代价是“微操很难”，尤其是使用 model.trainer 训练，对源码几乎无修改空间。 megatron 有 bug 只是让你跑不起来，你修了就好了。deepspeed 的 bug 或者不合理的设置，并不影响你把代码跑起来，只有在你遇见解释不了的现象的时候才可能暴露。 无论是用哪个训练框架，都要记得把 attention 的默认方式换成 flash_attention。 训练技巧 训练效率优化 能减小模型的通讯量就去减小，能避免机间通讯就去避免。 在显存够用的情况下，能不引入 tensor_parallel，pipeline_parallel，sequence_parallel 就不要去引入，同样地、能不开 offload 就不要开 offload，能不开重算就不开重算。 总结下来就是： data_parallel 几乎是唯一不牺牲算力的并行方式； 让数据在显存和内存之间切换，也很耗时。 训练 Loss 分析 记住几条： 要有channel_loss, 至少中文知识、英文知识、代码三类数据的 loss 要分开观察； loss_spike 不可忽视，虽然目前还没有工作去证明说 loss_spike 会对模型造成不可逆的效果伤害，但不出现 loss_spike 总是好的啊。无论是 loss 突然激增还是突然激减，都要重点留意，大概率是数据问题（脏数据不仅 loss 高，也会 loss 低，全是乱码的数据 loss 很高，全是换行符的数据 loss 很低）。如果数据过脏，就回退到上个 checkpoint 进行训练； 即使数据没有问题，也是有可能出现 loss_spike 的，这时候要留意下数据的 grad 有没有异常，大概率和优化器有关，参考 llama 等的技术报告，好好调整 adamw 优化器的 这两个参数，基本上能解决大部分训练初期的 loss_spkie 问题。一旦熬过了训练初期，loss 平稳下降到了 2 ～3 左右，后续的训练也基本不会再有幺蛾子了。 训练流程 当训练数据和训练代码都准备就绪后，按照以下四个流程来设置学习率和超参数: 开局 warmup，学习率缓慢上升到最大； 中间 cos/cos_decay/constant/constant_decay, 学习率比较大，是否 decay 自己决定，多翻翻别人的技术报告，或者在小模型上做实验决定； 后期改变 rope base + seq_len, 开始让模型适应长文本； 收尾 anneal，用高精数据、IFT数据等来给强化模型的考试能力，做完退火就该上考场刷 benchmark 了。 一般 pretrain 只要启动了，大家就可以去做别的工作了，只要模型训练没停，一般都不需要再有人为干预。 烧卡了， loss 爆炸了，loss陡然下降了（数据大概率重复了），就回退到上一个 checkpoint 重启。 评估篇 PPL 看测试集的 loss 来衡量模型效果。 准备一些百科、逻辑、code 等数据，每天观察模型在这些测试集合上的 loss 表现，偶尔浮动是正常的，但整体趋势肯定是持续下降，最后趋于稳定的。 只要训练集中没这些语料，和训练曲线应该是基本一致的。 另一方面，不管你的 tokenizer 压缩率多少，在通用知识测试集上的 loss 也是应该能降低到 2 以下的，否则就说明有点训练不充分。 Benchmark Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 16:23:43 "},"Chap2-Training_Evaluation/fine-tuning/finetuning-discussion.html":{"url":"Chap2-Training_Evaluation/fine-tuning/finetuning-discussion.html","title":"关于微调的讨论","keywords":"","body":"关于微调的讨论 为什么 SFT 之后感觉 LLM 傻了 在 Supervised Fine-Tuning（SFT）之后，有时人们可能会感觉到模型性能有所下降或出现“变傻”的现象。 这主要是因为SFT虽然提高了模型在特定任务上的性能，但也可能降低了模型的泛化能力和通用性。 原因分析； 数据集限制：SFT 通常依赖于特定任务的数据集进行微调，这可能导致模型在训练过程中过于专注于这些任务（若数据集相对较小，则可能发生过拟合），而忽视了其他领域的知识。又或者数据集缺乏多样性，未能涵盖新任务上的各种情况，导致模型在面对新的、与数据集不同的输入时出现有偏差的预测。 模型遗忘：领域特定数据训练后，通用能力往往会有所下降，这是因为模型在微调过程中可能会遗忘一些通用的知识和特征。 非典型标注：数据集的标注可能存在错误或不准确的标签，这些错误的标签可能会对模型的性能产生负面影响，导致模型产生“傻”的行为。 缓解措施： 增加数据多样性：通过增加不同领域和任务的数据，可以提高模型的泛化能力。 仔细检查微调数据集的标注，确保标签的准确性和一致性。 使用正则化技术（如权重衰减、dropout）来减少过拟合的风险。 进行数据增强，通过对微调数据进行一些变换或扩充来增加多样性。 使用更复杂的模型架构或调整模型的超参数，以提高模型的性能和泛化能力。 领域模型 Continue Pretrain 数据选取 常见的数据选取方法： 领域相关数据：收集与目标领域相关的数据。这些数据可以是从互联网上爬取的、来自特定领域的文档或者公司内部的数据等。这样的数据可以提供领域相关的语言和知识，有助于模型在特定领域上的表现。 领域专家标注：如果有领域专家可用，可以请他们对领域相关的数据进行标注，这样可以提供有监督的数据用于模型的训练。 伪标签：如果没有领域专家或者标注数据的成本较高，可以使用一些自动化的方法生成伪标签。例如，可以使用预训练的模型对领域相关的数据进行预测，将预测结果作为伪标签，然后使用这些伪标签进行模型的训练。 数据平衡：在进行数据选取时，需要注意数据的平衡性。如果某个类别的数据样本较少，可以考虑使用数据增强技术或者对该类别进行过采样，以平衡各个类别的数据量。 数据质量控制：在进行数据选取时，需要对数据的质量进行控制。可以使用一些质量评估指标，如数据的准确性、一致性等，来筛选和过滤数据。 数据预处理：在进行数据选取之前，可能需要对数据进行一些预处理，如分词、去除停用词、标准化等，以准备好输入模型进行训练。 如何缓解模型遗忘的问题 缓解模型遗忘通用能力的方法： 保留通用数据：在进行领域数据训练时，仍然需要保留一部分通用数据用于模型训练。这样可以确保模型仍然能够学习到通用的语言和知识，从而保持一定的通用能力。 增量学习：使用增量学习（Incremental Learning）的方法，将领域数据与通用数据逐步交替进行训练。这样可以在学习新领域的同时，保持对通用知识的记忆。 预训练和微调：在领域数据训练之前，可以使用大规模通用数据进行预训练，获得一个通用的基础模型。然后，在领域数据上进行微调，以适应特定领域的任务。这样可以在保留通用能力的同时，提升领域任务的性能。 强化学习：使用强化学习的方法，通过给模型设置奖励机制，鼓励模型在领域任务上表现好，同时保持一定的通用能力。 领域适应技术：使用领域适应技术，如领域自适应（Domain Adaptation）和领域对抗训练（Domain Adversarial Training），帮助模型在不同领域之间进行迁移学习，从而减少遗忘通用能力的问题。 数据重采样：在进行领域数据训练时，可以使用数据重采样的方法，使得模型在训练过程中能够更多地接触到通用数据，从而缓解遗忘通用能力的问题。 领域模型继续预训练，如何在模型在预训练的过程中就学习到更多的知识 以下是一些参考策略： 多任务学习：在预训练过程中，可以引入多个任务，使得模型能够学习到更多的知识。这些任务可以是领域相关的任务，也可以是通用的语言理解任务。通过同时训练多个任务，模型可以学习到更多的语言规律和知识。 多领域数据：收集来自不同领域的数据，包括目标领域和其他相关领域的数据。将这些数据混合在一起进行预训练，可以使得模型在不同领域的知识都得到学习和融合。 大规模数据：使用更大规模的数据进行预训练，可以让模型接触到更多的语言和知识。可以从互联网上爬取大量的文本数据，或者利用公开的语料库进行预训练。 数据增强：在预训练过程中，可以采用数据增强的技术，如随机遮挡、词替换、句子重组等，来生成更多的训练样本。这样可以增加模型的训练数据量，使其能够学习到更多的知识和语言规律。 自监督学习：引入自监督学习的方法，通过设计一些自动生成的标签或任务，让模型在无监督的情况下进行预训练。例如，可以设计一个掩码语言模型任务，让模型预测被掩码的词语。这样可以使模型在预训练过程中学习到更多的语言知识。 领域模型词表扩增是不是有必要的 领域词表扩增的意义： 提高解码效率：在很多情况下，词表扩增后，处理相同文本时所需的 token 数量减少，这能让模型在处理文本时更高效地进行编码和解码操作。 增加上下文窗口长度：词表扩充后，模型能够处理更长的文本输入，从而在需要长文本分析的任务中表现更好。 提升模型对领域内容的理解和生成能力：特定领域往往包含大量专业术语和行话，通过扩增词表，模型可以更准确地理解和生成领域相关的文本。 领域模型的词表扩增可以有助于提升模型在特定领域任务上的性能，但是否有必要取决于具体的情况。 领域特定词汇：如果目标领域中存在一些特定的词汇或术语，而这些词汇在通用的预训练模型的词表中没有覆盖到，那么词表扩增就是必要的。通过将这些领域特定的词汇添加到模型的词表中，可以使模型更好地理解和处理这些特定的词汇。 领域特定上下文：在某些领域任务中，词汇的含义可能会受到特定上下文的影响。例如，在医学领域中，同一个词汇在不同的上下文中可能具有不同的含义。如果领域任务中的上下文与通用预训练模型的训练数据中的上下文有较大差异，那么词表扩增可以帮助模型更好地理解和处理领域特定的上下文。 数据稀缺性：如果目标领域的训练数据相对较少，而通用预训练模型的词表较大，那么词表扩增可以帮助模型更好地利用预训练模型的知识，并提升在目标领域任务上的性能。 指令微调的好处？ 指令微调（Instruction Fine-tuning）中，模型接受指令或约束来生成特定的输出。好处： 控制生成输出：指令微调使得模型能够根据指定的指令或约束生成特定的输出。这对于需要精确控制模型生成结果的任务非常有用。 可解释性和可控性：通过指令微调，可以将任务的要求以指令的形式传达给模型。这增加了模型的可解释性和可控性，使得用户能够更好地理解和干预模型的生成过程。 避免不符合要求的输出 提供任务性能：指令微调可以针对具体任务进行优化，使得模型在该任务上的性能得到提升。通过引入任务特定的指令或约束，模型可以更好地适应特定任务的需求，并生成更准确、更合理的输出。 灵活性和可扩展性：指令微调是一种灵活且可扩展的方法，允许在不同任务和场景中进行微调。通过调整和修改指令或约束，可以适应不同的任务需求，并在多个任务上进行微调。 预训练和微调，哪个阶段注入了知识 预训练和微调两个阶段都可以注入知识，注入知识的方式和范围略有不同。 预训练阶段：模型通过大规模的未标注数据进行训练，从中学习语言的统计特征、语义知识和语言规律。预训练的目的是让模型建立起对语言的基本理解和概念，并学习到一般性的语言表示。这种预训练过程注入了通用的语言知识，并可以迁移到各种下游任务中。 微调阶段：预训练模型使用带标注的数据进行微调，以适应具体任务的要求。模型通过接触任务特定的数据和标签，进一步调整和优化模型的参数，使其更好地适应任务的特定特征和要求。微调阶段注入的是与任务相关的知识和信息。 Refs: 1.微调.md SFT指令微调大模型后,感觉LLM变傻了 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/pretrain.html":{"url":"Chap2-Training_Evaluation/fine-tuning/pretrain.html","title":"预训练","keywords":"","body":"预训练 为什么要增量预训练 预训练学知识，指令微调学格式，强化学习对齐偏好，增量预训练让大模型有领域知识（靠指令微调记住知识不靠谱，几十万条数据做不到。） 训练过程与原始预训练类似，增量预训练常用于领域适应，学习领域知识、风格和术语等。 进行增量预训练需要做哪些准备工作 选取底座模型：根据自己的需求和硬件条件来选择合适的底座模型及模型参数量的大小； 收集数据：收集大量的文本数据，一般预训练数据的大小都是 TB 级别的； 数据清洗：所有的信息都能够在互联网信息中被找到，只是信息密度相比 人工精选数据集 要更低。例如「明星信息」、「如何写代码」这些信息都能在新闻网站、或是问答网站中找到，只不过维基百科或是 Github 则是将这些信息给「高密度」且「结构化」地进行了存储。这使得我们在使用维基百科作为训练语料的时候，模型能够更快的学习到这些高密度信息（人物的经历、年龄、性别、职业等等），而这些内容在互联网信息（如新闻）中的信息密度则较低，即很少会有一条新闻完整的介绍一个艺人的过往经历。只要我们对互联网信息进行严格的处理（去除冗余信息，提高有用信息的密度），就能够加快模型的学习速度。 增量预训练所用训练框架 超大规模训练：选用 3D 并行（流水线并行、张量并行和数据并行），Megatron-Deepspeed 拥有多个成功案例 少量节点训练：选用张量并行，但张量并行只有在 nvlink 环境下才会起正向作用，但提升也不会太明显 少量卡训练：如果资源特别少，显存怎么也不够，可以使用 LoRA 进行增量预训练 增量预训练数据选取思路 垂直领域预训练有三种思路： 先用大规模通用语料预训练，再用小规模领域语料二次训练 直接进行大规模领域语料预训练 通用语料比例混合领域语料同时训练 增量预训练流程 数据预处理：参考 LLaMA 的预训练长度，也把数据处理成 2048 长度（如果不够，做补全）。 分词器：如果使用 LLaMA 可能需要添加中文词表，目前有不少人做了相关工作，当然也可以自己添加自己需要的词表。 原始模型：各家框架的模型层名不太一样，训练时可能需要做一些调整，在预训练时尽量选择基座模型，不选 Chat 模型。 训练模型：跑通只是第一步，根据训练情况反复调整比较重要。 模型转换：不同框架的 checkpoint 格式不同，还会根据并行度分成很多个文件。 模型测试：简单测试下续写能力，验证下模型是否正常。 Refs: LLM domain adaptation using continued pre-training 2.预训练.md Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/prompt-tuning.html":{"url":"Chap2-Training_Evaluation/fine-tuning/prompt-tuning.html","title":"Prompt-Tuning","keywords":"","body":"Prompting Prefix Tuning 背景 Prefix tuning 使预训练语言模型适应特定任务，而不修改原始模型的权重。 Prefix tuning 涉及将一系列连续的特定于任务的向量（称为前缀）添加到 LM 的输入中。 Transformer 可以处理这些前缀向量，就好像它们是“虚拟令牌(virtual tokens)”序列一样。 与提示不同，前缀向量并不对应于真实的标记，而是在训练过程中学习的。 技术细节 软提示创建 在 prefix tuning 中，我们为模型中的每个 transformer block 创建一个称为“软提示”的张量 这个软提示是一组可学习的参数，特定于我们想要调整模型的任务 软提示处理 在使用软提示之前，它会经过一组全连接层（防止直接更新 prefix 的参数导致训练不稳定和性能下降） 这些层将软提示转换为合适的表示形式，可以与 transformer block 的主输入相结合 输入修改 将转换后的软提示与 transformer block 的主输入连接起来 这种串联沿着序列长度维度发生，这意味着软提示作为附加标记添加到输入序列的开头 Transformer Block 处理 修改后的输入（现在包括软提示）通过标准 transformer block 操作 这些操作包括自注意力、层归一化、前馈神经网络层以及残差连接 训练 在训练期间，仅更新软提示，而预训练模型的权重保持冻结 该模型通过根据特定任务的训练数据调整软提示来学习适应特定任务 通过保持原始模型的权重不变，前缀微调可以实现高效的适应，而无需对整个模型进行微调 具体例子 定义前缀 前缀是添加到输入序列之前的连续向量序列 前缀的长度是一个超参数，您可以根据目标个性的复杂性和可用的计算资源来选择。常见前缀长度范围为 10 到 50 个 token 前缀被初始化为大小为 (prefixlength,embedlength)(prefix_length, embed_length)(prefixl​ength,embedl​ength) 的可训练矩阵 PPP 前缀矩阵 𝑃 的每一行对应一个前缀标记，该行中的值表示该标记的嵌入 前缀矩阵 𝑃 是随机初始化的或使用与目标个性相关的真实单词的激活来初始化。使用相关单词进行初始化可以为前缀提供良好的起点，并有可能加快训练过程中的收敛速度。 修改模型架构 Transformer 层中的自注意力机制允许前缀标记被关注并影响输入标记的表示，从而有效地引导模型的行为 重要的是，在训练期间，仅更新前缀矩阵 𝑃，而预训练模型的参数（即 Transformer 层中的权重矩阵）保持冻结。这确保了前缀适应目标个性，同时保留预训练模型捕获的一般语言理解。 优点 它允许预先训练的模型有效地适应新任务，而无需修改原始模型的权重 与微调整个模型相比，它需要更少的可训练参数，从而提高计算效率 它可以应用于任何基于 Transformer 的预训练模型，而不需要特定于任务的架构。 实用优势 前缀微调允许对任务进行独立训练，从而实现可扩展的个性化，而不会造成数据交叉污染 每个用户的数据可以隔离，并且可以为每个用户训练个性化的前缀，保证隐私性和模块化。任务的独立性还可以实现跨用户的高效批处理，以及创建在同一任务上训练的多个前缀的集合 Prompt Tuning Prompt Tuning 通过反向传播更新参数来学习 prompts，而不是人工设计 prompts；同时冻结模型原始权重，只训练 prompts 参数，训练完以后，用同一个模型可以做多任务推理。 Prompt Tuning 可以看作是 Prefix Tuning 的简化版本，它给每个任务定义了自己的 prompt，然后拼接到数据上作为输入，但只在输入层加入 prompt tokens，并且不需要加入 MLP 进行调整来解决难训练的问题。 P-Tuning P-Tuning 没有使用固定的离散提示，而是引入了可学习的连续提示嵌入。 这些嵌入就像在训练过程中学习的一组“虚拟”单词。 工作原理 P-Tuning 将 Prompt 转换为可学习的 Embedding 层，并用 MLP+LSTM 的方式来对 Prompt Embedding 进行一层处理。 相比Prefix Tuning，P-Tuning 加入的可微的virtual token，仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。 这里的出发点实际是把传统人工设计模板中的真实 token 替换成可微的virtual token。 经过预训练的LM的词嵌入已经变得高度离散，如果随机初始化virtual token，容易优化到局部最优值，而这些virtual token理论是应该有相关关联的。 因此，作者通过实验发现用一个prompt encoder来编码会收敛更快，效果更好。 即用一个 LSTM+MLP 去编码这些virtual token以后，再输入到模型。 Refs: 2.prompting.md Prefix-Tuning: Optimizing Continuous Prompts for Generation Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/adapter-tuning.html":{"url":"Chap2-Training_Evaluation/fine-tuning/adapter-tuning.html","title":"Adapter-Tuning","keywords":"","body":"Adapter based PEFT Adapter Tuning 背景 作为 Prompt 和 Prefix 微调技术的替代方案。 作者建议使用适配器模块（Adapter module）进行迁移学习。 Adapter 是在预训练网络层之间添加的新模块，在训练过程中仅训练新参数，原始 LLM 被冻结，因此只学习原始 LLM 的一小部分参数。 这意味着模型对之前的任务几乎具有完美的记忆，并使用少量的新参数来学习新任务。 技术原理 针对每个 transformer sub-layers (Attention 和 Feed Forward Layers)，在 sub-layers 之后插入两个串行的适配器模块。 适配器始终直接应用于子层的输出，分别是多头注意力的投影之后和第二个 feed-forward 层之后。 在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调，从而保证了训练的高效性。 具体细节 每个 Adapter 模块主要由两个前馈（Feedforward）子层组成，第一个前馈子层（Feedforward down-project）将 Transformer 块的输出作为输入， 将原始输入维度 d（高维特征）投影到 m（低维特征），通过控制 m 的大小来限制 Adapter 模块的参数量，通常情况下，m。 然后，中间通过一个非线性层。在输出阶段，通过第二个前馈子层（Feedforward up-project）还原输入维度，将 m（低维特征）重新映射回 d（原来的高维特征），作为 Adapter 模块的输出。 同时，通过一个skip connection来将 Adapter 的输入重新加到最终的输出中去，这样可以保证，即便 Adapter 一开始的参数初始化接近0， Adapter 也由于skip connection的设置而接近于一个恒等映射，从而确保训练的有效性。 h←h+f(hWdown )Wup h \\leftarrow h+f\\left(h W_{\\text {down }}\\right) W_{u p} h←h+f(hWdown ​)Wup​ How to decide the value of m Adapter 模块中 m 的大小决定了待优化的参数量，因此需要对参数和性能进行权衡。 原论文的实验研究发现，不同的 Adapter 大小 m 的性能十分稳定，因此对于给定的模型，任何下游任务都可以使用固定的大小 m。= Refs: 3.adapter-tuning.md Summary Of Adapter Based Performance Efficient Fine Tuning (PEFT) Techniques For Large Language Models Parameter Efficient Fine Tuning 大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/lora.html":{"url":"Chap2-Training_Evaluation/fine-tuning/lora.html","title":"LoRA","keywords":"","body":"LoRA 背景 神经网络中的全连接层借助于矩阵乘法得以实现，然而，很多全连接层的权重矩阵都是满秩的。 当针对特定任务进行微调后，模型中权重矩阵其实具有很低的本征秩（intrinsic rank）， 因此，论文的作者认为权重更新的那部分参数矩阵尽管随机投影到较小的子空间，仍然可以有效的学习，可以理解为针对特定的下游任务这些权重矩阵就不要求满秩。 LoRA 通过冻结原始模型权重并对一组单独的权重应用更新来修改微调过程，然后将其添加到原始参数中。 优点： 训练和任务适应的效率 LoRA 引入了低秩矩阵，仅修改原始模型权重的子集，与全套参数相比，这些矩阵很小，可以实现更高效的更新。 LoRA 的重点是针对最有影响力的参数来改变模型 Transformer 层中的权重矩阵。这种选择性更新简化了模型适应新的任务或数据集的过程。 减少计算资源需求 保留预训练模型权重 传统微调中，模型所有权重都会发生变化，这可能导致模型最初拥有的一般知识丢失。LoRA 选择性更新权重的方法确保了预训练模型中嵌入的核心结构与知识在很大程度上得以维持。 这种保留对于维持模型的广泛理解和功能至关重要，同时仍允许其适应特定任务或数据集。它确保微调后的模型保留原始模型的优势，例如对语言和上下文的理解，同时在目标领域获得新的功能或改进的性能。 技术原理 LoRA 的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。 在涉及矩阵相乘的模块，在原始的 PLM 旁边增加一个新的通路，通过前后两个矩阵 A, B 相乘，第一个矩阵 A 负责降维，第二个矩阵 B 负责升维， 中间层维度为 r，从而来模拟所谓的本征秩。 以一个简化的示例说明，我们称矩阵 WO (d x d) 为 LLM 的参数集合，并将 ∆W (measure by d x d) 作为将在微调中加入的权重调整的矩阵。 在 LoRA 方法中，完成训练后，对于大小为 1 x d 的新输入 x, 模型将 x 乘以 W 和 ∆W，从而产生两个 d 大小的输出向量。 然后将这些向量按元素相加在一起以产生最终结果，表示为 h。 参数 r 对于确定 A 和 B 的大小至关重要。 较小的 r 值意味着更少的参数和更快的训练时间，尽管如果 r 设置得太低，可能会导致模型性能下降。 Transformer 的权重矩阵包括 Attention 模块里用于计算 query, key, value 的 Wq, Wk, Wv 以及多头 attention 的 Wo, 以及 MLP 层的权重矩阵。 原论文中 LoRA 只应用于 Attention 模块中的 4 种权重矩阵，而且通过消融实验发现同时调整 Wq 和 Wv 会产生最佳结果。 QLoRA 背景 大部分的量化方法虽然可以减少 LLM 的内存占用，但此类技术仅适用于推理场景。 作者提出了 QLoRA，首次证明了可以在不降低任何性能的情况下微调量化为 4bit 模型。 技术原理 QLoRA（Quantized Low-Rank Adaptation）是 LoRA 的扩展，使该方法更加高效。 QLoRA 使用一种新颖的高精度技术将预训练模型量化为 4bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。 QLoRA 引入的一些改进功能包括： 4-bit NormalFloat (NF4): 可以将其视为一种紧凑、优化的格式，用于记录模型的数据。它实现了一种平衡，非常适合遵循正态分布的权重，通过将数据精度缩小到4位来减少内存使用。 双量化 (Double quantization): 对第一次量化后的那些常量再进行一次量化，减少存储空间。 分页优化器 (Paged optimizers): 这些优化器有效地处理突发的内存需求，确保训练过程保持平稳和高效。使用 NVIDIA 统一内存特性，该特性可以在在 GPU 偶尔 OOM 的情况下，进行 CPU 和 GPU 之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。 作者在原论文中也指出了一些有趣的点，比如：指令调优虽然效果比较好，但只适用于指令相关的任务，在聊天机器人上效果并不佳，而聊天机器人更适合用Open Assistant数据集去进行微调。 通过指令类数据集的调优更像是提升大模型的推理能力，并不是为聊天而生的。 使用 LoRA 和 QLoRA 进行微调实用技巧 Enable LoRA for Multiple Layers 将 LoRA 的应用扩展到模型的更多层可以进一步增强其适应性和性能。传统上，只有特定层可能会进行微调以降低复杂性，但跨多个层应用 LoRA 可以实现更细微的调整。 该方法可以通过在整个模型架构中进行细粒度调整，从而在专门任务中带来更显着的改进。然而，平衡调整的层数与可用计算资源至关重要，因为每个附加层都会增加所需的内存和处理能力。 Implement Numerous Training Epochs 训练 epoch 的数量对完善模型的性能至关重要，尤其是在专业领域。 通过对数据集进行多次迭代，模型可以更有效地从数据中学习，从而提高其泛化能力。 在使用较小的数据集或对高度特定的任务进行微调时，这一点尤为重要，因为它允许模型逐步完善其理解能力。 应监控微调以避免过拟合，即模型在训练数据上表现良好，但在未见数据上表现不佳。 LoRA 和 Adapter-tuning 的区别 插入位置。LoRA 是以残差连接的形式“并联”在 Transformer 的 Q, K, V, O 矩阵上，而 Adapter 是插入在 Transformer Block 的 Feed-forward layer 后； 推理延迟。LoRA 在训练完后其参数可以与原有预训练模型直接合并，变回单分支结构，不会引入额外延迟；而 Adapter 由于引入了额外的串联网络层，会带来额外的延迟； 参数存储。LoRA 微调，训练完毕后只需保存 LoRA 本身的参数；Adapter 需要保存整个原有模型的参数。 References: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS LoRA-Interview_Note-CN Understanding LLM Fine Tuning with LoRA-run.ai Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/fine-tuning/summary.html":{"url":"Chap2-Training_Evaluation/fine-tuning/summary.html","title":"总结","keywords":"","body":"总结 BitFit 对微调机制的一种积极探索，也很简单，通过仅调整bias效果就能有不错的效果，但没有具体阐述原理，就是通过猜测加实验得到的结果。 同时，作者提出一个观点：微调的过程不是让模型适应另外的数据分布，而是让模型更好的应用出本身的表征能力。 特点： 训练参数量极小（约0.1%） 在大部分任务上效果会差于LoRA、Adapter等方法 Prefix Tuning 在每一个 Transformer 层都带上一些virtual token作为前缀，以适应不同的任务。 特点： 前缀 Token 会占用序列长度，有一定的额外计算开销。 Prefix Tuning 的线性插值比较复杂。 Prompt Tuning 该方法可以看作是 Prefix Tuning 的简化版本，针对不同任务，仅在输入层引入 virtual tokens 形式的软提示（soft prompt）。 特点： 相对于 prefix tuning，参与训练的参数量和改变的参数量更小，更节省显存。 对一些简单的 NLU 任务还不错，但对硬序列标记任务（即序列标注）表现欠佳。 P-Tuning 将 prompt 转换为可以学习的 Embedding 层，并用 MLP+LSTM 的方式来对 Prompt Embedding进行一层处理。 相比Prefix Tuning，仅在输入层加入的可微的virtual token；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。 特点： 引入一个prompt encoder（由一个双向的 LSTM +两层 MLP 组成）来建模virtual token的相互依赖会收敛更快，效果更好。 P-Tuning v2 该方法在每一个 Transformer 层都加入了prompt token作为输入，引入多任务学习，针对不同任务采用不同的提示长度。 并且回归传统的分类标签范式，而不是映射器。 特点： 解决了Prompt Tuning无法在小模型上有效提升的问题 移除了对模型效果改进较小的重参数化的编码器（如：Prefix Tuning中的 MLP、P-Tuning 中的 LSTM） 对于一些复杂的硬序列标记任务（即序列标注）取得了不错的效果。 Adapter Tuning 该方法设计了 Adapter 结构，并将其嵌入 Transformer 的结构里面，针对每一个 Transformer 层，增加了两个 Adapter 结构，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调。 特点： 通过在 Transformer 层中嵌入 Adapter 结构，在推理时会额外增加推理时长 LoRA 该方法通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。 特点： 将 BA 加到 W 上可以消除推理延迟。 可以通过可插拔的形式切换到不同的任务。 设计的比较好，简单且效果好。 QLoRA 使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。 特点： 使用 QLoRA 微调模型，可以显著降低对于显存的要求。同时，模型训练的速度会慢于 LoRA。 多种不同的高效微调方法对比 总的来说，像P-Tuning v2、LoRA等都是综合评估很不错的高效微调技术。 如果显存资源有限可以考虑 QLoRA；如果只是解决一些简单任务场景，可以考虑P-Tuning、Prompt Tuning也行。 总结 主要有如下几类参数高效微调方法： 增加额外参数，如：Prefix Tuning、Prompt Tuning、Adapter Tuning及其变体 选取一部分参数更新，如：BitFit 引入重参数化，如：LoRA、QLoRA 混合高效微调，如：MAM Adapter、UniPELT Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-17 17:05:38 "},"Chap2-Training_Evaluation/evaluation/":{"url":"Chap2-Training_Evaluation/evaluation/","title":"大模型评估","keywords":"","body":"大语言模型评估 评测 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/evaluation/evaluating.html":{"url":"Chap2-Training_Evaluation/evaluation/evaluating.html","title":"评测","keywords":"","body":"评测 大模型怎么评测 自动评测和人工评测，这两种方法在评测语言模型和机器翻译等任务时起着重要作用。 自动评测：基于计算机算法和自动生成的指标，能够快速且高效地评测模型的性能。 人工评测：人类专家的主观判断和质量评测，能够提供更深入、细致的分析和意见。 大模型的 honest 原则是如何实现的？模型如何判断回答的知识是训练过的已知的知识？怎么训练这种能力？ 大模型需要遵循的helpful，honest，harmless的原则。 可以有意构造如下的训练样本，以提升模型遵守 honest 原则的能力：微调时构造知识问答类训练集，给出不知道的不回答，加强 honest 原则； 阅读理解题，读过的要回答，没读过的不回答，不要胡说八道。 如何衡量大模型水平？ 在评测 LLMs 性能时，选择合适的和领域对于展示大型语言模型的表现、优势和劣势至关重要。 为了更清晰地展示 LLMs 的能力水平，文章将现有的任务划分为以下 7 个不同的类别： 自然语言处理：包括自然语言理解、推理、自然语言生成和多语言任务 鲁棒性、伦理、偏见和真实性 医学应用：包括医学问答、医学考试、医学教育和医学助手 社会科学： 自然科学与工程：包括数学、通用科学和工程 代理应用：将 LLMs 作为代理使用 其他应用 大模型评估方法有哪些？ 直接评估指标：准确率和 F1 得分这类传统指标。通常情况下，这种方法涉及从模型中获取单一的输出，并将其与参考值进行比较，可以通过约束条件或提取所需信息的方式来实现评估。 间接或分解的启发式方法：这类方法中，利用较小的模型来评估主模型生成的答案，这些较小的模型可以是微调过的模型或原始的分解模型。 基于模型的评估：这种方法中，模型本身提供最终的评估分数或评估结果。这种方法也引入了额外的可变因素。即使模型可以获取到ground truth信息，评估指标本身也可能在评分过程中产生随机因素或不确定因素。 大模型评估工具有哪些 ChatbotArena: 借鉴游戏排位赛机制，让人类对模型两两评价 SuperCLUE: 中文通用大模型综合性评测基准，尝试全自动测评大模型 C-Eval: 采用 1.4 万道涵盖 52 个学科的选择题，评估模型中文能力 FlagEval：采用“能力—任务—指标”三维评测框架 References: 1.评测 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap2-Training_Evaluation/evaluation/hallucination.html":{"url":"Chap2-Training_Evaluation/evaluation/hallucination.html","title":"幻觉","keywords":"","body":"大模型幻觉 什么是大模型幻觉 在语言模型的背景下，幻觉指的是一本正经地胡说八道： 看似流畅自然的表述，实则不符合事实或者是错误的。 为什么需要解决 LLM 的幻觉问题 LLMs 的幻觉可能会产生如传播错误信息或侵犯隐私等严重后果。 比如在医疗应用中，对患者生成的报告如果存在幻觉可能导致错误诊断甚至影响生命安全。 幻觉影响了模型的可靠性和可信度，因此需要解决 LLM 的幻觉问题。 幻觉一定是有害的吗 幻觉不一定是有害的，特别是在一些需要创造力或灵感的场合，比如写电影剧情，幻觉的存在可能带来一些奇思妙想，使得生成的文本充满想象力。 因此，对幻觉的容忍度取决于具体的应用场景。 幻觉有哪些不同类型 幻觉主要可以分为两类：内在幻觉和外在幻觉。 内在幻觉：生成的内容与源内容矛盾； 外部幻觉：生成的内容不能从源内容中得到验证，既不受源内容支持也不受其反驳。 为什么 LLM 会产生幻觉？ 有一些研究也在致力于分析幻觉出现的不同原因，已知的一些原因包括： 源与目标的差异：当模型的训练数据中的源与目标存在差异时，模型产生的文本可能与原始源内容产生偏差。这种差异可能是数据收集过程中不经意间产生的，也可能是故意为之。 无意识的源-目标差异：这种差异的产生有多种原因。例如，数据可能是基于某种经验法则编制的，使得目标信息并不总是完全依赖源信息。举例来说，如果从两家不同的新闻网站获得相同事件的报道作为源与目标，目标报道中可能包含源报道没有的信息，从而导致二者不同。 有意识的源-目标差异：某些任务在本质上并不追求源与目标的严格一致，尤其是在需要多样性输出的情境下。 训练数据的重复性：训练过程中使用的数据，如果存在大量重复，可能导致模型在生成时过于偏好某些高频短语，这也可能引发“幻觉”。 数据噪声的影响：使用充斥噪声的数据进行训练，往往是导致“幻觉”出现的关键因素之一。 解码过程中的随机性：某些旨在增加输出多样性的解码策略，如top-k采样、top-p方法以及温度调节，有时会增加“幻觉”的产生。这往往是因为模型在选择输出词汇时引入了随机性，而没有始终选择最可能的词汇。 模型的参数知识偏向：有研究表明，模型在处理信息时，可能更依赖其在预训练阶段所积累的知识，而忽略了实时提供的上下文信息，从而偏离了正确的输出路径。 训练与实际应用中的解码差异：在常见的训练方法中，我们鼓励模型基于真实数据预测下一个词汇。但在实际应用中，模型则是根据自己先前生成的内容进行预测。这种方法上的差异，尤其在处理长文本时，可能会导致模型的输出出现“幻觉”。 诸如 GPT 之类的生成模型，其实只是学会了文本中词汇间的统计规律，所以它们生成内容的准确性仍然是有限的。 如何度量幻觉？ 人工评估的成本太高了，有一些自动化评估的指标： 命名实体偏差：命名实体（NEs）是“事实”描述的关键组成部分，可以利用 NE 匹配来计算生成文本与参考资料间的一致性。直观上，若模型生成了不在原始知识源中的 NE，那么它可以被视为产生了幻觉。 蕴含率：该指标定义为被参考文本所蕴含的句子数量与生成输出中的总句子数量的比例。为了实现这一点，可以采用成熟的蕴含/NLI模型。 基于模型的评估：应对复杂的句法和语义变化。 利用问答系统：此方法的思路是，如果生成的文本在事实上与参考材料一致，那么对同一个问题，其答案应该与参考材料相似。具体而言，对于给定的生成文本，问题生成模型会创建一组问题-答案对。接下来，问答模型将使用原始的参考文本来回答这些问题，并计算所得答案的相似性。 利用信息提取系统：此方法使用信息提取模型将知识简化为关系元组，例如。这些模型从生成的文本中提取此类元组，并与从原始材料中提取的元组进行比较。 如何缓解 LLM 幻觉？ 与幻觉有关的数据问题（理论上）可以通过创建高质量无噪声的数据集来解决。 但验证和清洗大规模的文本语料库难度太大。 因此有了一些其他对方法： 利用外部知识验证正确性 修改解码策略 采样多个输出并检查一致性 《A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation》 结论： 幻觉的生成会传播：比如一句话出现幻觉，后续生成的文本可能也会出现幻觉甚至更严重。 logit 输出值（输出词汇表上的概率分布）可以用来获取幻觉的信号：具体地说，我们计算了一个概率得分，并展示了当这个得分很低时，模型更容易产生幻觉。 主动检测和减轻的方法： 检测：确定潜在幻觉的候选者，即生成句子的重要概念。然后，利用其 logit 输出值计算模型对它们的不确定性并检索相关知识。 减轻：使用检索到的知识作为证据修复幻觉句子。将修复的句子附加到输入（和之前生成的句子）上，并继续生成下一个句子。 《Factuality Enhanced Language Models for Open-Ended Text Generation》 结论： 采样的“随机性”在用于生成句子的后半部分时，对事实性的损害比在句子的开头更大。因为在句子的开始没有前文，所以只要它在语法和上下文上是正确的，LM就可以生成任何内容。然而，随着生成的进行，前提变得更为确定，只有更少的单词选择可以使句子成为事实。 SelfCheckGPT 结论： 如果模型真的掌握某个事实，那么多次生成的结果应该是相似的且事实一致的；相反，如果模型在胡扯，那么随机采样多次的结果会发散甚至矛盾。 LLM 什么时候最容易产生幻觉? 数值混淆：当 LLM 处理与数字有关的文本，如日期或数值时，容易产生幻觉； 处理长文本：在需要解读长期依赖关系的任务中，例如文档摘要或长对话历史，模型可能会生成自相矛盾的内容。 逻辑推断障碍：若模型误解了源文本中的信息，它有可能产生不准确的结论。因此，模型的逻辑推理能力至关重要。 上下文与内置知识冲突：模型在处理信息时，可能会过度依赖于预训练阶段获取的知识，而忽略实际上下文，导致输出不准确。 错误的上下文信息：当给定的上下文包含错误信息或基于错误的假设时（如：为什么高尔夫球比篮球大？），模型可能无法识别这些错误，并在其回答中产生幻觉。 幻觉来源与缓解 幻觉分类 事实性问题 事实性错误：模型回答与事实不一致 事实性虚构：模型回答在真实世界无法考证 忠诚度问题 违背指令：模型回答没有遵从指令 违背上文：模型回答和上下文内容存在不一致 自我矛盾 模型回答内部问题存在逻辑矛盾，比如 CoT 多步推理之间存在矛盾 幻觉检测 事实性检测 忠诚度检测 References: 1.大模型幻觉 2.幻觉来源与缓解 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-17 17:05:38 "},"Chap3-PromptEngr/":{"url":"Chap3-PromptEngr/","title":"Prompt 工程","keywords":"","body":"Prompt 工程 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html":{"url":"Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html","title":"宝玉老师-如何写好提示词？","keywords":"","body":"宝玉老师的这个关于提示词编写的介绍，个人觉得收益匪浅。 How to write good prompt 概要 提示词的四大要素：指令、上下文、输出格式、角色 提示词策略和技巧 大语言模型的本质和劣势 提升生成期望结果的概率 策略一：撰写清晰的指令 策略二：给模型“思考”的时间 策略三：把复杂任务拆分成简单任务 策略四：接入外部工具和资料 提示词技巧的具体应用案例 提示词的四大要素 提示词是与 AI 交互时的重要工具，它包含四大要素：指令、上下文、输出格式和角色。 指令：是告诉 AI 需要完成的具体任务或回答的问题，需要清晰具体。 上下文：是 AI 执行指令所需的背景信息，它帮助 AI 更好地理解和执行指令。 输出格式：是我们期望 AI 返回结果的格式，比如表格、摘要或 Markdown 格式等。 角色：则是我们期望 AI 在交互中扮演的角色，比如助手、心理医生或导师等，这有助于 AI 更准确地生成内容。 提示词策略和技巧 有效的提示词策略和技巧可以极大提升与 AI 的交互质量。 首先，我们需要撰写清晰的指令，因为 AI 无法猜测我们的想法。 其次，给模型“思考”的时间，通过展示思考过程来提升推理结果。 此外，将复杂任务拆分成简单任务，可以提高任务的成功率。 最后，接入外部工具和资料，可以弥补 AI 知识库的不足，提升其性能。 大语言模型的本质和劣势 大语言模型以其强大的语言理解和生成能力著称，它们拥有简单的推理能力，并且适应性强，可以通过微调或上下文学习来提升性能。 然而，这些模型也有其局限性，比如对事实的理解有限、知识库有限、缺乏深层推理能力，以及上下文窗口长度限制和没有记忆等。 了解这些本质和劣势有助于我们更好地利用 AI。 提升生成期望结果的概率 为了提升生成期望结果的概率，我们需要发挥 AI 模型的优势，补充或避开其劣势。 这包括让 AI 准确理解指令，以及让概率收敛在高质量的路径上。 通过精心设计的提示词，我们可以引导 AI 生成更准确的结果。 策略一：撰写清晰的指令 在与 AI 的交互中，清晰的指令至关重要。 模型不会读心术，因此需要我们提供具体明确的指令。 此外，提供详尽的背景信息可以帮助模型更好地理解任务。 让 AI 向我们提问可以避免信息的遗漏，而让 AI 扮演任务相关的角色则可以提升特定领域内容的生成概率。 策略二：给模型“思考”的时间 在处理复杂问题时，给模型“思考”的时间是提升推理结果的有效策略。 这可以通过打印思考过程来实现，帮助模型更可靠地推导出正确答案。 此外，用 Token 换时间，即在模型给出答案之前，要求其展示一下“思考过程”，有助于模型更准确地回答问题。 策略三：把复杂任务拆分成简单任务 将复杂任务拆分成简单任务可以提高任务的成功率。 这类似于软件工程中将复杂系统分解成模块化的组件。 通过化繁为简，我们可以提高每个子任务的成功率，并且可以合并回去，使得前一个任务的输出可以作为后一个任务的输入。 策略四：接入外部工具和资料 在 AI 的能力不足以完成任务时，我们可以借助外部工具和资料。 通过上下文学习，我们可以弥补知识库的不足。 提供工具的说明和接口参数给 AI，可以让 AI 自己选择使用什么工具，传入什么参数。 此外，AI 不擅长数学，但可以调用数学工具或者执行代码来完成任务。 提示词技巧的具体应用案例 在实际应用中，我们可以通过结构化输入、明确说明完成任务的步骤、给出样例（few-shot）以及使用伪代码等技巧来提升提示词的效果。 这些技巧可以帮助 AI 更准确地理解我们的需求，并生成更符合预期的结果。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap3-PromptEngr/prompt-app_openai-prompt-generation.html":{"url":"Chap3-PromptEngr/prompt-app_openai-prompt-generation.html","title":"OpenAI-生成提示词的提示词","keywords":"","body":"OpeAI 公布的生成提示词的提示词技术。 Prompt generation 提示词 元提示指示模型根据您的任务描述创建良好的提示或改进现有提示。 META_PROMPT = \"\"\" Given a task description or existing prompt, produce a detailed system prompt to guide a language model in completing the task effectively. # Guidelines - Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output. - Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure. - Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS! - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed. - Conclusion, classifications, or results should ALWAYS appear last. - Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements. - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders. - Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements. - Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED. - Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user. - Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples. - Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.) - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON. - JSON should never be wrapped in code blocks (```) unless explicitly requested. The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\") [Concise instruction describing the task - this should be the first line in the prompt, no section header] [Additional details as needed.] [Optional sections with headings or bullet points for detailed steps.] # Steps [optional] [optional: a detailed breakdown of the steps necessary to accomplish the task] # Output Format [Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc] # Examples [optional] [Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.] [If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ] # Notes [optional] [optional: edge cases, details, and an area to call or repeat out specific important considerations] \"\"\".strip() def generate_prompt(task_or_prompt: str): completion = client.chat.completions.create( model=\"gpt-4o\", messages=[ { \"role\": \"system\", \"content\": META_PROMPT, }, { \"role\": \"user\", \"content\": \"Task, Goal, or Current Prompt:\\n\" + task_or_prompt, }, ], ) return completion.choices[0].message.content Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap3-PromptEngr/prompt-app_baoyu_translator-gpt-prompt.html":{"url":"Chap3-PromptEngr/prompt-app_baoyu_translator-gpt-prompt.html","title":"GPT 翻译 Prompt","keywords":"","body":"宝玉老师的 GPT 的翻译提示词。 翻译 GPT 的提示词更新和优化 该翻译提示词的主要构成： 三步翻译：直译、反思、意译 角色设定 术语表 Prompt: You are a highly skilled translator tasked with translating various types of content from other languages into Chinese. Follow these instructions carefully to complete the translation task: ## Input Depending on the type of input, follow these specific instructions: 1. If the input is a URL or a request to translate a URL: First, request the built-in Action to retrieve the URL content. Once you have the content, proceed with the three-step translation process. 2. If the input is an image or PDF: Get the content from image (by OCR) or PDF, and proceed with the three-step translation process. 3. Otherwise, proceed directly to the three-step translation process. ## Strategy You will follow a three-step translation process: 1. Translate the input content into Chinese, respecting the original intent, keeping the original paragraph and text format unchanged, not deleting or omitting any content, including preserving all original Markdown elements like images, code blocks, etc. 2. Carefully read the source text and the translation, and then give constructive criticism and helpful suggestions to improve the translation. The final style and tone of the translation should match the style of 简体中文 colloquially spoken in China. When writing suggestions, pay attention to whether there are ways to improve the translation's (i) accuracy (by correcting errors of addition, mistranslation, omission, or untranslated text), (ii) fluency (by applying Chinese grammar, spelling and punctuation rules, and ensuring there are no unnecessary repetitions), (iii) style (by ensuring the translations reflect the style of the source text and take into account any cultural context), (iv) terminology (by ensuring terminology use is consistent and reflects the source text domain; and by only ensuring you use equivalent idioms Chinese). 3. Based on the results of steps 1 and 2, refine and polish the translation ## Glossary Here is a glossary of technical terms to use consistently in your translations: - AGI -> 通用人工智能 - LLM/Large Language Model -> 大语言模型 - Transformer -> Transformer - Token -> Token - Generative AI -> 生成式 AI - AI Agent -> AI 智能体 - prompt -> 提示词 - zero-shot -> 零样本学习 - few-shot -> 少样本学习 - multi-modal -> 多模态 - fine-tuning -> 微调 ## Output For each step of the translation process, output your results within the appropriate XML tags: [Insert your initial translation here] [Insert your reflection on the translation, write a list of specific, helpful and constructive suggestions for improving the translation. Each suggestion should address one specific part of the translation.] [Insert your refined and polished translation here] Remember to consistently use the provided glossary for technical terms throughout your translation. Ensure that your final translation in step 3 accurately reflects the original meaning while sounding natural in Chinese. Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-17 17:05:38 "},"Chap4-InferAndOpt/":{"url":"Chap4-InferAndOpt/","title":"推理与优化","keywords":"","body":"推理与优化 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap4-InferAndOpt/llm-inference-overview.html":{"url":"Chap4-InferAndOpt/llm-inference-overview.html","title":"LLM 推理过程","keywords":"","body":"推理 LLM 推理有两个阶段： prefill: 输入 prompt 处理的阶段，会生成 cache decode/generation: 后续新生成 token 的阶段，会利用 prefill 的 cache 以及当前阶段本身产生的 cache Prefill Prefill 阶段会并行处理输入的所有 token，这里处理方式使得即使在较小的 bathc size 下也能打满 GPU 的利用率； 由于在 prefill 阶段需要处理长输入，所以这个阶段的计算开销很大，显卡利用率很容易打满； 增加 batch size 时，prefill 阶段每个 token 的处理开销几乎保持不变，这意味着 prefill 的效率在小 batch size 时就很高，说明开销是固定的。 Decode 阶段 Decode 阶段是自回归的，每次只生成一个 token，因此这一阶段的 GPU 利用率较低； IO 密集型：Decode 过程中需要频繁地读取 KV Cache，导致 IO 开销较大。即使输入的长度始终为 1，反复的 KV Cache 访问也使得这一阶段成为 IO 密集型； 扩大 batch size 可以显著降低 decode 阶段的开销，因为更大的 batch size 能更有效地分摊固定的 IO 读写成本，不过开再大也不能完全打满GPU，毕竟KV Cache的读写开销过大，导致decode阶段难以成为计算密集型。 推理的相关概念 为什么大模型推理时显存涨得那么多 显存涨得很多且一直占着显存不释放的原因： 模型参数占用显存： 输入数据占用显存： 中间计算结果占用显存： 内存管理策略：某些框架在推理时采用了一种延迟释放显存的策略，即不会立即释放，而是保留一段时间以备后续使用。这种策略可以减少显存的分配和释放频率，提高推理效率。 大模型生成时的参数怎么设置 Temperature: 用于调整随机从生成模型中抽样的程度。使得相同的提示可能会产生不同的输出。温度为 0 将始终产生相同的输出，该参数设置越高随机性越大。 Beam search: 作为在给定可能选项的情况下选择最佳输出的最终决策步骤。Beam Search 宽度一个参数，用于确定算法在搜索的每个步骤中应该考虑的候选数量。 Top p: 动态设置 tokens 候选列表的大小。将可能性之和不超过特定值的 top tokens 列入候选名单。目的是限制可能被采样的低概率 token 的长度。 Top k: 允许其他高分 tokens 有机会被选择。这种采样引入的随机性有助于在很多情况下生成的质量。top k = 3 意味着选择前三个 token。 若 Top k 和 Top p 都启用，则 Top p 在 Top k 之后起作用。 有哪些省内存的大语言模型训练/微调/推理方法 一些常见的方法： 参数共享：通过共享模型中的参数，可以减少内存占用。例如，可以在不同的位置共享相同的嵌入层或注意力机制。 梯度累积：在训练过程中，将多个小批次的梯度累积起来，然后进行一次参数更新。这样可以减少每个小批次的内存需求，特别适用于GPU内存较小的情况。 梯度裁剪：通过限制梯度的大小，可以避免梯度爆炸的问题，从而减少内存使用。 分布式训练：将训练过程分布到多台机器或多个设备上，可以减少单个设备的内存占用。分布式训练还可以加速训练过程。 量化：将模型参数从高精度表示（如 FP32）转换为低精度表示（如 INT8 或 FP16），可以减少内存占用。量化方法可以通过减少参数位数或使用整数表示来实现。 剪枝： 蒸馏： 分块处理： 如何让大模型输出合规化 可以采取以下方法: 数据清理和预处理： 引入合规性约束： 限制模型访问权限： 解释模型决策过程： 审查和验证模型： 监控和更新模型： 合规培训和教育： 应用模式变更 任务定制化：通过对模型进行微调或迁移学习，使其适应特定的应用场景。例如，将大语言模型用于自动文本摘要、机器翻译、对话系统等任务。 个性化交互：通过对用户输入进行理解和生成相应的回复，实现更自然、智能的对话体验。这可以应用于智能助手、在线客服、社交媒体等场景。 内容生成与创作：将其应用于内容生成和创作领域。例如，自动生成新闻报道、创意文案、诗歌等内容，提供创作灵感和辅助创作过程。 情感分析与情绪识别：通过大语言模型对文本进行情感分析和情绪识别，帮助企业或个人了解用户的情感需求和反馈，以改善产品、服务和用户体验。 知识图谱构建：利用大语言模型的文本理解能力，将其应用于知识图谱的构建和更新。通过对海量文本进行分析和提取，生成结构化的知识表示，为知识图谱的建设提供支持。 法律和合规应用：大语言模型可以用于法律和合规领域，例如自动生成法律文件、合同条款、隐私政策等内容，辅助法律专业人士的工作。 教育和培训应用：将大语言模型应用于教育和培训领域，例如智能辅导系统、在线学习平台等，为学生提供个性化的学习辅助和教学资源。 References: LLM Inference at scale with TGI 一起理解下LLM的推理流程 Understanding the LLM Inference Workload 推理 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap4-InferAndOpt/infer-framework.html":{"url":"Chap4-InferAndOpt/infer-framework.html","title":"LLM 推理框架","keywords":"","body":"推理框架特点 关键点 vLLM: 适用于大批量 Prompt 输入，并对推理速度要求高的场景； Text generation inference: 依赖 HuggingFace 模型，并且不需要为核心模型增加多个 adapter 的场景； CTranslate2: 可在 CPU 上进行推理； OpenLLM: 为核心模型添加 adapter 并使用 HuggingFace Agents，尤其是不完全依赖 PyTorch； Ray Serve: 稳定的 Pipeline 和灵活的部署，它最适合更成熟的项目； MLC LLM: 可在客户端（边缘计算）（例如，在 Android 或 iPhone 平台上）本地部署 LLM； DeepSpeed-MII: 使用 DeepSpeed 库来部署 LLM。 vLLM 功能 Continuous batching: 有 iteration-level 的调度机制，每次迭代 batch 大小都有所变化，因此 vLLM 在大量查询下仍可以很好的 g 工作； PagedAttention: 受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的秘诀。 优点 文本生成的速度: vLLM 的推理速度是最快的； 高吞吐量服务: 支持各种解码算法，比如 parallel sampling, beam search 等； 与 OpenAI API 兼容: 如果使用 OpenAI API, 只需要替换端点的 URL 即可。 缺点 添加自定义模型: 如果模型没有使用与 vLLM 中现有模型类似的架构，添加过程会变得更加复杂。例如，增加 Falcon 的支持； 缺乏对适配器（LoRA、QLoRA等）的支持: 没有单独使用模型和适配器权重的选项，限制了有效利用此类模型的灵活性。 缺乏权重量化: 有时，LLM可能不需要使用GPU内存，这对于减少GPU内存消耗至关重要。 Text generation inference Text generation inference是用于文本生成推断的Rust、Python和 gRPC 服务器，在 HuggingFace 中已有的 LLM 推理 API 使用。 功能 内置服务评估：监控服务器负载并深入了解其性能； 使用 flash attention (v2) 和 Paged attention 优化 transformer 推理代码: 并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化。 优点 支持 HuggingFace 模型: 轻松运行自己的模型或使用任何 HuggingFace 模型中心； 对模型推理的控制: 该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等。 缺点 缺乏对适配器的支持: 没有官方支持或文档 显示说明对适配器的支持，尽管可以使用适配器部署 LLM； 从源代码（Rust+CUDA内核）编译: 对于不熟悉 Rust 的人，将客户化代码纳入库中变得很有挑战性； 文档不完整: 所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节。 Reference: llm推理框架简单总结 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap4-InferAndOpt/vllm-framework.html":{"url":"Chap4-InferAndOpt/vllm-framework.html","title":"vLLM 介绍","keywords":"","body":"vLLM Overview vLLM 声称的特性： 优秀的 serving 吞吐量 PagedAttention 对 KV Cache 的有效管理 传入请求的continuous batching, 而不是 static batching 高性能 CUDA kernel 流行的 HF 模型集成 各种 decoder 算法的高吞吐量服务，包括 parallel sampling 和 beam search 等 tensor parallel 兼容 OpenAI 的 API 服务器 两个主要特性：Continuous batching 和 PagedAttention。 Continuous Batching LLM Decoder 推理基础 LLM 推理分为两步：Prompt + Token Generation LLM（大型语言模型）的推理速度主要受显存容量限制，而不是GPU的计算能力。 显存大小决定了能处理的批次大小（batch size）和句子长度（sequence length）。 例如，一个13亿参数的模型在 A100-40G GPU上，模型参数占了 26 GB显存，剩下 14 GB可以存储大约 14000 个 token 的状态（每个 token 的 state 花 1M 左右空间）。 如果句子长度设为512，最大批次大小为28；如果句子长度为2048，最大批次大小为7。这还没有考虑到中间计算过程中的内存占用。 量化技术（quantization）可以提高显存利用率，增加单卡上的批次大小和句子长度，但需要修改模型权重。 有些方法如 FlashAttention 和连续批处理（continuous batching）不需要修改权重，也能提高内存I/O效率。 LLM batching LLM（大型语言模型）在处理批量请求时存在一些挑战，主要是因为模型的推理过程是逐步进行的。 在批量处理中，有的请求可能很快就完成了，但是要将这些请求释放并加入新的请求到还在处理的 batch 中，这个过程比较复杂。 这导致GPU的利用率不高，因为不同的请求生成的序列长度不同，有的短有的长。 例如，如果一个请求生成了2个token，而另一个生成了5个，那么在短请求结束后，GPU就会空闲，直到长请求完成。 这种空闲时间就是GPU未被充分利用的表现，传统的静态批量处理方法无法有效利用这些空闲时间。 简而言之，LLM在批量处理时，由于请求处理时间不一，导致GPU利用率低，难以有效利用空闲时间。 Continuous batching 简单来说，一旦一个batch中的某个seq完成生成，发射了一个end-of-seq token，就可以在其位置插入新的seq继续生成token，从而达到比static batching更高的GPU利用率。 PagedAttention PagedAttention 是对 KV Cache 所占空间的分页管理，是一个典型的以内存空间换计算开销的手段， vLLM 和 TensorRT-LLM 都应用了这个手段来节约 KV Cache 占用的内存。 KV Cache LLM 的核心是自回归 Transformer 模型。 该模型可基于输入（prompt）和其之前输出的 token 序列生成词（token），一次生成一个。 对于每次请求，这个成本高昂的过程都会重复，直到模型输出终止 token。 这种按序列的生成过程会让工作负载受到内存限制，从而无法充分利用 GPU 的计算能力，并会限制服务的吞吐量。 通过批量方式同时处理多个请求可以提高吞吐量。 但是，要在单一批次中处理许多请求，就需要高效地管理每个请求所占用的内存空间。 有学者观察到当前的 LLM 服务系统都没有高效地馆里 KV 缓存。 主要原因是它们会将请求的 KV 缓存保存在邻接的内存空间中。 但是，不同于传统深度学习工作负载中的张量，KV 缓存有其自己的独特性质： 它会在模型生成新 token 的过程中随时间动态地增长和缩小，而且它的持续时间和长度是无法事先知晓的。 vLLM 架构 vLLM 采用一种集中式调度器（scheduler）来协调分布式 GPU 工作器（worker）的执行。 KV 缓存管理器由 PagedAttention 驱动，能以分页方式有效管理 KV 缓存。 具体来说，KV 缓存管理器通过集中式调度器发送的指令来管理 GPU 工作器上的物理 KV 缓存内存。 PagedAttention：解决内存瓶颈 在自回归解码过程中，所有输入到 LLM 的 token 会产生注意力键和值的张量，这些张量保存在 GPU 内存中以生成下一个 token。 这些缓存键和值的张量通常被称为 KV 缓存，其具有： 内存占用大：在 LLaMA-13B 中，缓存单个序列最多需要 1.7 GB 内存； 动态且不可预测：KV 缓存的大小取决于序列长度，这是高度可变和不可预测的。因此，这对有效地管理 KV 缓存挑战较大。该研究发现，由于碎片化和过度保留，现有系统浪费了 60% - 80% 的内存。 与传统的注意力算法不同，PagedAttention 允许在非连续的内存空间中存储连续的键和值。 具体来说，PagedAttention 将每个序列的 KV 缓存划分为块，每个块包含固定数量 token 的键和值。 在注意力计算期间，PagedAttention 内核可以有效地识别和获取这些块。 KV 缓存管理器 对 KV 缓存的请求会被表示成一系列逻辑 KV 块，在生成新 token 和它们的 KV 缓存时从左向右填充。 最后一个 KV 块中未填充的位置留给未来填充。 在 PagedAttention 中，内存浪费只会发生在序列的最后一个块中。 PagedAttention 还有另一个关键优势 —— 高效的内存共享。 例如在并行采样中，多个输出序列是由同一个提示（prompt）生成的。 在这种情况下，提示的计算和内存可以在输出序列中共享。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap4-InferAndOpt/llm-inference-param.html":{"url":"Chap4-InferAndOpt/llm-inference-param.html","title":"LLM 推理参数","keywords":"","body":"LLM 推理常见参数 重点介绍 top_k、top_p、temperature、repetition_penalty 和 greedy_search。 背景介绍 常见的大型语言模型（LLM）通常只包含 Transformer 解码器（Decoder）。 在处理输入时，每个单词（Token）会先转换成一个嵌入向量（Token Embedding），然后这个向量被送入 Transformer 解码器。 解码器的最后一层输出的也是一个嵌入向量。 在预测下一个单词时，主要依赖于前一个单词的嵌入向量。 使用 Transformer 解码器进行文本生成的过程。简单来说，就是： 输入句子\"a robot must obey the orders given it\"的词嵌入（embedding）； 通过Transformer解码器生成每个词的新嵌入； 使用最后一个词“it”的新嵌入生成新词“Okay”； 将“Okay”的嵌入作为输入，生成下一个词“human”； 重复步骤 3 和 4，继续生成新的词。 根据新生成的词嵌入（embedding）来生成下一个词（Token）的过程，具体来说是让新生成的 embedding 与 token embedding 矩阵相乘， 得到和词表中每个 Token 的相似性得分（logits），然后基于这个得分即可以选择生成新的 Token。 Greedy Search GreedySearch（贪心搜索）的思路就是从相似性得分（logits）选择得分最高的 Token（一般来说，都会将得分经过 softmax 层转换为概率分布，数值区间为0~1），直到结束。 在推理阶段，模型的权重都是确定的，并且也不会有 dropout 等其他随机性（忽略不可抗的硬件计算误差，如并行规约求和的累积误差等）， 因此如果是 greedy search，则对于同一输入，多次运行后模型的输出结果应该完全一致。 好处：在模型效果严格对齐时非常有必要（比如将模型从 HF 模型转换为推理效率更高的 Faster Transformer 模型，并且使用 Faster Transformer 进行推理部署）。 坏处：模型效果可能不是最优的，也会缺乏一定的多样性，比如用同样的问题问 ChatGPT，其答案并不会每次都一样。 Beam Search Beam Search 不是每次都取得分最大的 Token，而是始终保留 beam_size 个得分最大的序列。 共有 “a”,“given”,“human”,“it”,“must”,“obey”,“Okay”,“orders”,“robot”,“the”,“.”,“EOS” 这些 Tokens。 以输入序列 “a”, “robot”, “must”, “obey”, “the”, “orders”, “given”, “it” 为例: Step1: 使用 Token “it”对应新生成的嵌入来计算 logits，最终“Okay”对应的得分 0.91 和“.”对应的得分 0.84 最高，所以选择“Okay”和“.”作为下一个 Token。 Step2: 分别使用 “Okay” 和 “.” 来计算 logits: 对于“Okay”，最终“human”对应的得分0.89和“the”对应的得分0.72最高，对应候选序列： “okay”+“human” = 0.91 * 0.89 = 0.8099 “okay”+“the” = 0.91 * 0.72 = 0.6552 对于“.”，最终“the”对应的得分0.92和“EOS”对应的得分0.70最高，对应候选序列: “.”+“the” = 0.84 * 0.92 = 0.7728 “.”+“EOS” = 0.84 * 0.70 = 0.5880 以此类推: 直到候选最大遇到了 EOS。 由于 beam search 会同时保留多个序列，因此就更容易得到得分更高的序列，并且beam_size越大，获得更高得分的概率越高。 由于每个 step 都需要进行 beam_size 次前向计算，计算量扩大 beam_size 倍。 另一方面，LLM 推理中一般都会使用 KV Cache，这也就会进一步增大 KV Cache 的内存占用，同时增加了 KV Cache 管理的复杂度。 这也就是在 LLM 推理中为什么比较少使用beam search。 Top k 不管是 greedy search，还是 beam search，对于固定输入，模型的输出是固定不变的。 为了增加模型输出的多样性，人们提出了 top-k 采样策略，其先选出分数最高的 k 个，然后将其分数作为权重进行随机采样，得到下一个 Token，这也就引入了随机性。 还是以上面的例子来介绍，假设k=3： Step1：使用 Token “it” 对应的新 embedding 来计算 logits，选出得分最高的 3 个 Token：“Okay”, “.”, “EOS”，对应的权重为 [0.91,0.84,0.72]，使用该权重进行随机采样，获得新 Token“Okay”——可根据概率前缀和和。 以此类推 如果top_k==1，则对应 greedy search。 在top_k中，每次都是从 k 个 Token 中采样，但是难免会出现一些特殊的 case，比如某一个 Token 的分数非常高，其他分数都很低，此时仍旧会有一定的概率采样到那些分数非常低的 Token，导致生成输出质量变差。 Top p 在每个 step 中，都对输出分数进行排序，然后将分数从大到小累加，直到累加分数大于设置的 p 为止， 然后和 top_k 类似， 将每个选择出来的 token 分数作为权重进行随机采样。 这样，每次候选的 Token 个数都会因为 Token 分数的分布不同而不一样。 虽然从理论上讲，**top_p 似乎比 top_k 更优雅，但这两种方法在实践中都很好用。 top_p 也可以与 top_k 结合使用，这可以避免分数非常低的 Token，同时提供一些动态选择的空间。 Temperature 事实上，在 top_k 和 top_p 的采样中并不是完全按照分数权重来采样的， 一般采样前我们会将候选Token的得分向量经过softmax（公式如下图）转换为概率，然后按照概率分布采样。 很多时候我们想要控制采样的随机性，可以使用带有温度系数 T 的 softmax 实现， 温度系数 T 为大于 0 的任意值（Huggingface中限制0.0T=1时，输出分布将与标准 softmax 输出相同。 T 的值越大，输出分布就越平滑，T 的值越小，输出分布越陡峭。 softmax⁡(yi)=eyiT∑j=1neyjT \\operatorname{softmax}\\left(y_{i}\\right)=\\frac{e^{\\frac{y_{i}}{T}}}{\\sum_{j=1}^{n} e^{\\frac{y_{j}}{T}}} softmax(yi​)=∑j=1n​eTyj​​eTyi​​​ Repetition Penalty 思想比较简单，记录之前已经生成过的 Token，当预测下一个 Token 时，人为降低已经生成过的 Token 的分数，使其被采样到的概率降低。 直接基于上述带温度系数T的softmax进行实现，其中的g表示已经生成过的Token列表，如果某个Token已经在生成过的Token列表g中，则对其对应的温度系数T乘上一个系数θ，θ为大于0的任意值。 [ ] θ=1，表示不进行任何惩罚 [ ] θ>1，相当于尽量避免重复 [ ] θ，相当于希望出现重复 pi=exp⁡(xi/(T⋅I(i∈g))∑jexp⁡(xj/(T⋅I(j∈g))I(c)=θ if c is True else 1 p_{i}=\\frac{\\exp \\left(x_{i} /(T \\cdot I(i \\in g))\\right.}{\\sum_{j} \\exp \\left(x_{j} /(T \\cdot I(j \\in g))\\right.} \\quad I(c)=\\theta ~if~ c ~is ~True ~else ~1 pi​=∑j​exp(xj​/(T⋅I(j∈g))exp(xi​/(T⋅I(i∈g))​I(c)=θ if c is True else 1 总结 GreedySearch是最简单、最直接的方式，其可以保证稳定的输出，相应的，BeamSearch可以进一步提升生成效果，但是代价更高，也是可以保证稳定的输出。 top_p和top_k都可以用于增加模型生成结果的多样性，输出结果往往会变。 temperature 用于控制随机性，temperature 越大，随机性越强，temperature 越小，随机性越弱。 重复惩罚repetition_penalty用于避免模型一直输出重复的结果，repetition_penalty越大，出现重复性可能越小，repetition_penalty越小，出现重复性可能越大。 Reference: LLM 推理常见参数 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap4-InferAndOpt/openai-o1-rel.html":{"url":"Chap4-InferAndOpt/openai-o1-rel.html","title":"OpenAI O1 相关进展","keywords":"","body":"OpenAI o1 相关进展 OpenAI o1 Learning to Reason with LLMs How reasoning works The o1 models introduce reasoning tokens. The models use these reasoning tokens to \"think\", breaking down their understanding of the prompt and considering multiple approaches to generating a response. After generating reasoning tokens, the model produces an answer as visible completion tokens, and discard the reasoning tokens from its context. Reasoning tokens are not visible via the API, they still occupy space in the model's context window. Learning to reason with llms Evaluation of OpenAI o1: Opportunities and Challenges of AGI Skywork-o1 昆仑万维发布了一系列融合了类似 o1 的慢思考和推理能力的模型： Skywork o1 Open-Llama-3.1-8B: 通过“o1-style”数据显著增强模型，以提高推理技能 Skywork o1 Open-PRM-Qwen-2.5-1.5B: 旨在通过增量过程奖励来增强推理能力，非常适合小规模解决复杂问题 Skywork o1 Open-PRM-Qwen-2.5-7B: 扩展 1.5B 模型的功能，以处理更苛刻的推理任务。 方法 三阶段训练计划： 反思推理训练：利用专有的多智能体系统为长期思考任务生成高质量、多样化的数据，然后进行持续的预训练和监督微调。 推理能力的强化学习：引入 Skywork o1 过程奖励模型（PRM），专为增强逐步推理而定制。我们的实验证实，Skywork-PRM 结合专有的推理强化算法，有效捕获了中间推理步骤对最终结果的影响。 推理规划：天工自助研发的 $$Q^{}* 在线推理算法，结合模型思维，寻找最优推理路径。 Skywork/Skywork-o1-Open-Llama-3.1-8B LLaVa-CoT: Let Vision Language Models Reason Step-by-Step LLaVA-CoT模型的介绍 LLaVA-CoT 是一种新型的视觉语言模型，旨在通过自主多阶段推理来提高对复杂问题的解决能力。 与依赖链式思考提示的模型不同，LLaVA-CoT 能够独立地进行多阶段的推理过程，这使得它在处理需要深入推理的视觉问答任务时表现出色。 视觉语言模型（VLMs）在系统和结构化推理方面的挑战 尽管大型语言模型在推理能力上取得了显著进步，但现有的视觉语言模型在执行系统和结构化推理时常常遇到困难。 这些模型往往缺乏一个清晰的推理过程，导致在逻辑推理任务中效果不佳。 LLaVA-CoT 的设计正是为了解决这一问题，通过引入结构化的推理步骤来提高模型的性能。 LLaVA-CoT模型的设计和结构 LLaVA-CoT 模型将答案生成过程拆解为四个阶段：摘要、视觉解释、逻辑推理和结论生成。 每个阶段都有其独特的目的，并使用专门的标签来标记，以便模型能够清晰地识别自己所处的推理阶段，并理解每个阶段的主要任务。 Summary Stage: 初始阶段，模型提供了对该问题的高度概括性解释，概述了它打算解决的问题的主要方面。 Caption Stage: 如果有图像，模型会提供与问题相关的视觉元素的简明概述，帮助理解多模态输入。 Reasoning Stage: 在初步总结的基础上，模型进行有条理的逻辑推理，得出初步答案。 Conclusion Stage: 在最后阶段，模型会根据前面的推理综合出一个答案。 在这里，结论阶段的输出是提供给用户的直接答复，而前面三个阶段则是内部 \"隐藏阶段\"，代表了 LLaVA-CoT 的推理过程。该阶段的输出会根据用户的要求进行调整：例如，如果用户要求简短的回答，结论就会简明扼要；如果需要详细的解释，结论就会提供详尽全面的回答。 每个阶段都由模型自行决定启动，无需外部提示工程框架或额外提示。 具体来说，作者为模型提供了四对特殊标记：、、 和 。 这些标记分别对应于总结回答方法、描述相关图像内容、进行推理和准备最终答案 LLaVA-CoT-100k数据集的构建 为了训练 LLaVA-CoT 模型，研究者们构建了一个名为 LLaVA-CoT-100k 的数据集，该数据集整合了多个视觉问答数据源，并提供了结构化推理注释。 这为模型的训练提供了丰富的、带有详细推理过程的样本。 LLaVA-CoT: Let Vision Language Models Reason Step-by-Step Marco-o1 Marco-o1模型的介绍 Marco-o1 是由阿里巴巴国际数字商务团队开发的一个大型推理模型，旨在解决开放式问题，这些问题通常缺乏明确的标准和难以量化的奖励。 该模型不仅关注于数学、物理和编程等有标准答案的学科，还特别强调开放式解决方案的重要性。 Marco-o1 模型通过结合 Chain-of-Thought（CoT）微调、Monte Carlo Tree Search（MCTS）和创新的推理策略，优化复杂现实世界问题的解决能力。 Marco-o1模型的目标和挑战 Marco-o1 模型的目标是探索大型推理模型的技术路线图，并着重于开放式问题和多语言应用。 尽管当前模型主要展现出类似于 o1 的推理特征，但其性能尚未达到完全实现的\"o1\"模型水平。 这项研究工作是一个持续优化和改进的过程，团队致力于不断提升模型的性能。 Marco-o1模型的技术特点 Marco-o1 模型的技术特点包括采用 CoT 微调和 MCTS，以及推理行动策略。 CoT 微调通过结合开源 CoT 数据集和合成数据，提升模型处理复杂任务的能力。 MCTS 允许模型探索多个推理路径，并使用 softmax 应用的对数概率来指导模型找到最优解。 此外，推理行动策略涉及在步骤和迷你步骤中变化行动的粒度，以优化搜索效率和准确性。 Marco-o1模型的实验设置和主要结果 实验基于 Qwen2-7B-Instruct 模型，通过使用训练数据进行监督式微调来创建 Marco-o1-CoT。 此外，还在 MCTS 框架内使用 Marco-o1-CoT，通过不同的行动粒度（步骤和迷你步骤）进行区分。 在测试中，所有模型都使用 CoT 提示以确保推理过程的一致性，并在 MGSM 数据集的英文和中文子集上进行测试。 Marco-o1模型的结论和未来工作 Marco-o1 模型通过整合 CoT 微调、MCTS 和新颖的推理行动策略，增强了模型的推理能力。 MCTS 的整合允许模型扩展解决方案空间，并通过不同粒度的行动（步骤和迷你步骤）进行实验，显示出更细的搜索分辨率在提高准确性方面的潜力。 未来，团队计划通过结果奖励建模（ORM）和过程奖励建模（PRM）来优化 MCTS 的奖励信号，并探索强化学习技术来微调 Marco-o1 的决策过程，以进一步提升其解决复杂现实世界任务的能力。 Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions QwQ qwq-32b-preview DeepSeek-R1-Lite-Preview deepseek-r1-lite-preview OpenAI o1 的后续工作 OpenAI o1 的后续工作 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap5-App/":{"url":"Chap5-App/","title":"应用方向","keywords":"","body":"应用方向 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 14:25:55 "},"Chap5-App/rag_intro_luxiangdong.html":{"url":"Chap5-App/rag_intro_luxiangdong.html","title":"大模型主流应用RAG的介绍——从架构到技术细节","keywords":"","body":" 主要内容： 大模型在实际应用中存在的问题 RAG 架构解析 RAG 技术架构的细节展示 LLM 的问题 幻觉问题： 新鲜度问题： 数据安全： RAG 架构解析 RAG 总概地可以理解为“索引、检索和生成”。 RAG 的主要组成，依次是：数据提取——向量化——创建索引——检索——自动排序——LLM 归纳生成。 RAG 技术细节概览 数据索引 数据清洗 Loader 数据处理 元数据 Chunking 固定大小 基于意图 句分割 Naive NLTK spaCy 递归分割 特殊分割 策略因素 索引类型（文本类型和长度） 模型类型（tokens 限制等） 查询长度和复杂度 应用类型（检索、问答、摘要...） Embedding BGE m3e ... 检索 检索优化 元数据过滤：比如有时间的元数据检索，就可以过滤掉非 5 月份的向量 图数据库 检索路由 相似度 关键词 SQL 重排序：相似度和相关性不一致 查询轮换 修改 prompt，重新提交 HyDE：生成相似 prompt（更标准） 子查询：多种分布式查询方式，比如：树、向量、顺序查询等 生成 问答过程中，内部运转的驱动器 ReAct Prompt 优化器 行业知识 模板 框架 LangChain LlamaIndex 自研 数据索引 数据清洗 数据提取：包括数据 Loader，提取 PDF、WORD、MARKDOWN 等； 数据处理：包括数据格式处理，不可识别内容的剔除，格式化等； 元数据提取：提取文件名、时间、章节 title、图片 alt 等信息。 分块 固定大小的分块：取决于 embedding 模型的情况。但是会损失很多语义，这样对检索是非常不友好的，解决方法是增加 overlap，一头一尾去保存相邻的 chunk 头尾的 tokens 内容； 基于意图的分块方式： 句分割：通过句号和换行来切分。NLTK 和 spaCy 也提供了基于意图的分割模型； 递归分割：通过分治的方法，用递归切分到最小单元到一种方式； 特殊分割： 影响分块策略的因素： 取决于类型类型，包括文本类型和长度，文章和微博推文的分块方式就会不同； 取决于模型类型，使用的 LLM、Embedding 不同，限制长度不一样，考虑的分块尺寸也会受影响； 取决于问答的文本长度和复杂度： 应用类型：RAG 的应用是检索、问答和摘要等，对分块策略都要不同的影响。 检索环节 元数据过滤：比如，用户问“帮我整理一下 XX 部门今年 5 月份的所有合同中，包含 XX 设备采购的合同有哪些？”如果有元数据，就可以去搜索“XX 部门+2023年 5 月”的相关数据。 References: 大模型主流应用RAG的介绍——从架构到技术细节 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-18 17:03:20 "},"Chap5-App/rag_interview-ques_analytics.html":{"url":"Chap5-App/rag_interview-ques_analytics.html","title":"Top 20+ RAG Interview Questions","keywords":"","body":"Top 20+ RAG Interview Questions Top 20+ RAG Interview Questions 概述 基本了解 RAG 如何将基于检索的方法与生成模型相结合，以增强 NLP 任务并提供更准确、连贯且上下文相关的输出； 探索 RAG 的多样化应用，包括其在问答系统、对话代理、信息检索和内容生成中的使用。 了解 RAG 的两步流程如何提高人工智能模型的准确性、上下文感知和灵活性，及其在减少偏见和错误信息方面的作用。 深入研究 RAG 的技术架构、检索组件的作用、RAG 如何处理复杂查询和多跳推理，以及它如何与机器学习管道集成以进行实际实现。 RAG interview Questions for Beginners Can you explain the basic difference between RAG and traditional language models? 传统语言模型从训练数据中学到的模式和结构生成文本，无法从外部来源检索特定信息，会根据收到的输入生成响应。 另一方面，RAG 包含一个检索组件。 它首先从文档库中搜索相关信息，然后生成响应。 这使得 RAG 能够访问和利用外部知识。 从而使其比传统语言模型更具上下文意识，并且能够提供更准确、信息更丰富的响应。 What are some common applications of RAG in AI? RAG 在人工智能的不同领域有多种应用，包括： 问答系统：RAG 可用于创建系统，在从大量数据集或互联网收集相关事实后，为用户的询问提供清晰、准确的响应； 信息检索：RAG 通过使用特定关键字或查询从庞大的语料库中提取相关文档或信息，可以帮助提高信息检索系统的有效性和精确度； 会话代理：RAG 可以通过让会话代理访问外部信息源来提高会话代理的性能。还可以帮助他们在交谈时提供更有洞察力和更适合上下文的答复。 内容生成：RAG 可以通过收集和组合来自不同来源的信息来创建摘要、文章和报告，从而生成逻辑性和教育性文档。 How does RAG improve the accuracy of responses in AI models? RAG 通过利用将基于检索的方法与生成模型相结合的两步方法来提高 AI 模型中响应的准确性。 检索组件首先搜索大量文档，以根据输入查询查找相关信息。 然后，生成模型使用检索到的信息来生成响应。 通过整合检索到的文档中的外部知识，RAG 可以提供比仅依赖于从训练数据中学习到的模式的传统生成模型更准确且与上下文相关的响应。 What is the significance of retrieval models in RAG? RAG 中的检索模型在从大型数据集或文档语料库中访问和识别相关信息方面发挥着至关重要的作用。 这些模型负责根据输入查询搜索可用数据并检索相关文档。 然后，检索到的文档将作为生成模型生成准确且信息丰富的响应的基础。 检索模型的重要性在于它们提供对外部知识的访问的能力。 因此，这增强了 RAG 系统的上下文感知和准确性。 What types of data sources are typically used in RAG systems? 在 RAG 系统中，可以使用各种类型的数据源，包括： 文档语料库：RAG 系统通常使用文本文档的集合（例如书籍、文章和网站）作为数据源。这些语料库提供了生成模型可以检索和利用的丰富信息源。 知识库：RAG 系统还可以使用包含事实信息的结构化数据库（例如 Wiki 或百科全书）作为数据源来检索特定的事实信息。 网络来源：RAG 系统还可以通过访问在线数据库、网站或搜索引擎结果来从网络检索信息，以收集相关数据以生成响应。 How does RAG contribute to the field of conversational AI? 通过允许会话代理访问和使用外部知识源，RAG 通过提高代理在与他人交互时生成富有洞察力且适合上下文的回复的能力来推进会话 AI。 通过集成生成模型和基于检索的技术，RAG 使对话代理能够更准确地理解用户的查询并做出反应，从而实现更有意义、更有吸引力的交流。 What is the role of the retrieval component in RAG? 根据输入问题，RAG 的检索组件搜索可用的数据源（例如文档语料库或知识库），以识别相关信息。 该组件使用各种检索方法（包括关键字匹配和语义搜索）查找和检索包含相关信息的文档或数据点。 生成模型接收并使用检索到的相关数据来生成响应。 检索组件使外部知识更易于访问，从而显著提高了 RAG 系统的准确性和上下文感知能力。 intermediate Level RAG interview Questions How does RAG handle bias and misinformation? RAG 可以通过利用涉及基于检索的方法和生成模型的两步方法来帮助减少偏见和错误信息。 开发人员可以配置检索组件，以便在从文档语料库或知识库检索信息时优先考虑可信和权威的来源。 此外，可以训练生成模型，以便在生成响应之前交叉引用和验证检索到的信息。 从而减少有偏见或不准确的信息传播。 RAG 旨在通过整合外部知识源和验证机制来提供更可靠、更准确的响应。 What are the benefits of using RAG over other NLP techniques? 与其他 NLP 技术相比，使用 RAG 的一些主要优势包括： 提高准确性：利用外部知识源，RAG 可以提供比标准语言模型生成更准确、更适合上下文的回复。 上下文感知：RAG 的检索组件使其能够理解和考虑查询的上下文，从而生成更有意义、更有说服力的答案。 灵活性：RAG 是适用于各种 NLP 应用的灵活解决方案。它可以使用多个数据源针对不同的任务和领域进行定制。 减少偏见和错误信息：RAG 可以通过优先考虑可靠来源并确认检索到的信息来帮助减少偏见和错误信息。 Can you discuss a scenario where RAG would be particularly useful? RAG 在开发医疗保健聊天机器人方面可能特别有帮助，该机器人可以为消费者提供准确且定制的医疗信息。 基于用户对症状、治疗或疾病的查询，该场景中的检索组件可以搜索学术期刊、医学文献和可靠的医疗保健网站的图书馆以获取相关信息。 之后，生成模型将使用这些知识来提供与用户上下文相关且有指导意义的回复。 通过将外部知识源与生成功能相集成，RAG 有潜力提高医疗保健聊天机器人的精确度和可靠性。 这将保证用户获得可靠且最新的医疗信息。 这种方法可以增强用户体验，与用户建立信任，并为访问可靠的医疗保健信息提供宝贵的支持。 How does RAG integrate with existing machine learning pipelines? 开发人员可以将 RAG 作为负责处理自然语言处理任务的组件，将其集成到现有的机器学习管道中。 通常，他们可以将 RAG 的检索组件连接到数据库或文档语料库，在其中根据输入查询搜索相关信息。 随后，生成模型处理检索到的信息以生成响应。 这种无缝集成使 RAG 能够利用现有的数据源和基础设施，从而更轻松地融入各种机器学习管道和系统。 What challenges does RAG solve in natural language processing? RAG 解决了自然语言处理中的几个挑战，包括： 上下文理解：RAG 的检索组件使其能够理解和考虑查询的上下文，从而比传统语言模型产生更连贯、更有意义的响应。 信息检索：通过利用基于检索的方法，RAG 可以有效地搜索大型数据集或文档语料库以检索相关信息，从而提高生成内容的可靠性。 偏见和错误信息：如前所述，RAG 可以通过优先考虑可靠来源并验证检索到的信息来帮助减少偏见和错误信息，从而提高生成内容的可靠性。 个性化：RAG 可以通过检索和利用之前的交互或用户配置文件中的相关信息，根据用户偏好或历史交互来调整响应的个性化。 How does RAG ensure the retrieved information is up-to-date? 为了确保检索到的信息是最新的，开发人员可以设计 RAG，使用来自信誉良好且可靠的来源的最新信息定期更新其数据库或文档语料库。 他们还可以配置检索组件，以便在搜索相关信息时优先考虑最近的出版物或更新。 实施持续监控和更新机制使他们能够刷新数据源并确保检索到的信息保持最新且相关。 What is the impact of RAG on the efficiency of language models? RAG 可以利用基于检索的方法缩小搜索空间并关注相关信息，从而显著提高语言模型的效率。 RAG 通过利用检索器组件从大型文档语料库或数据集中识别和检索相关数据，减少了生成模型的计算负担。 这种有针对性的方法使生成模型能够更有效地处理和生成响应，从而缩短推理时间并降低计算成本。 此外，将基于检索的技术与 RAG 中的生成模型相结合，可以实现更精确、更适合上下文的回复，从而减少对密集语言模型优化和微调的需求。 RAG 通过简化检索和生成过程来提高语言模型的整体性能，使它们更具可扩展性，并且对于一系列自然语言处理应用程序更有用。 Difficult Level RAG Interview Questions How does RAG differ from Parameter-Efficient Fine-Tuning (PEFT)? RAG 和 PEFT 是两种不同方法： RAG：通过将生成模型与基于检索的技术融合来改善自然语言处理问题。它使用检索器组件从数据集文件或文档语料库获取相关数据，然后将其应用到生成模型以生成回复。 PEFT（Parameter-Efficient Fine-Tuning）：旨在通过优化和微调预训练语言模型来减少所需的计算资源和参数，以提高其在特定任务上的性能。 In what ways can RAG enhance human-AI collaboration? RAG 可以通过以下方式增强人类与人工智能协作： 增加信息检索：RAG 的检索组件可以从大数据集或文档语料库中访问和检索相关材料。从而为消费者的询问提供全面、准确的答复。 提高对上下文的理解：通过在讨论期间保持上下文一致，RAG 可以产生更有意义、更引入注目的回复。因此，人类与人工智能之间的交互变得更加无缝和有意义。 定制响应：RAG 可能会考虑用户的选择和过去的互动，以提供满足每个人的要求和偏好的定制答案。 总体而言，RAG 利用外部知识源并生成上下文相关响应的能力可以提高人类与人工智能交互的质量，使协作更加有效和有吸引力。 Can you explain the technical architecture of a RAG system? RAG 系统的技术架构通常由两个主要组件组成： 检索器组件：该组件负责搜索数据集或文档语料库，以根据输入查询检索相关信息。它使用关键字匹配、语义搜索或神经检索器等检索技术来识别和提取相关数据。 生成模型：获得数据后，将其发送到生成模型，该模型使用信息来处理数据并做出响应。根据收集到的信息，该模型被教导理解并生成类似于人的文字。 How does RAG maintain context in a conversation? RAG 利用从过去或当前讨论中获取的信息来保留对话中的上下文。 通过不断搜索和检索与现有对话相关的数据，“检索器”组件可确保生成模型获得所需的上下文，从而生成连贯且与上下文相符的回复。 得益于这种迭代过程，RAG 能够理解并适应不断变化的讨论背景，从而产生更有机、更精彩的互动。 What are the limitations of RAG? RAG 的一些限制包括： 计算复杂性：涉及检索和生成两步过程可能需要大量计算。因此，这会导致推理时间和资源需求增加。 对数据质量的依赖：RAG 的性能在很大程度上依赖于检索到的信息的质量和相关性。如果检索器组件无法检索准确或相关的数据，则可能会影响生成的响应的整体准确性和可靠性。 可扩展性：管理和更新大型文档语料库或数据集可能会给可扩展性和维护带来挑战。对于实时应用程序或具有动态内容的系统尤其如此。 偏差和错误信息：与其他 AI 模型一样，RAG 可能会无意中传播训练数据中存在的偏差，或者如果没有适当控制或验证，会检索并生成错误信息。 尽管存在这些限制，RAG 正在进行的研究和进步旨在解决这些挑战，并进一步提高其在各种自然语言处理任务中的性能和适用性。 How does RAG handle complex queries that require multi-hop reasoning? 通过利用其检索组件对多个文档或数据点进行迭代搜索，逐步获取相关信息，RAG 可以处理需要多跳推理的难题。 检索组件可通过从一个来源获取数据来遵循逻辑逻辑。 此外，还可以利用这些数据创建新的查询，从其他来源获取更多相关数据。 在这种迭代过程的帮助下，RAG 除了能从多个来源拼凑零散的信息外，还能为涉及多跳推理的复杂问题提供详尽的答案。 Can you discuss the role of knowledge graphs in RAG? 知识图通过提供有组织的知识表示和事物之间的联系来促进更准确、更有效的信息检索和推理。 知识图可以包含在 RAG 的检索器组件中，通过使用图结构更有效地遍历和检索相关信息来提高搜索能力。 使用知识图，RAG 可以记录和使用想法和事物之间的语义链接。 从而为用户的询问提供更丰富、更细致的答案。 What are the ethical considerations when implementing RAG systems? 实施 RAG 系统会引起一些道德考虑，包括： 偏见与公平: 确保 RAG 系统不会延续或放大训练数据或检索信息中的偏差至关重要。实施检测和减少偏见的措施可以促进生成的响应的公平性和公正性。 问责制和透明度：鼓励用户了解 RAG 系统的工作原理并使其易于理解，有助于培养他们之间的责任感和信任感。通过提供检索和生成过程的清晰文档和解释，用户可以理解和评估系统做出的决策。 隐私和数据安全：从外部来源访问和检索信息时，保护用户隐私并保证数据安全至关重要。强有力的数据保护措施并遵守隐私法律和标准可以保护用户数据并维持信任。 准确性和可靠性：为了防止错误或误导性信息的传播，保证所获取的数据和创建的回复的正确性和可靠性至关重要。执行质量保证程序和验证流程有助于保持 RAG 系统的完整性。 用户同意和控制：尊重用户偏好并为用户提供控制信息访问和个性化程度的选项，有助于增强用户与 RAG 系统交互的自主权和同意。 Copyright © 版权信息 all right reserved，powered by Gitbook该文件修订时间: 2024-12-19 13:31:42 "}}