## 预训练


### 为什么要增量预训练
**预训练学知识**，**指令微调学格式**，**强化学习对齐偏好**，增量预训练让大模型有领域知识（靠指令微调记住知识不靠谱，几十万条数据做不到。）

训练过程与原始预训练类似，增量预训练常用于领域适应，学习领域知识、风格和术语等。


### 进行增量预训练需要做哪些准备工作
1. **选取底座模型**：根据自己的需求和硬件条件来选择合适的底座模型及模型参数量的大小；
2. **收集数据**：收集大量的文本数据，一般预训练数据的大小都是 TB 级别的；
3. **数据清洗**：所有的信息都能够在互联网信息中被找到，只是**信息密度**相比 *人工精选数据集* 要更低。例如「明星信息」、「如何写代码」这些信息都能在新闻网站、或是问答网站中找到，只不过*维基百科*或是 *Github* 则是将这些信息给「高密度」且「结构化」地进行了存储。这使得我们在使用维基百科作为训练语料的时候，模型能够更快的学习到这些高密度信息（人物的经历、年龄、性别、职业等等），而这些内容在互联网信息（如新闻）中的信息密度则较低，即很少会有一条新闻完整的介绍一个艺人的过往经历。只要我们对**互联网信息进行严格的处理**（去除冗余信息，提高有用信息的密度），就能够加快模型的学习速度。


### 增量预训练所用训练框架
- **超大规模训练**：选用 3D 并行（流水线并行、张量并行和数据并行），Megatron-Deepspeed 拥有多个成功案例
- **少量节点训练**：选用张量并行，但张量并行只有在 nvlink 环境下才会起正向作用，但提升也不会太明显
- **少量卡训练**：如果资源特别少，显存怎么也不够，可以使用 LoRA 进行增量预训练


### 增量预训练数据选取思路

垂直领域预训练有三种思路：
- 先用大规模通用语料预训练，再用小规模领域语料二次训练
- 直接进行大规模领域语料预训练
- 通用语料比例混合领域语料同时训练


### 增量预训练流程
- 数据预处理：参考 LLaMA 的预训练长度，也把数据处理成 2048 长度（如果不够，做补全）。
- 分词器：如果使用 LLaMA 可能需要添加中文词表，目前有不少人做了相关工作，当然也可以自己添加自己需要的词表。
- 原始模型：各家框架的模型层名不太一样，训练时可能需要做一些调整，在预训练时尽量选择基座模型，不选 Chat 模型。
- 训练模型：跑通只是第一步，根据训练情况反复调整比较重要。
- 模型转换：不同框架的 checkpoint 格式不同，还会根据并行度分成很多个文件。
- 模型测试：简单测试下续写能力，验证下模型是否正常。

---

Refs:
- [LLM domain adaptation using continued pre-training](https://medium.com/@gilinachum/llm-domain-adaptation-using-continued-pre-training-part-1-3-e3d10fcfdae1)
- [2.预训练.md](https://github.com/wdndev/llm_interview_note/blob/main/05.%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83/2.%E9%A2%84%E8%AE%AD%E7%BB%83/2.%E9%A2%84%E8%AE%AD%E7%BB%83.md)

