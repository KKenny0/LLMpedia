Transformer Expander 是一种交互式可视化工具，旨在帮助任何人了解 GPT 等基于 Transformer 的模型如何工作。
它直接在您的浏览器中运行实时 GPT-2 模型，允许您试验自己的文本并实时观察 Transformer 的内部组件和操作如何协同工作以预测下一个令牌。

> - [transformer-explainer](https://github.com/poloclub/transformer-explainer)
> - [transformer-explainer-demo](https://poloclub.github.io/transformer-explainer/)


## 什么是 Transformer？
从根本上来说，文本生成 Transformer 模型的运行原理是下一个单词预测：给定用户的文本提示，该输入之后最有可能出现的下一个单词是什么？ 
Transformer 的核心创新和强大之处在于它们使用自注意力机制，这使得它们能够比以前的架构更有效地处理整个序列并捕获远程依赖关系。

## Transformer 结构
每个文本生成 Transformer 都包含以下三个关键组件：

1. Embedding
2. Transformer 块:
   - 注意力机制: Transformer模块的核心组件。它允许标记与其他标记进行通信，捕获上下文信息和单词之间的关系。
   - 多层感知器层: 一个独立运行在每个令牌上的前馈网络。注意力层的目标是在 token 之间路由信息，而 MLP 的目标是细化每个 token 的表示。
3. 输出概率: 最后的线性层和 softmax 层将处理后的嵌入转换为概率，使模型能够预测序列中的下一个标记。

## Embedding
嵌入的用武之地：它将文本转换为模型可以使用的数字表示。
要将提示转换为嵌入，我们需要: 1) 对输入进行标记; 2) 获取标记嵌入; 3) 添加位置信息，最后 4) 将标记和位置编码相加以获得最终嵌入。

### Step1: Tokenization
标记化是将输入文本分解为更小、更易于管理的片段（称为标记）的过程。
这些标记可以是单词或子单词。
“数据”和“可视化”一词对应于独特的标记，而“授权”一词则分为两个标记。
令牌的完整词汇表在训练模型之前确定：GPT-2 的词汇表有 50,257 个唯一令牌。
现在我们将输入文本分割成具有不同 ID 的标记，我们可以从嵌入中获取它们的向量表示。

### Step2: Token Embedding
GPT-2 Small 将词汇表中的每个 token 表示为 768 维向量；向量的维度取决于模型。
这些嵌入向量存储在形状为 (50,257, 768) 的矩阵中，包含大约 3900 万个参数！
这个广泛的矩阵允许模型为每个标记分配语义。

### Step3: 位置编码
嵌入层还对输入提示中每个标记的位置信息进行编码。不同的模型使用不同的位置编码方法。 
GPT-2 从头开始训练自己的位置编码矩阵，并将其直接集成到训练过程中。

### Step4: 最终 Embedding
最后，我们将令牌和位置编码相加以获得最终的嵌入表示。这种组合表示捕获了标记的语义及其在输入序列中的位置。

## Transformer 块
Transformer 处理的核心在于 Transformer 块，其中包括多头自注意力和多层感知器层。
大多数模型由多个这样的块组成，这些块按顺序依次堆叠。
Token 表示从第一个区块到第 12 个区块逐层演变，使模型能够建立对每个代币的复杂理解。
这种分层方法导致输入的高阶表示。

### 多头注意力
1. Query, Key and Value 矩阵
2. Masked Self-Attention
   - Attention Score
   - Masking
   - Softmax
3. Output: 该模型使用屏蔽的自注意力分数并将其与值矩阵相乘以获得自注意力机制的最终输出。

## MLP
在多个自注意力头捕获输入标记之间的不同关系后，连接的输出将通过多层感知器（MLP）层，以增强模型的表示能力。
MLP 块由两个线性变换组成，中间有一个 GELU 激活函数。
第一个线性变换将输入的维度从 768 增加四倍到 3072。
第二个线性变换将维度降低回原始大小 768，确保后续层接收一致维度的输入。
与自注意力机制不同，MLP 独立处理 token，并简单地将它们从一种表示映射到另一种表示。
