
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Transformer 结构-Wiki · LLMpedia</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="Kenny Wu">
        
        
    
    <link rel="stylesheet" href="../../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-chapter-fold/chapter-fold.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-katex-pp/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Infrastructure_transformer_transformer-explainer.html" />
    
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../../">
            
                <a href="../../">
            
                    
                    前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../">
            
                <a href="../">
            
                    
                    模型结构
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" >
            
                <span>
            
                    
                    Transformer 架构与模型
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.2.1.1" data-path="Infrastructure_transformer_wiki.html">
            
                <a href="Infrastructure_transformer_wiki.html">
            
                    
                    Transformer 结构-Wiki
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="Infrastructure_transformer_transformer-explainer.html">
            
                <a href="Infrastructure_transformer_transformer-explainer.html">
            
                    
                    Transformer 可视化解释
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="Infrastructure_luxiangdong_Transformer-OverallArch.html">
            
                <a href="Infrastructure_luxiangdong_Transformer-OverallArch.html">
            
                    
                    土猛的员外-Transformer 架构的整体指南
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="Infrastructure_HF_Encoder-models.html">
            
                <a href="Infrastructure_HF_Encoder-models.html">
            
                    
                    Encoder 模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="Infrastructure_HF_Decoder-models.html">
            
                <a href="Infrastructure_HF_Decoder-models.html">
            
                    
                    Decoder 模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="Infrastructure_HF_Encoder-Decoder-models.html">
            
                <a href="Infrastructure_HF_Encoder-Decoder-models.html">
            
                    
                    Encoder-Decoder 模型
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" >
            
                <span>
            
                    
                    注意力机制
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="../attention/Advanced_Blog_AttentionAttention.html">
            
                <a href="../attention/Advanced_Blog_AttentionAttention.html">
            
                    
                    Lilian-Attention?Attention!
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="../attention/mha-related.html">
            
                <a href="../attention/mha-related.html">
            
                    
                    缓存优化与效果-KV
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../../Chap2-Training_Evaluation/">
            
                <a href="../../Chap2-Training_Evaluation/">
            
                    
                    训练与评估
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../../Chap2-Training_Evaluation/distributed-training/overview.html">
            
                <a href="../../Chap2-Training_Evaluation/distributed-training/overview.html">
            
                    
                    分布式训练
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="../../Chap2-Training_Evaluation/distributed-training/train-framework.html">
            
                <a href="../../Chap2-Training_Evaluation/distributed-training/train-framework.html">
            
                    
                    分布式训练框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="../../Chap2-Training_Evaluation/distributed-training/data-parallel.html">
            
                <a href="../../Chap2-Training_Evaluation/distributed-training/data-parallel.html">
            
                    
                    数据并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="../../Chap2-Training_Evaluation/distributed-training/pipeline-parallel.html">
            
                <a href="../../Chap2-Training_Evaluation/distributed-training/pipeline-parallel.html">
            
                    
                    流水线并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="../../Chap2-Training_Evaluation/distributed-training/pipeline-parallel.html">
            
                <a href="../../Chap2-Training_Evaluation/distributed-training/pipeline-parallel.html">
            
                    
                    张量并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="../../Chap2-Training_Evaluation/distributed-training/moe-parallel.html">
            
                <a href="../../Chap2-Training_Evaluation/distributed-training/moe-parallel.html">
            
                    
                    MoE 并行
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../../Chap2-Training_Evaluation/fine-tuning/overview.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/overview.html">
            
                    
                    微调
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="../../Chap2-Training_Evaluation/fine-tuning/llm-training-ybq.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/llm-training-ybq.html">
            
                    
                    LLM 训练-ybq
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="../../Chap2-Training_Evaluation/fine-tuning/finetuning-discussion.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/finetuning-discussion.html">
            
                    
                    关于微调的讨论
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="../../Chap2-Training_Evaluation/fine-tuning/pretrain.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/pretrain.html">
            
                    
                    预训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.4" data-path="../../Chap2-Training_Evaluation/fine-tuning/prompt-tuning.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/prompt-tuning.html">
            
                    
                    Prompt-Tuning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.5" data-path="../../Chap2-Training_Evaluation/fine-tuning/adapter-tuning.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/adapter-tuning.html">
            
                    
                    Adapter-Tuning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.6" data-path="../../Chap2-Training_Evaluation/fine-tuning/lora.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/lora.html">
            
                    
                    LoRA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.7" data-path="../../Chap2-Training_Evaluation/fine-tuning/summary.html">
            
                <a href="../../Chap2-Training_Evaluation/fine-tuning/summary.html">
            
                    
                    总结
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../../Chap2-Training_Evaluation/evaluation/">
            
                <a href="../../Chap2-Training_Evaluation/evaluation/">
            
                    
                    大模型评估
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="../../Chap2-Training_Evaluation/evaluation/evaluating.html">
            
                <a href="../../Chap2-Training_Evaluation/evaluation/evaluating.html">
            
                    
                    评测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="../../Chap2-Training_Evaluation/evaluation/hallucination.html">
            
                <a href="../../Chap2-Training_Evaluation/evaluation/hallucination.html">
            
                    
                    幻觉
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.4" >
            
                <span>
            
                    
                    蒸馏和压缩
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.4.1" >
            
                <span>
            
                    
                    知识蒸馏
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4.2" >
            
                <span>
            
                    
                    剪枝和量化
            
                </span>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../../Chap3-PromptEngr/">
            
                <a href="../../Chap3-PromptEngr/">
            
                    
                    Prompt 工程
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" >
            
                <span>
            
                    
                    Prompt 技术
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1.1" data-path="../../Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html">
            
                <a href="../../Chap3-PromptEngr/prompt-tech_baoyu_how-to-write-good-prompt.html">
            
                    
                    宝玉老师-如何写好提示词？
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.2" >
            
                <span>
            
                    
                    应用场景
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.2.1" data-path="../../Chap3-PromptEngr/prompt-app_openai-prompt-generation.html">
            
                <a href="../../Chap3-PromptEngr/prompt-app_openai-prompt-generation.html">
            
                    
                    OpenAI-生成提示词的提示词
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.2" data-path="../../Chap3-PromptEngr/prompt-app_baoyu_translator-gpt-prompt.html">
            
                <a href="../../Chap3-PromptEngr/prompt-app_baoyu_translator-gpt-prompt.html">
            
                    
                    GPT 翻译 Prompt
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../../Chap4-InferAndOpt/">
            
                <a href="../../Chap4-InferAndOpt/">
            
                    
                    推理与优化
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../../Chap4-InferAndOpt/llm-inference-overview.html">
            
                <a href="../../Chap4-InferAndOpt/llm-inference-overview.html">
            
                    
                    LLM 推理过程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../../Chap4-InferAndOpt/infer-framework.html">
            
                <a href="../../Chap4-InferAndOpt/infer-framework.html">
            
                    
                    LLM 推理框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../../Chap4-InferAndOpt/vllm-framework.html">
            
                <a href="../../Chap4-InferAndOpt/vllm-framework.html">
            
                    
                    vLLM 介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="../../Chap4-InferAndOpt/llm-inference-param.html">
            
                <a href="../../Chap4-InferAndOpt/llm-inference-param.html">
            
                    
                    LLM 推理参数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="../../Chap4-InferAndOpt/openai-o1-rel.html">
            
                <a href="../../Chap4-InferAndOpt/openai-o1-rel.html">
            
                    
                    OpenAI O1 相关进展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" >
            
                <span>
            
                    
                    推理加速
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.7" >
            
                <span>
            
                    
                    多模态处理
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.7.1" >
            
                <span>
            
                    
                    图像-文本模型
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.7.2" >
            
                <span>
            
                    
                    跨模态注意力机制
            
                </span>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.8" >
            
                <span>
            
                    
                    内容与计算优化
            
                </span>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../../Chap5-App/">
            
                <a href="../../Chap5-App/">
            
                    
                    应用方向
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" >
            
                <span>
            
                    
                    文本生成与摘要
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1.1" >
            
                <span>
            
                    
                    自然语言生成
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.1.2" >
            
                <span>
            
                    
                    文本摘要
            
                </span>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6.2" >
            
                <span>
            
                    
                    问答与对话技术
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.2.1" >
            
                <span>
            
                    
                    Chatbot 技术
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2.2" >
            
                <span>
            
                    
                    问答系统与检索增强生成
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.2.2.1" data-path="../../Chap5-App/rag_intro_luxiangdong.html">
            
                <a href="../../Chap5-App/rag_intro_luxiangdong.html">
            
                    
                    大模型主流应用RAG的介绍——从架构到技术细节
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2.2.2" data-path="../../Chap5-App/rag_interview-ques_analytics.html">
            
                <a href="../../Chap5-App/rag_interview-ques_analytics.html">
            
                    
                    Top 20+ RAG Interview Questions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2.2.3" data-path="../../Chap5-App/rag_advanced-rag-v2_ordax.html">
            
                <a href="../../Chap5-App/rag_advanced-rag-v2_ordax.html">
            
                    
                    RAG 技术超全全景图:从基础到高级实践
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6.3" >
            
                <span>
            
                    
                    代码生成与分析
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.3.1" >
            
                <span>
            
                    
                    编程助手
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3.2" >
            
                <span>
            
                    
                    自动代码补全
            
                </span>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6.4" >
            
                <span>
            
                    
                    AI Agent
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.4.1" data-path="../../Chap5-App/agentic_interview-ques_medium.html">
            
                <a href="../../Chap5-App/agentic_interview-ques_medium.html">
            
                    
                    Top 25 Agentic AI Interview Questions and Answers
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../.." >Transformer 结构-Wiki</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <html><head></head><body><div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><ul><li><span class="title-icon "></span><a href="#architecture"><b>1.1. </b>Architecture</a></li><ul><li><span class="title-icon "></span><a href="#embedding"><b>1.1.1. </b>Embedding</a></li><li><span class="title-icon "></span><a href="#un-embedding"><b>1.1.2. </b>Un-embedding</a></li><li><span class="title-icon "></span><a href="#encoder-decoder-overview"><b>1.1.3. </b>Encoder-decoder (overview)</a></li><li><span class="title-icon "></span><a href="#feedforward-network"><b>1.1.4. </b>Feedforward network</a></li><li><span class="title-icon "></span><a href="#scaled-dot-product-attention"><b>1.1.5. </b>Scaled dot-product attention</a></li><li><span class="title-icon "></span><a href="#encoder"><b>1.1.6. </b>Encoder</a></li><li><span class="title-icon "></span><a href="#decoder"><b>1.1.7. </b>Decoder</a></li></ul><li><span class="title-icon "></span><a href="#full-transformer-architecture"><b>1.2. </b>Full transformer architecture</a></li><ul><li><span class="title-icon "></span><a href="#sublayers"><b>1.2.1. </b>Sublayers</a></li></ul></ul></ul></div><a href="#" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><blockquote>
<p><a href="https://arc.net/l/quote/ikjwlynv" target="_blank">Transformer (deep learning architecture)</a></p>
</blockquote>
<h2 id="architecture"><a name="architecture" class="anchor-navigation-ex-anchor" href="#architecture"><i class="fa fa-link" aria-hidden="true"></i></a>1.1. Architecture</h2>
<p>&#x4E3B;&#x8981;&#x7EC4;&#x4EF6;:</p>
<ul>
<li>Tokenizers:</li>
<li>Embedding layer: convert tokens and positions of the tokens into vector representations</li>
<li>Transformer layers: carry out repeated transformations on the vector representations, extracting more and more linguistic information.<ul>
<li>Alternating attention and feedforward layers.</li>
<li>Two major types transformer layers: encoder layers and decoder layers</li>
</ul>
</li>
<li>Un-embedding layer: convert the final vector representations back to a probability distribution over the tokens</li>
</ul>
<h3 id="embedding"><a name="embedding" class="anchor-navigation-ex-anchor" href="#embedding"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.1. Embedding</h3>
<p>Each token is converted into an embedding vector via a lookup table.
Equivalently stated, it multiplies a <code>one-hot representation</code> of the token by an embedding matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span>.</p>
<h3 id="un-embedding"><a name="un-embedding" class="anchor-navigation-ex-anchor" href="#un-embedding"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.2. Un-embedding</h3>
<p>Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.</p>
<p>The un-embedding layer is a linear-softmax layer:
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mi>n</mi><mi>E</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mi>W</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
UnEmbed(x) = softmax(xW+b)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord mathdefault">m</span><span class="mord mathdefault">b</span><span class="mord mathdefault">e</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="encoder-decoder-overview"><a name="encoder-decoder-overview" class="anchor-navigation-ex-anchor" href="#encoder-decoder-overview"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.3. Encoder-decoder (overview)</h3>
<p>The original transformer model used an <strong>encoder-decoder</strong> architecture.
The encoder consists of encoding layers that process all the input tokens together one layer after another,
while the decoder consists of decoding layers that iteratively process the encoder&apos;s output and the decoder&apos;s output tokens so far.</p>
<p>Both the encoder and decoder layers have a <strong>feed-forward neural network</strong> for <em>additional processing</em> of their outputs and
contain <strong>residual connections</strong> and <strong>layer normalization</strong> steps.</p>
<h3 id="feedforward-network"><a name="feedforward-network" class="anchor-navigation-ex-anchor" href="#feedforward-network"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.4. Feedforward network</h3>
<p>The feedforward network (FFN) modules in a Transformer are <strong>2-layered multilayer perceptrons</strong>.
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3018d1cafd676461ec6a1927aff651d73ce78377" alt="2-layered multilayer perceptrons"></p>
<h3 id="scaled-dot-product-attention"><a name="scaled-dot-product-attention" class="anchor-navigation-ex-anchor" href="#scaled-dot-product-attention"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.5. Scaled dot-product attention</h3>
<h4 id="attention-head"><a name="attention-head" class="anchor-navigation-ex-anchor" href="#attention-head"><i class="fa fa-link" aria-hidden="true"></i></a>Attention head</h4>
<p>The attention mechanism used in the Transformer architecture are <strong>scaled dot-product attention units</strong>.</p>
<p>For each vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i, query}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> in the query sequence, it is multiplied by a matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding="application/x-tex">W^Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span></span></span> to produce a query vector
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding="application/x-tex">q_i = x_{i, query}W^Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1274389999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span></span></span>. The matrix of all query vectors is the query matrix: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><msub><mi>X</mi><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding="application/x-tex">Q = X_{query}W^Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1274389999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>Similarly, we construct the key matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><msub><mi>X</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow></msub><msup><mi>W</mi><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">K = X_{key}W^K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1274389999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span></span></span></span> and the value matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><msub><mi>X</mi><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></msub><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding="application/x-tex">V = X_{value}W^V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.991331em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>It is usually the case that all <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msup><mi>W</mi><mi>Q</mi></msup><mo separator="true">,</mo><msup><mi>W</mi><mi>K</mi></msup><mo separator="true">,</mo><msup><mi>W</mi><mi>V</mi></msup></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle W^{Q},W^{K},W^{V}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.085771em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.891331em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span></span></span></span></span></span></span></span></span> are square matrices.</p>
<p>Attention weights are calculated using the query and key vectors&#xFF1A;the attention weight from token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span> to token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> 
is the dot product between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>.
The attention weights are divided by the square root of the dimension of the key vectors, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle {\sqrt {d_{k}}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.23390500000000003em;"></span><span class="mord"><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.006095em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.966095em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.23390500000000003em;"><span></span></span></span></span></span></span></span></span></span></span>, 
which stabilizes gradients during training, and passed through a softmax which normalizes the weights</p>
<p>The matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mi>Q</mi></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle Q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mi>K</mi></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle V}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span></span> are defined as the matrices 
where the {\displaystyle i}th rows are vectors <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msub><mi>q</mi><mi>i</mi></msub></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle q_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msub><mi>k</mi><mi>i</mi></msub></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle k_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msub><mi>v</mi><mi>i</mi></msub></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle v_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> respectively. 
Then we can represent the attention as:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b2afc7240eb97375a384b1628c18438e3068e3f" alt="attention head"></p>
<p>The attention mechanism requires the following three equalities to hold:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b25b25f22a8a09c26350d2f065628dd4b3911669" alt="attention mechanism equalities"></p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/Transformer%2C_attention_block_diagram.png" alt="Scaled dot-product attention, block diagram."></p>
<p>If the attention is used in a <em>self-attention</em> fashion, then <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{query} = X_{key} = X_{value}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.
If the attention is used in a <em>cross-attention</em> fashion, then usually <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow></msub><mo mathvariant="normal">&#x2260;</mo><msub><mi>X</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow><mi>v</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">X_{query} \neq X_{key} = X_{val}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel">&#xE020;</span></span><span class="fix"></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</p>
<h4 id="multihead-attention"><a name="multihead-attention" class="anchor-navigation-ex-anchor" href="#multihead-attention"><i class="fa fa-link" aria-hidden="true"></i></a>Multihead attention</h4>
<p>In a transformer model, an attention head consist of three matrices: (W^Q), (W^K), and (W^V),
where (W^Q) and (W^K) determine the relevance between tokens for attention scoring,
and (W^V) along with (W^O) influences how attended tokens affect subsequent layers and output logits.</p>
<p>Multiple attention heads in a layer allow the model to capture different definitions of &quot;relevance&quot;.
As tokens progress through layers, the scope of attention can expand, enabling the model to grasp more complex and long-range dependencies. </p>
<p>The outputs from all attention heads are concatenated and fed into the feed-forward neural network layers.</p>
<p>Concretely, let the multiple attention heads be indexed by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mi>i</mi></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">i</span></span></span></span></span>, then we have:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2e699202635f8646a75cf1b0658697c6825e123a" alt="multihead attention formula">
where the matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> is the concatenation of word embeddings, and the matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.236103em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592389999999998em;"><span style="top:-2.4231360000000004em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span style="top:-3.180908em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> 
are &quot;projection matrices&quot; owned by individual attention head <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mi>i</mi></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">i</span></span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msup><mi>W</mi><mi>O</mi></msup></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle W^{O}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8913309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></span> is a final projection matrix owned by the whole multi-headed attention head.</p>
<p>It is theoretically possible for each attention head to have a different head dimension <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msub><mi>d</mi><mtext>head</mtext></msub></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle d_{\text{head}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">head</span></span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>, but that is rarely the case in practice.</p>
<p>As an example, in the smallest GPT-2 model, there are only self-attention mechanisms.
It has the following dimensions:
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>e</mi><mi>m</mi><mi>b</mi></mrow></msub><mo>=</mo><mn>768</mn><mo separator="true">,</mo><msub><mi>n</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub><mo>=</mo><mn>12</mn><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">
d_{emb} = 768, n_{head}=12, d_{head}=64
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">7</span><span class="mord">6</span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span></span>
Since, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mn>12</mn><mo>&#xD7;</mo><mn>64</mn><mo>=</mo><mn>768</mn></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle 12\times 64=768}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#xD7;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">6</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">7</span><span class="mord">6</span><span class="mord">8</span></span></span></span></span>, its output projection matrix, 
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msup><mi>W</mi><mi>O</mi></msup><mo>&#x2208;</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mn>12</mn><mo>&#xD7;</mo><mn>64</mn><mo stretchy="false">)</mo><mo>&#xD7;</mo><mn>768</mn></mrow></msup></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle W^{O}\in \mathbb {R} ^{(12\times 64)\times 768}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9771em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&#x2208;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mord mtight">2</span><span class="mbin mtight">&#xD7;</span><span class="mord mtight">6</span><span class="mord mtight">4</span><span class="mclose mtight">)</span><span class="mbin mtight">&#xD7;</span><span class="mord mtight">7</span><span class="mord mtight">6</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></span> is a square matrix.</p>
<h4 id="masked-attention"><a name="masked-attention" class="anchor-navigation-ex-anchor" href="#masked-attention"><i class="fa fa-link" aria-hidden="true"></i></a>Masked attention</h4>
<p>It may be necessary to cut out attention links between some word-pairs.
For example, the decoder, when decoding for the token position <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span>,
should not have access to the token at position <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>.
This may be accomplished before the softmax stage by adding a mask matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 
that is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mo>&#x2212;</mo><mi mathvariant="normal">&#x221E;</mi></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle -\infty }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">&#x2212;</span><span class="mord">&#x221E;</span></span></span></span></span> at entries where the attention link must be cut, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mn>0</mn></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle 0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord"><span class="mord">0</span></span></span></span></span> at other places: 
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8d99a80dbf8da6e52c37ba3c9965387a19f82975" alt="masked attention"></p>
<p>For example, the following matrix is commonly used in decoder self-attention modules, called &quot;causal masking&quot;:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/981c71d86645b9f71d314dc671903905c0c30a9a" alt="Causal masking">
In words, it means that each token can pay attention to itself, and every token before it, but not any after it. </p>
<h3 id="encoder"><a name="encoder" class="anchor-navigation-ex-anchor" href="#encoder"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.6. Encoder</h3>
<p>An encoder consists of an embedding layer, followed by multiple encoder layers.</p>
<p>Each encoder layer consists of <strong>two major components</strong>: a self-attention mechanism and a feed-forward layer.</p>
<p>The encoder layers are stacked.
The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. 
This sequence of vectors is processed by the second encoder, and so on. 
<em>The output from the final encoder layer is then used by the decoder.</em></p>
<p>As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), 
so there is no need for causal masking.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Transformer%2C_one_encoder_block.png/440px-Transformer%2C_one_encoder_block.png" alt="One encoder layer"></p>
<h3 id="decoder"><a name="decoder" class="anchor-navigation-ex-anchor" href="#decoder"><i class="fa fa-link" aria-hidden="true"></i></a>1.1.7. Decoder</h3>
<p>Each decoder consists of <strong>three major components</strong>: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network.</p>
<p>The decoder functions use an additional attention mechanism to draw relevant information from the encodings generated by the encoders.
This mechanism can also be called the <em>encoder-decoder attention</em>.</p>
<p>Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings.
The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.
Thus, the self-attention module in the decoder is causally masked.</p>
<p>The cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding.
Schematically, we have:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53517bba056de79c117a30490add4f73868af2b7" alt="cross attention schema">
where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msup><mi>H</mi><mi>E</mi></msup></mstyle></mrow><annotation encoding="application/x-tex">{\displaystyle H^{E}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8913309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">E</span></span></span></span></span></span></span></span></span></span></span></span></span> is the matrix with rows being the output vectors from the encoder.</p>
<p>The last decoder is followed by a final un-embedding layer to produce the output probabilities over the vocabulary. 
Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.</p>
<h2 id="full-transformer-architecture"><a name="full-transformer-architecture" class="anchor-navigation-ex-anchor" href="#full-transformer-architecture"><i class="fa fa-link" aria-hidden="true"></i></a>1.2. Full transformer architecture</h2>
<h3 id="sublayers"><a name="sublayers" class="anchor-navigation-ex-anchor" href="#sublayers"><i class="fa fa-link" aria-hidden="true"></i></a>1.2.1. Sublayers</h3>
<p>Each encoder layer contains 2 sublayers: the self-attention and the feedforward network.
Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.</p>
<p>The final points of detail are the <em>residual connections</em> and <em>layer normalization (LayerNorm, or LN)</em>,
which while conceptually unnecessary, are necessary for numerical stability and convergence.</p>
<p>The original 2017 Transformer used the post-LN convention.
It was difficult to train and required careful hyperparameter tuning and a &quot;warm-up&quot; in learning rate,
where it starts small and gradually increases.</p>
<footer class="page-footer"><span class="copyright">Copyright &#xA9; &#x7248;&#x6743;&#x4FE1;&#x606F; all right reserved&#xFF0C;powered by Gitbook</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;: 
2024-12-19 16:32:17
</span></footer></body></html>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                
                <a href="Infrastructure_transformer_transformer-explainer.html" class="navigation navigation-next navigation-unique" aria-label="Next page: Transformer 可视化解释">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Transformer 结构-Wiki","level":"1.2.1.1","depth":3,"next":{"title":"Transformer 可视化解释","level":"1.2.1.2","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.md","articles":[]},"previous":{"title":"Transformer 架构与模型","level":"1.2.1","depth":2,"ref":"","articles":[{"title":"Transformer 结构-Wiki","level":"1.2.1.1","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_wiki.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_wiki.md","articles":[]},{"title":"Transformer 可视化解释","level":"1.2.1.2","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_transformer-explainer.md","articles":[]},{"title":"土猛的员外-Transformer 架构的整体指南","level":"1.2.1.3","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_luxiangdong_Transformer-OverallArch.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_luxiangdong_Transformer-OverallArch.md","articles":[]},{"title":"Encoder 模型","level":"1.2.1.4","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-models.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-models.md","articles":[]},{"title":"Decoder 模型","level":"1.2.1.5","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Decoder-models.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Decoder-models.md","articles":[]},{"title":"Encoder-Decoder 模型","level":"1.2.1.6","depth":3,"path":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-Decoder-models.md","ref":"Chap1-ModelArch/transformer_arch/Infrastructure_HF_Encoder-Decoder-models.md","articles":[]}]},"dir":"ltr"},"config":{"plugins":["-lunr","-search","search-pro","back-to-top-button","expandable-chapters","chapter-fold","splitter","code","github","tbfed-pagefooter","pageview-count","copy-code-button","anchor-navigation-ex","katex-pp"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy 版权信息","modify_label":"该文件修订时间: ","modify_format":"YYYY-MM-DD HH:mm:ss"},"chapter-fold":{},"github":{"url":"https://github.com/KKenny0/LLMpedia"},"splitter":{},"search-pro":{},"code":{"copyButtons":true},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"anchor-navigation-ex":{"showLevel":true,"associatedWithSummary":true,"mode":"float","showGoTop":true,"printLog":false,"multipleH1":true,"float":{"floatIcon":"fa fa-navicon","showLevelIcon":false,"level1Icon":"","level2Icon":"","level3Icon":""},"pageTop":{"showLevelIcon":false,"level1Icon":"","level2Icon":"","level3Icon":""}},"back-to-top-button":{},"pageview-count":{},"copy-code-button":{},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"katex-pp":{},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"expandable-chapters":{}},"theme":"default","keywords":"AI,LLM,笔记","author":"Kenny Wu","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"LLMpedia","introduction":{"path":"README.md","title":"LLMpedia"},"gitbook":"*","description":"搜集和分享个人喜欢的大语言模型（LLM）相关资源的在线百科全书。"},"file":{"path":"Chap1-ModelArch/transformer_arch/Infrastructure_transformer_wiki.md","mtime":"2024-12-19T08:32:17.405Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2024-12-20T02:39:46.657Z"},"basePath":"../..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../../gitbook/gitbook.js"></script>
    <script src="../../gitbook/theme.js"></script>
    
        
        <script src="../../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-chapter-fold/chapter-fold.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

